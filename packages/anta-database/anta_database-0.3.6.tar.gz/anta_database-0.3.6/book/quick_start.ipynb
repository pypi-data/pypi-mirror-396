{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb9d44e",
   "metadata": {},
   "source": [
    "# Quick start\n",
    "\n",
    "## Installation\n",
    "\n",
    "The Python module can be directly installed from [PyPI](https://pypi.org/project/anta-database/) with:\n",
    "\n",
    "    pip install anta_database\n",
    "\n",
    "Note that this module was developed for Python versions >=3.11. It is also recommanded to install it in a dedicated fresh python environment to avoid dependency issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bbc91",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "This Python module is designed to query, filter, and visualize data stored in an AntADatabase folder. While the AntADatabase is not yet publicly available, you can contact me for access to test the tool.\n",
    "You can already have a look at this guide to have an idea of the features of this tool. This Jupyter Notebook can be directly downloaded (top bar) and ran locally (assuming you had downloaded or compiled the AntADatabase folder).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437de49",
   "metadata": {},
   "source": [
    "## Initializing the database\n",
    "\n",
    "Having the AntADatabase folder stored on your machine, you can initialize import and initialize the 'Database' class which provides SQL query and filter functions as well as quick visualization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bff0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the database and create the SQL table (required only once or when adding new datasets)\n",
    "from anta_database import Database\n",
    "\n",
    "db = Database('/home/anthe/documents/data/isochrones/AntADatabase/', index_database=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b324f7",
   "metadata": {},
   "source": [
    "Note: the `index_database=True` argument creates a SQLite table (AntADatabase.db) that indexes all datasets in the AntADatabase folder. This ensures that future queries reflect data that is actually present in AntADatabase. This allows you to download the current entire AntADatabase or parts of it. You only need to set `index_database=True` once or when you add new datasets to the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For subsequent uses (no need to re-index unless new datasets are added)\n",
    "db = Database('/home/anthe/documents/data/isochrones/AntADatabase/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19165338",
   "metadata": {},
   "source": [
    "## Querying the database\n",
    "This section provides examples for querying the database using the `query()` and `filter_out()` functions.\n",
    "These functions allow you to browse and filter data based on various criteria such as datasets, institutes, projects, regions, layer ages and more.\n",
    "\n",
    "Use the `query()` without argument for an overview of the entire data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24813ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a88ddd",
   "metadata": {},
   "source": [
    "The `query()` can take one or multiple arguments. Each argument can take a string or a list of strings for multiple selections.  \n",
    "You can query by dataset, institute, project, age, region, IMBIE basin, variable, or flight ID. \n",
    "\n",
    "### **Parameters**\n",
    "| Parameter         | Description                                                                                     | Example Values                     |\n",
    "|-------------------|-------------------------------------------------------------------------------------------------|-------------------------------------|\n",
    "| `dataset`         | Name of the dataset(s) of interest.                                                             | `'Cavitte_2020'`, `['Franke_2025', 'Winter_2018']` |\n",
    "| `institute`       | Institute(s) that produced the data.                                                           | `'BAS'`, `['AWI', 'NASA']`            |\n",
    "| `project`         | Project(s) under which the data were collected.                                                | `'OIB'`                             |\n",
    "| `acquisition_year`| Year(s) in which the radar data were acquired. Supports ranges and inequalities.               | `'2000-2010'`, `'<1990'`, `'2005'` |\n",
    "| `age`             | Age(s) in years before present of the layer(s) of interest. Supports ranges and inequalities.   | `'10000'`, `'37500-38500'`, `'>90000'` |\n",
    "| `region`          | Region(s) of interest (e.g., `'EAIS'`, `'WAIS'`).                                              | `'EAIS'`                            |\n",
    "| `IMBIE_basin`     | IMBIE basin(s) of interest (e.g., `'G-H'`, `'Ap-B'`).                                           | `'G-H'`                             |\n",
    "| `var`             | Variable(s) of interest. Possible values: `'IRH_DEPTH'`, `'IRH_NUM'`, `'ICE_THK'`, `BED_ELEV'`, `'SURF_ELEV'`.      | `'ICE_THK'`                        |\n",
    "| `flight_id`       | ID of a particular flight line. Supports regex with `'%'`.                                     | `'DC_LDC_DIVIDE'`, `'%WSB%'`       |\n",
    "\n",
    "Note that `age` and `acquisition_year` can take ranges:\n",
    " - '<' for ages or acquisition year younger than X (age='<50000' means all ages younger than 50000 yrs old)\n",
    " - '>' for ages or acquisition year older than X (age='>50000' means all ages older 50000 yrs old)\n",
    " - for ages or acquisition years between a certain range, use '-' (acquisition_year='2000-2010' means all data acquired between these dates)\n",
    " - '<=' and '>=' will also work as you expect\n",
    "\n",
    " In addition, all arguments support regex with `'%'`:\n",
    "  - flight_id='OIA%' means all data with flight ID which starts by OIA\n",
    "  - dataset='%2025' means all dataset which ends by 2025 (so published in 2025)\n",
    "  - flight_id='%WSB%' means all data which has WSB anywhere in its flight id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe48269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples of queries:\n",
    "db.query(dataset='Cavitte_2020') # all data from Cavitte et al. 2020\n",
    "db.query(institute='BAS') # all data that was acquired by BAS\n",
    "db.query(project='OIB') # all data that was acquired during OIB campaigns\n",
    "db.query(age='38100') # all datasets with the 38.1ka isochrone\n",
    "db.query(var='ICE_THK') # all datasets with ICE_THK variable\n",
    "db.query(IMBIE_basin='G-H') # all flight lines that cross the G-H basin\n",
    "db.query(flight_id='DC_LDC_DIVIDE') # all layers with the flight ID DC_LDC_DIVIDE\n",
    "db.query(flight_id='%WSB%') # all flight lines with WSB in the flight ID\n",
    "db.query(dataset=['Franke_2025', 'Winter_2018'], age='38100') # example of multiple criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80bdcd",
   "metadata": {},
   "source": [
    "The filter_out() function allows the pre-filter out some data so they would never be included in the next queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "db.filter_out(acquisition_year='<1990')  # filter out all data acquired before 1990\n",
    "db.query() # now all queries will exclude data acquired before 1990\n",
    "\n",
    "db.filter_out()  # reset all filters to include all data again\n",
    "db.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925cf0c3",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Use the results of the query in the plotting functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e071c",
   "metadata": {},
   "source": [
    "Current implemented plotting functions are:\n",
    "- plot.dataset(): plots locations of the data, with different colors for the different datasets\n",
    "- plot.institute(): plots locations of the data, with different colors for the different institutes\n",
    "- plot.var(): color-coded scatter plot of the variable of interest.\n",
    "- plot.flight_id(): color-coded trace IDs. Useful for identifying specific traces of interest.\n",
    "- plot.transect_1D(): plots depths of the IRH and Bed along a single flight line \n",
    "\n",
    "In Jupyter Notebook, use '%matplotlib qt' or '%matplotlib widget' depending on your IDE, to switch to the matplotlib widget that allows you to zoom in etc.\n",
    "Use '%matplotlib inline' (default) to plot the figure in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fe92e",
   "metadata": {},
   "source": [
    "### Plot datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "results = db.query(IMBIE_basin='G-H')\n",
    "db.plot.dataset(results,\n",
    "                title='IRH data crossing the G-H IMBIE basin',\n",
    "                xlim=(-2000, -500), # set the plot extent in km\n",
    "                ylim=(-1000, 250),\n",
    "                marker_size=1, # adjust the size of the markers\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0217254",
   "metadata": {},
   "source": [
    "Note: all flight lines in the database are associated with one or several IMBIE basin(s). This depends if the flight line crosses one or multiple basins. The plot above shows all the flight lines which have traced IRH data and that crosses at some point the G-H basin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735309e",
   "metadata": {},
   "source": [
    "### Plot variables\n",
    "Example of plotting the IRH depth of a specific layer found across multiple datasets. Here we still select a range of ages close from each other, which could be attributed to the same layer (but different dating method used maybe). Note the warning in the case, but we can ignore it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a2cbc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "results = db.query(age='37500-38500', var='IRH_DEPTH')\n",
    "db.plot.var(results, title='AntArchitecture 38ka isochrone depth',\n",
    "                downsampling_factor=10, # downscale the datasets n times, which makes little visual difference but lighter to plot\n",
    "                xlim=(-500, 2400),\n",
    "                ylim=(-2200, 2200),\n",
    "                scale_factor=0.7, # adjust the size of the plot \n",
    "                marker_size=1.2,\n",
    "                # save='AntA_38ka_depth.png'\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ada192",
   "metadata": {},
   "source": [
    "The above plot shows the absolute IRH depth relative to the ice surface as it is traced. It is often more informative to look at the IRH fraction depth: the depth of a layer relative to the ice thickness (IRH_DEPTH/ICE_THK*100). The fraction depth variable is not directly stored in the database to reduce disk usage. It is probably more efficient to compute it when needed. For this, use the option fraction_depth=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29343c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "results = db.query(age=['37600', '38000', '38100', '38200', '38500'], var='IRH_DEPTH')\n",
    "db.plot.var(results, \n",
    "            fraction_depth=True,\n",
    "            title='AntArchitecture 38ka isochrone fractional depth',\n",
    "                downsampling_factor=10, # downscale the datasets n times, which makes little visual difference but lighter to plot\n",
    "                xlim=(-500, 2400),\n",
    "                ylim=(-2200, 2200),\n",
    "                scale_factor=0.7, # adjust the size of the plot \n",
    "                marker_size=1.2,\n",
    "                # save='AntA_38ka_depth.png'\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b53478",
   "metadata": {},
   "source": [
    "Note that if the variable ICE_THK is not present in a dataset, the fractional depth cannot be computed. In the case, NaN values will be generated instead without warning. So if you get a blank plot, please check if the dataset you queried contains the ICE_THK with db.query(dataset='Author_YYYY')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee330fa6",
   "metadata": {},
   "source": [
    "### Plot datasets\n",
    "\n",
    "The IRH_NUM variable shows the number of traced isochrones (layers) per data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.query(var='IRH_NUM')\n",
    "db.plot.var(results, title='IRH Density over the Antarctic Ice Sheet',\n",
    "                downsampling_factor=100,\n",
    "                scale_factor=1,\n",
    "                marker_size=1.2,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185b390",
   "metadata": {},
   "source": [
    "### Plot flight IDs\n",
    "\n",
    "This plot is useful when we want to identify a specific flight line. One can then identify a flight line of interest on the 2D map, then plot the traced IRHs along the transect (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93287978",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.query(dataset='Winter%', flight_id=['EPICA%'])\n",
    "db.plot.flight_id(results, title='Winter et al. 2018 - EPICA',\n",
    "                xlim=(-500, 1000),\n",
    "                ylim=(1000, 2200),\n",
    "                marker_size=1.2,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b02a87",
   "metadata": {},
   "source": [
    "### Plot layer depths along transect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a358b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.query(dataset='Cavitte_2020', flight_id='DC_LDC_DIVIDE')\n",
    "db.plot.transect_1D(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d77ba",
   "metadata": {},
   "source": [
    "Use the elevation=True option to plot the transect in absolute elevation above sea level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c3457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.plot.transect_1D(results, elevation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee8359",
   "metadata": {},
   "source": [
    "Other possible arguments for the plot methods:\n",
    "- cmap: provide your colormap of choice (as LinearSegmentedColormap). Tip: 'import colormaps as cmaps' for a large choice of colormaps (see [Colormaps docs](https://pratiman-91.github.io/colormaps/)\n",
    "- vmin and vmax: sets the minimum and maximum values for the colorbar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcf714",
   "metadata": {},
   "source": [
    "## BEDMAP\n",
    "\n",
    "The AntADatabase now contains all the BEDMAP (1, 2 and 3) data. This is useful to see the whole extent of the existing radar data, or to reconnect the IRH datasets to BEDMAP in order to get the Bed Elevation or Ice Thickness when those are not included.\n",
    "\n",
    "By default, BEDMAP is not shown in the query, since it is not an IRH dataset. To include it, initialize the Database with the option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de501f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Database('/home/anthe/documents/data/isochrones/AntADatabase/', include_BEDMAP=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dcee6d",
   "metadata": {},
   "source": [
    "## Get files from the database\n",
    "\n",
    "You may want to make your own plots or further process the data after querying the database. One option is to get the list of the files from your query and open them individually with either xarray or h5py. For this, use the get_files() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "results = db.query(dataset='Cavitte_2020', flight_id='DC_LDC_DIVIDE')\n",
    "file_list = db.get_files(results)\n",
    "f = file_list[0]\n",
    "ds = xr.open_dataset(f, engine='h5netcdf')\n",
    "print(ds)\n",
    "plt.plot(ds.Distance, ds.IRH_DEPTH)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0f26c",
   "metadata": {},
   "source": [
    "xarray provides a nice interface for interacting with the data. This works great when dealing with one file, and for example quickly plot all the layers (see above). But xarray adds some overload, which feels slow when reading many files. h5py on the other hand reads the underlying data right away, which is much more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "results = db.query(dataset='Cavitte_2020')\n",
    "file_list = db.get_files(results)\n",
    "for f in file_list:\n",
    "    with h5py.File(f, 'r') as ds:\n",
    "        plt.scatter(ds['PSX'][:], ds['PSY'][:], c=ds['ICE_THK'][:], s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e3be1",
   "metadata": {},
   "source": [
    "## Generate data from the database\n",
    "\n",
    "Note: This part could be developed further in the future if there is the need. But for now, I am designing a separate Python module for constraining my ice sheet model of use, which is tailored to this database and other parallel processing libraries. However, the [Model-comparison](https://antoinehermant.github.io/anta_database/pism_example.html) section already give some bits of code about it.\n",
    "\n",
    "The data_generator() function reads the query and 'yield' the dataframes for later use. It uses h5py to read all the data efficiently, and creates pandas dataframes, including all the variables and ages from the query. Columns for IRH DEPTH are named after the age. Basically, it reads the data with h5py as shown above and restructure it as bit, which can be easier to manage layers than with h5py dimensions.\n",
    "Here is a quick example of how this can be used for computing the mean layer depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c22b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.query(age=['37600', '38000', '38100', '38200', '38500'], var='IRH_DEPTH')\n",
    "lazy_dfs = db.data_generator(results)\n",
    "\n",
    "import numpy as np\n",
    "mean_depth_trs = []\n",
    "min_depth = float('inf')\n",
    "max_depth = float('-inf')\n",
    "for df, md in lazy_dfs:\n",
    "    depth_values = df[md['age']].values\n",
    "    mean_depth_trs.append(np.nanmean(depth_values))\n",
    "    min_depth = min(min_depth, np.nanmin(depth_values))\n",
    "    max_depth = max(max_depth, np.nanmax(depth_values))\n",
    "\n",
    "\n",
    "mean_depth = np.nanmean(mean_depth_trs)\n",
    "std_dev = np.nanstd(mean_depth_trs, ddof=1)\n",
    "print(f\"The mean depth of the 38ka isochrone across East Antarctica is {round(mean_depth, 2)} m ranging from {round(min_depth, 2)} m to {round(max_depth, 2)} m.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0f078",
   "metadata": {},
   "source": [
    "Note that the data_generator returns a simple pandas DataFrame for each flight line, containing all queried variables and ages (if exist). The IRH depth of each layer is stored by columns named by age (so the IRH_DEPTH of the layer 38000 is df['38000']). As shown above, one can use the metadata (md) of the current dataframe (df) to get its age (md['age']).\n",
    "Furthermore, if one needs the fraction depth, one can compute it using the ICE_THK (has to be included in the query), or the data_generator has this option: db.data_generator(results, fraction_depth=True). With this option, the layer columns will now be IRH fraction depth instead of absolute IRH_DEPTH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d2825",
   "metadata": {},
   "source": [
    "# Pro tips\n",
    "\n",
    "The Database methods always keep the last query in memory. This means that one does not have to actually pass argument.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4396357",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.query(var='IRH_NUM')\n",
    "# db.plot.var() # This is equivalent to the overview plot above. Note that without downscaling it is very heavy to plot.\n",
    "\n",
    "import numpy as np\n",
    "mean_density_trs = []\n",
    "min_density = float('inf')\n",
    "max_density = float('-inf')\n",
    "for df, md in db.data_generator():    # without explicitly passing results, it uses the last query.\n",
    "    density_values = df['IRH_NUM']\n",
    "    mean_density_trs.append(np.mean(density_values))\n",
    "    min_density = min(min_density, min(density_values))\n",
    "    max_density = max(max_density, max(density_values))\n",
    "\n",
    "mean_density = np.mean(mean_density_trs)\n",
    "print(f\"The average number of picked layers per data point in the AntADatabase is {int(mean_density)}, ranging from {int(min_density)} to {int(max_density)}. \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
