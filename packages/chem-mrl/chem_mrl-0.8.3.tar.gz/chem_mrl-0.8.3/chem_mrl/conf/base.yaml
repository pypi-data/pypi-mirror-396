defaults:
  - base_config_schema
  - /model: chem_2d_mrl # chem_mrl, chem_2d_mrl, classifier or dice_loss_classifier
  - _self_

datasets:
  # Provide train_dataset and/or val_dataset to train and evaluate on different datasets.
  # Datasets should be compatible with datasets.DatasetDict or datasets.Dataset (individual local files).
  - key: pubchem_10m_genmol_similarity # Should be unique
    train_dataset: # may be set to null if only using a val_dataset
      name: Derify/pubchem_10m_genmol_similarity # HuggingFace dataset name or path should load as a datasets.DatasetDict
      subset: null # Optional subset
      split_key: train
      label_cast_type: float32 # float64, float32, or float16 (used by chem_mrl model)
      sample_size: null # Number of training samples to load. Uses seeded sampling if a seed is set.
    val_dataset: # May be set to null if only using a train_dataset
      name: Derify/pubchem_10m_genmol_similarity
      subset: null
      split_key: validation # If loading from different local files, this should be set to train
      label_cast_type: float32
      sample_size: null
    test_dataset:
      null # may be set to null to disable evaluation on a test dataset
      # name: Derify/pubchem_10m_genmol_similarity
      # subset: null
      # split_key: test # If loading from different local files, this should be set to train
      # label_cast_type: float16
      # sample_size: null
    smiles_a_column_name: smiles_a
    smiles_b_column_name: smiles_b # Can be set to `null` when training a classifier model
    label_column_name: similarity

early_stopping_patience: 3 # Number of epochs to wait before early stopping
scale_learning_rate: false # Scale learning rate by sqrt(batch_size)
use_normalized_weight_decay:
  false # overrides the weight decay specified in training_args
  # Normalized weight decay for adamw optimizer - https://arxiv.org/pdf/1711.05101.pdf
  # optimized hyperparameter lambda_norm = 0.05 for AdamW optimizer
  # Hyperparameter search indicates a normalized weight decay outperforms
  # the default adamw weight decay

model_card_data: # A model card data object that contains information about the model.
  # This is used to generate a model card when saving the model. If not set, a default model card data object is created.
  _target_: sentence_transformers.model_card.SentenceTransformerModelCardData
  language: null
  license: apache-2.0
  model_name: "ChemMRL: SMILES Matryoshka Representation Learning Embedding Transformer"
  model_id: Derify/ChemMRL
  tags:
    - sentence-transformers
    - modchembert
    - cheminformatics
    - smiles
    - molecular-similarity
    - feature-extraction
    - dense

config_kwargs: # Additional model configuration parameters to be passed to the Hugging Face Transformers config.
  # See the `AutoConfig.from_pretrained documentation for more details.
  # <https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoConfig.from_pretrained>`_
  embedding_dropout: 0.1

# Note: SentenceTransformerTrainingArguments extends transformers.TrainingArguments
# https://sbert.net/docs/package_reference/sentence_transformer/training_args.html
# https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments
training_args:
  _target_: sentence_transformers.SentenceTransformerTrainingArguments
  run_name: chem_mrl
  resume_from_checkpoint: null # null or the path to a valid checkpoint to resume training from.
  output_dir: training_output/${training_args.run_name}_${now:%Y-%m-%d_%H-%M-%S} # Path to save model, checkpoints and evaluation results
  overwrite_output_dir: false
  num_train_epochs: 3
  warmup_steps: 100000
  lr_scheduler_type: warmup_stable_decay
  lr_scheduler_kwargs:
    # do not define num_warmup_steps since Trainer will calculate it based on num_training_steps and warmup_ratio/warmup_steps
    num_decay_steps: 100000
    warmup_type: linear
    decay_type: 1-sqrt
  optim: stable_adamw # transformer's compatible optimizer name
  optim_args: decouple_lr=True,max_lr=8.0e-6 # set max_lr when using warmup steps/ratio
  learning_rate: 8.0e-6
  weight_decay: 1.0e-5 # use 1.0e-5 when decouple_lr=True
  seed: 42
  data_seed: 42
  eval_on_start: true
  eval_strategy: steps # 'no', 'steps', 'epoch'
  eval_steps: 50000
  load_best_model_at_end: false
  metric_for_best_model: eval_${datasets[0].key}_${model.eval_metric} # Change index for your dataset of choice but keep the format as is
  save_strategy: steps # 'no', 'steps', 'epoch'
  save_steps: 50000
  save_total_limit: 10
  logging_strategy: steps # 'no', 'steps', 'epoch'
  logging_steps: 500
  logging_first_step: true
  logging_nan_inf_filter: true
  logging_dir: ${training_args.output_dir}/logs
  report_to: all # codecarbon and wandb installed by sentence-transformers
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 64
  tf32: false
  fp16: true
  fp16_full_eval: true
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  max_grad_norm: null # unnecessary when using StableAdamW optimizer
  disable_tqdm: false
  dataloader_num_workers: 0 # Must be set to 0
  dataloader_pin_memory: true
