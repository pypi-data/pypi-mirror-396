# ğŸ§± Multi-Process Logging

**`tools.mp_logging`** provides a lightweight, dependencyâ€‘free, **processâ€‘safe logging system** built on top of Pythonâ€™s standard `logging` library.

It ensures that logs produced from multiple worker processes (e.g., using `multiprocessing.Pool`) remain **ordered, uncorrupted, and easy to configure**, using a central QueueListener and worker-side QueueHandlers.

This module is ideal for simulation engines, ETL pipelines, schedulers, and highâ€‘throughput analytics where multiple processes must emit logs safely.

---

## ğŸš€ Key Features

- ğŸ§µ **Processâ€‘safe logging** using a central QueueListener  
- ğŸ”Œ **Zero external dependencies** â€” pure Python standard library  
- ğŸ§± **APIâ€‘compatible** with standard `logging.getLogger()`  
- ğŸ§ª **Typed** â€” strict MyPy compliance  
- ğŸ§² **Centralized handler setup** â€” workers never touch log files directly  
- ğŸ“¦ **Dropâ€‘in utility** for multiprocessingâ€‘based systems  

---

## âš¡ Why You Need This

The standard Python logging module is **threadâ€‘safe** but **not processâ€‘safe**.  
When multiple worker processes write to the same file handler:

- Lines get **interleaved**
- Outputs become **corrupted**
- Race conditions appear under high load

`tools.mp_logging` solves this by sending logs through a multiprocessing queue to a single, centralized listener in the master process.

---

## ğŸ”§ Quick Example

```python
import multiprocessing as mp
from tools.mp_logging import setup_logging, configure_worker, getLogger

def worker(i: int) -> None:
    logger = getLogger(__name__)
    logger.info(f"Working on item {i}")

if __name__ == "__main__":
    with setup_logging() as cfg:
        with mp.Pool(
            processes=4,
            initializer=configure_worker,
            initargs=(cfg.queue,)
        ) as pool:
            pool.map(worker, range(10))
```

---

## ğŸ§  How It Works

Internally, the module uses this architecture:

```
 Workers                           Main Process
----------                         ------------------------------
QueueHandler -> mp.Queue -> QueueListener -> Stream/File Handlers
```

### **In workers**
- A `QueueHandler` forwards `LogRecord` objects to the shared queue.
- Workers do *not* configure any handlers.

### **In the main process**
- A `QueueListener` receives all log records.
- Your desired handlers (console, file, rotating file, etc.) are attached only here.
- Logs are written in deterministic order.

---

## ğŸ§° Functional API

### `setup_logging(level=logging.INFO, handlers=None, queue=None)`
Initialize the logging system in the **main process**.

- Creates a `multiprocessing.Queue[LogRecord]`
- Starts a QueueListener that forwards logs to handlers
- Returns a `MPLoggingConfig` object with:
  - `queue`
  - `listener`

### `configure_worker(queue, level=logging.INFO, keep_existing_handlers=False)`
Prepare a worker process for safe logging.

- Installs a `QueueHandler`
- Removes other handlers unless explicitly kept

### `getLogger(name=None)`
Thin wrapper around `logging.getLogger`.  
Use exactly like the standard logging API.

### Root-level convenience functions
These mirror Python logging:

- `debug()`
- `info()`
- `warning()`
- `error()`
- `exception()`
- `critical()`

---

## ğŸ§ª Testing Example

A minimal multi-process test:

```python
from tools.mp_logging import setup_logging, configure_worker, getLogger
import multiprocessing as mp

def worker(q):
    configure_worker(q)
    getLogger(__name__).info("hello from worker")

def test():
    with setup_logging() as cfg:
        p = mp.Process(target=worker, args=(cfg.queue,))
        p.start()
        p.join()
```

---

## ğŸ“¦ Package Layout

```
tools/
â””â”€â”€ mp_logging/
    â”œâ”€â”€ __init__.py         # Public API (getLogger, setup_logging, ...)
    â””â”€â”€ core.py             # Typed queue-based logging implementation
tests/
â””â”€â”€ mp_logging/
    â””â”€â”€ test_mp_logging.py
```

---

## ğŸ§© Integration Ideas

Use mp_logging whenever you have:

- A simulation engine with parallel workers  
- Workerâ€‘based ETL or computation pipelines  
- A data logger that must aggregate events from several processes  
- Multiprocessing schedulers or batch workers  
- Any workload that writes logs from multiple processes  

---

## ğŸ› ï¸ Best Practices

- Configure logging **once** in the main process  
- Workers should always call `configure_worker(queue)`  
- Avoid heavy formatting in workers â€” let the listener handle it  
- Use JSON or structured logging handlers on the main process for analytics pipelines  

---

## ğŸ“š Reference

`tools.mp_logging` documentation lives alongside:

- [`tools.label_selector`](label_selector.md)
- Shared simulation utilities (coming soon)

mp_logging is part of the **disco-tools** suite of reusable components.

