Metadata-Version: 2.4
Name: hud-python
Version: 0.4.74
Summary: SDK for the HUD platform.
Project-URL: Homepage, https://github.com/hud-evals/hud-python
Project-URL: Bug Tracker, https://github.com/hud-evals/hud-python/issues
Project-URL: Documentation, https://docs.hud.ai
Author-email: HUD <founders@hud.ai>
License: MIT License
        
        Copyright (c) 2025 Human Union Data, Inc
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
License-File: LICENSE
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Python: <3.13,>=3.11
Requires-Dist: anthropic>=0.75
Requires-Dist: blessed>=1.20.0
Requires-Dist: datasets>=2.14.0
Requires-Dist: google-genai
Requires-Dist: httpx<1,>=0.23.0
Requires-Dist: hud-fastmcp-python-sdk>=0.1.2
Requires-Dist: hud-mcp-python-sdk>=3.13.2
Requires-Dist: hud-mcp-use-python-sdk==2.3.20
Requires-Dist: langchain==0.3.27
Requires-Dist: numpy>=1.24.0
Requires-Dist: openai>=2.8.1
Requires-Dist: opentelemetry-api>=1.34.1
Requires-Dist: opentelemetry-exporter-otlp-proto-http>=1.34.1
Requires-Dist: opentelemetry-instrumentation-mcp==0.47.0
Requires-Dist: opentelemetry-sdk>=1.34.1
Requires-Dist: packaging>=21.0
Requires-Dist: pathspec>=0.12.1
Requires-Dist: pillow>=11.1.0
Requires-Dist: prompt-toolkit==3.0.51
Requires-Dist: pydantic-settings<3,>=2.2
Requires-Dist: pydantic<3,>=2.6
Requires-Dist: questionary==2.1.0
Requires-Dist: rich>=13.0.0
Requires-Dist: scarf-sdk>=0.1.0
Requires-Dist: toml>=0.10.2
Requires-Dist: tornado>=6.5.2
Requires-Dist: typer>=0.9.0
Requires-Dist: watchfiles>=0.21.0
Requires-Dist: wrapt>=1.14.0
Provides-Extra: agent
Requires-Dist: dotenv>=0.9.9; extra == 'agent'
Requires-Dist: ipykernel; extra == 'agent'
Requires-Dist: ipython<9; extra == 'agent'
Requires-Dist: jupyter-client; extra == 'agent'
Requires-Dist: jupyter-core; extra == 'agent'
Requires-Dist: pillow>=11.1.0; extra == 'agent'
Requires-Dist: playwright; extra == 'agent'
Requires-Dist: pyautogui>=0.9.54; extra == 'agent'
Requires-Dist: pyright==1.1.407; extra == 'agent'
Requires-Dist: pytest-asyncio; extra == 'agent'
Requires-Dist: pytest-cov; extra == 'agent'
Requires-Dist: pytest-mock; extra == 'agent'
Requires-Dist: pytest<9,>=8.1.1; extra == 'agent'
Requires-Dist: ruff>=0.11.8; extra == 'agent'
Provides-Extra: agents
Requires-Dist: dotenv>=0.9.9; extra == 'agents'
Requires-Dist: ipykernel; extra == 'agents'
Requires-Dist: ipython<9; extra == 'agents'
Requires-Dist: jupyter-client; extra == 'agents'
Requires-Dist: jupyter-core; extra == 'agents'
Requires-Dist: pillow>=11.1.0; extra == 'agents'
Requires-Dist: playwright; extra == 'agents'
Requires-Dist: pyautogui>=0.9.54; extra == 'agents'
Requires-Dist: pyright==1.1.407; extra == 'agents'
Requires-Dist: pytest-asyncio; extra == 'agents'
Requires-Dist: pytest-cov; extra == 'agents'
Requires-Dist: pytest-mock; extra == 'agents'
Requires-Dist: pytest<9,>=8.1.1; extra == 'agents'
Requires-Dist: ruff>=0.11.8; extra == 'agents'
Provides-Extra: bedrock
Requires-Dist: anthropic[bedrock]>=0.75; extra == 'bedrock'
Provides-Extra: dev
Requires-Dist: dotenv>=0.9.9; extra == 'dev'
Requires-Dist: ipykernel; extra == 'dev'
Requires-Dist: ipython<9; extra == 'dev'
Requires-Dist: jupyter-client; extra == 'dev'
Requires-Dist: jupyter-core; extra == 'dev'
Requires-Dist: pillow>=11.1.0; extra == 'dev'
Requires-Dist: playwright; extra == 'dev'
Requires-Dist: pyautogui>=0.9.54; extra == 'dev'
Requires-Dist: pyright==1.1.407; extra == 'dev'
Requires-Dist: pytest-asyncio; extra == 'dev'
Requires-Dist: pytest-cov; extra == 'dev'
Requires-Dist: pytest-mock; extra == 'dev'
Requires-Dist: pytest<9,>=8.1.1; extra == 'dev'
Requires-Dist: ruff>=0.11.8; extra == 'dev'
Provides-Extra: rl
Requires-Dist: bitsandbytes>=0.41.0; (sys_platform == 'linux') and extra == 'rl'
Requires-Dist: liger-kernel>=0.5.0; (sys_platform == 'linux') and extra == 'rl'
Requires-Dist: peft>=0.17.1; extra == 'rl'
Requires-Dist: vllm==0.10.1.1; extra == 'rl'
Description-Content-Type: text/markdown

<div align="left">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/hud-evals/hud-python/main/docs/logo/hud_logo_dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/hud-evals/hud-python/main/docs/logo/hud_logo.svg">
    <img src="https://raw.githubusercontent.com/hud-evals/hud-python/main/docs/logo/hud_logo.svg" alt="HUD" width="150" style="margin-bottom: 24px;"/>
  </picture>
</div>

OSS RL environment + evals toolkit. Wrap software as environments, run benchmarks, and train with RL ‚Äì locally or at scale.

[![PyPI version](https://img.shields.io/pypi/v/hud-python?style=flat-square)](https://pypi.org/project/hud-python/)
[![License](https://img.shields.io/badge/license-MIT-green?style=flat-square)](LICENSE)
[![Add docs to Cursor](https://img.shields.io/badge/Add%20docs%20to-Cursor-black?style=flat-square)](https://cursor.com/en/install-mcp?name=docs-hud-python&config=eyJ1cmwiOiJodHRwczovL2RvY3MuaHVkLmFpL21jcCJ9)
[![Discord](https://img.shields.io/discord/1327447144772407390?label=Discord&logo=discord&style=flat-square)](https://discord.gg/wkjtmHYYjm)
[![X Follow](https://img.shields.io/twitter/follow/hud_evals?style=social)](https://x.com/intent/user?screen_name=hud_evals)
[![Shop](https://img.shields.io/badge/_-white.svg?label=shop&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAACxMAAAsTAQCanBgAAAF6SURBVChTlZA9ixNhFIWf8yaTpFHRRMXCKpAZhCAYFvwoLHZhwUKw9A9YCJb+Bq0sxGbBQrTxX1j41dvIRAjGZbdwRUUGIzPMeyw2swS3WZ/ynHvP5VylafoAWAd+5Xm+wX+SpukmcMf29RDCZrD9BViz3f53+CjYngKZpD5A2/Y7SQBMJpOkKIprdV1vdzqdHzHGblmW9Ww2+5pl2TmAxWKxmM/nP8fj8cmqqtZijJ9sb0u6ABBWjh0riuIt8CqE8LGu66e2d5MkeQ8QY3xme7fb7T4ZjUbrZVl+jjFuSXoEXGxCDgIl9WzfAO5LSmzvNB771R6vzG4Bx0MIt/M8vwV8aLyDQNt70+n0G1AspaTxVln+aghQluVsKbvxVysflT9NQK/XO7R/SGiQ9Nt2aftElmWXJd1kv0kbeANQVdWl4XB4XtJouXaqNRgMHkrqS+r0+/3XwD1JXdungRfAVWBi+6WkK8D3EMJz22cl3W21WgNgx3YAzvwFd0Chdq03gKUAAAAASUVORK5CYII=&style=social)](https://shop.hud.ai)
[![Scarf](https://static.scarf.sh/a.png?x-pxid=6530ff33-4945-452b-81f9-626872593933)](https://scarf.sh)


### Are you an enterprise building agents?

[üìÖ Hop on a call](https://cal.com/jay-hud) or [üìß founders@hud.ai](mailto:founders@hud.ai)

## Highlights

- üöÄ **[MCP environment skeleton](https://docs.hud.ai/core-concepts/mcp-protocol)** ‚Äì any agent can call any environment.
- ‚ö°Ô∏è **[Live telemetry](https://hud.ai)** ‚Äì inspect every tool call, observation, and reward in real time.
- üóÇÔ∏è **[Public benchmarks](https://hud.ai/leaderboards)** ‚Äì OSWorld-Verified, SheetBench-50, and more.
- üåê **[Cloud browsers](environments/remote_browser/)** ‚Äì AnchorBrowser, Steel, BrowserBase integrations for browser automation.
- üõ†Ô∏è **[Hot-reload dev loop](environments/README.md#phase-5-hot-reload-development-with-cursor-agent)** ‚Äì `hud dev` for iterating on environments without rebuilds.
- üéì **[One-click RL](https://hud.ai/models)** ‚Äì Run `hud rl` to get a trained model on any environment.

> We welcome contributors and feature requests ‚Äì open an issue or hop on a call to discuss improvements!

## Installation

```bash
# SDK - MCP servers, telemetry, evaluation
pip install hud-python

# CLI - RL pipeline, environment design
uv tool install hud-python@latest --python 3.12
# uv tool update-shell
```

> See [docs.hud.ai](https://docs.hud.ai), or add docs to any MCP client:
> `claude mcp add --transport http docs-hud https://docs.hud.ai/mcp`

Before starting, get your HUD_API_KEY at [hud.ai](https://hud.ai).


## Quickstart: Evals

For a tutorial that explains the agent and evaluation design, run:

```python
uvx hud-python quickstart
```

Or just write your own agent loop (more [examples here](examples/)).

```python
import asyncio, hud, os
from hud.settings import settings
from hud.clients import MCPClient
from hud.agents import ClaudeAgent
from hud.datasets import Task  # See docs: https://docs.hud.ai/reference/tasks

async def main() -> None:
    with hud.trace("Quick Start 2048"): # All telemetry works for any MCP-based agent (see https://hud.ai)
        task = {
            "prompt": "Reach 64 in 2048.",
            "mcp_config": {
                "hud": {
                    "url": "https://mcp.hud.ai/v3/mcp",  # HUD's cloud MCP server (see https://docs.hud.ai/core-concepts/architecture)
                    "headers": {
                        "Authorization": f"Bearer {settings.api_key}",  # Get your key at https://hud.ai
                        "Mcp-Image": "hudpython/hud-text-2048:v1.2"  # Docker image from https://hub.docker.com/u/hudpython
                    }
                }
            },
            "evaluate_tool": {"name": "evaluate", "arguments": {"name": "max_number", "arguments": {"target": 64}}},
        }
        task = Task(**task)

        # 1. Define the client explicitly:
        client = MCPClient(mcp_config=task.mcp_config)
        agent = ClaudeAgent(
            mcp_client=client,
            model="claude-sonnet-4-5",  # requires ANTHROPIC_API_KEY
        )

        result = await agent.run(task)

        # 2. Or just:
        # result = await ClaudeAgent().run(task)

        print(f"Reward: {result.reward}")
        await client.shutdown()

asyncio.run(main())
```

The above example let's the agent play 2048 ([See replay](https://hud.ai/trace/6feed7bd-5f67-4d66-b77f-eb1e3164604f))

![Agent playing 2048](https://raw.githubusercontent.com/hud-evals/hud-python/main/docs/src/images/2048_1.gif)

## Quickstart: Training

RL using GRPO a Qwen2.5-VL model on any hud dataset:

```bash
hud get hud-evals/2048-basic # from HF
hud rl 2048-basic.json
```

> See [agent training docs](https://docs.hud.ai/train-agents/quickstart)

Or make your own environment and dataset:

```bash
hud init my-env && cd my-env
hud dev --interactive
# When ready to run:
hud rl
```

> See [environment design docs](https://docs.hud.ai/build-environments)

## Benchmarking Agents

This is Claude Computer Use running on our proprietary financial analyst benchmark [SheetBench-50](https://huggingface.co/datasets/hud-evals/SheetBench-50):

![Trace screenshot](https://raw.githubusercontent.com/hud-evals/hud-python/main/docs/src/images/trace_sheet.gif)

> [See this trace on _hud.ai_](https://hud.ai/trace/9e212e9e-3627-4f1f-9eb5-c6d03c59070a)

This example runs the full dataset (only takes ~20 minutes) using [run_evaluation.py](examples/run_evaluation.py):

```bash
python examples/run_evaluation.py hud-evals/SheetBench-50 --full --agent claude
```

Or in code:

```python
import asyncio
from hud.datasets import run_dataset
from hud.agents import ClaudeAgent

results = await run_dataset(
    name="My SheetBench-50 Evaluation",
    dataset="hud-evals/SheetBench-50",      # <-- HuggingFace dataset
    agent_class=ClaudeAgent,                # <-- Your custom agent can replace this (see https://docs.hud.ai/evaluate-agents/create-agents)
    agent_config={"model": "claude-sonnet-4-5"},
    max_concurrent=50,
    max_steps=30,
)
print(f"Average reward: {sum(r.reward for r in results) / len(results):.2f}")
```

> Running a dataset creates a job and streams results to the [hud.ai](https://hud.ai) platform for analysis and [leaderboard submission](https://docs.hud.ai/evaluate-agents/leaderboards).

## Building Environments (MCP)

This is how you can make any environment into an interactable one in 5 steps:

1. Define MCP server layer using [`MCPServer`](https://docs.hud.ai/reference/environments)

```python
from hud.server import MCPServer
from hud.tools import HudComputerTool

mcp = MCPServer("My Environment")

# Add hud tools (see all tools: https://docs.hud.ai/reference/tools)
mcp.tool(HudComputerTool())

# Or custom tools (see https://docs.hud.ai/build-environments/adapting-software)
@mcp.tool("launch_app"):
def launch_app(name: str = "Gmail")
...

if __name__ == "__main__":
    mcp.run()
```

2. Write a simple Dockerfile that installs packages and runs:

```python
CMD ["python", "-m", "hud_controller.server"]
```

And build the image:

```bash
hud build # runs docker build under the hood
```

Or run it in interactible mode

```bash
hud dev
```

3. Debug it with the CLI to see if it launches:

```console
$ hud debug my-name/my-environment:latest

‚úì Phase 1: Docker image exists
‚úì Phase 2: MCP server responds to initialize 
‚úì Phase 3: Tools are discoverable
‚úì Phase 4: Basic tool execution works
‚úì Phase 5: Parallel performance is good

Progress: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 5/5 phases (100%)
‚úÖ All phases completed successfully!
```

Analyze it to see if all tools appear:

```console
$ hud analyze hudpython/hud-remote-browser:latest
‚†è ‚úì Analysis complete
...
Tools
‚îú‚îÄ‚îÄ Regular Tools
‚îÇ   ‚îú‚îÄ‚îÄ computer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Control computer with mouse, keyboard, and screenshots
...
‚îî‚îÄ‚îÄ Hub Tools
    ‚îú‚îÄ‚îÄ setup
    ‚îÇ   ‚îú‚îÄ‚îÄ navigate_to_url
    ‚îÇ   ‚îú‚îÄ‚îÄ set_cookies
    ‚îÇ   ‚îú‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ evaluate
        ‚îú‚îÄ‚îÄ url_match
        ‚îú‚îÄ‚îÄ page_contains
        ‚îú‚îÄ‚îÄ cookie_exists
        ‚îú‚îÄ‚îÄ ...

üì° Telemetry Data
 Live URL  https://live.anchorbrowser.io?sessionId=abc123def456
```

4. When the tests pass, push it up to the docker registry:

```bash
hud push # needs docker login, hud api key
```

5. Now you can use `mcp.hud.ai` to launch 100s of instances of this environment in parallel with any agent, and see everything live on [hud.ai](https://hud.ai):

```python
from hud.agents import ClaudeAgent

result = await ClaudeAgent().run({  # See all agents: https://docs.hud.ai/reference/agents
    "prompt": "Please explore this environment",
    "mcp_config": {
        "my-environment": {
            "url": "https://mcp.hud.ai/v3/mcp",
            "headers": {
                "Authorization": f"Bearer {os.getenv('HUD_API_KEY')}",
                "Mcp-Image": "my-name/my-environment:latest"
            }
        }
        # "my-environment": { # or use hud run which wraps local and remote running
        #     "cmd": "hud",
        #     "args": [
        #         "run",
        #         "my-name/my-environment:latest",
        #     ]
        # }
    }
})

```

> See the full environment design guide and common pitfalls in [`environments/README.md`](environments/README.md)

## Leaderboards & benchmarks

All leaderboards are publicly available on [hud.ai/leaderboards](https://hud.ai/leaderboards) (see [docs](https://docs.hud.ai/evaluate-agents/leaderboards))

![Leaderboard](https://raw.githubusercontent.com/hud-evals/hud-python/main/docs/src/images/leaderboards_3.png)

We highly suggest running 3-5 evaluations per dataset for the most consistent results across multiple jobs.

Using the [`run_dataset`](https://docs.hud.ai/reference/tasks#run_dataset) function with a HuggingFace dataset automatically assigns your job to that leaderboard page, and allows you to create a scorecard out of it:

## Reinforcement Learning with GRPO

This is a Qwen‚Äë2.5‚ÄëVL‚Äë3B agent training a policy on the 2048-basic browser environment:

![RL curve](https://raw.githubusercontent.com/hud-evals/hud-python/main/docs/src/images/rl_2.png)

Train with the new interactive `hud rl` flow:

```bash
# Install CLI
uv tool install hud-python@latest --python 3.12

# Option A: Run directly from a HuggingFace dataset
hud rl hud-evals/2048-basic

# Option B: Download first, modify, then train
hud get hud-evals/2048-basic
hud rl 2048-basic.json

# Optional: baseline evaluation
hud eval 2048-basic.json
```

Supports multi‚Äëturn RL for both:
- Language‚Äëonly models (e.g., `Qwen/Qwen2.5-7B-Instruct`)
- Vision‚ÄëLanguage models (e.g., `Qwen/Qwen2.5-VL-3B-Instruct`)

By default, `hud rl` provisions a persistent server and trainer in the cloud, streams telemetry to `hud.ai`, and lets you monitor/manage models at `hud.ai/models`. Use `--local` to run entirely on your machines (typically 2+ GPUs: one for vLLM, the rest for training).

Any HUD MCP environment and evaluation works with our RL pipeline (including remote configurations). See the guided docs: `https://docs.hud.ai/train-agents/quickstart`.

Pricing: Hosted vLLM and training GPU rates are listed in the [Training Quickstart ‚Üí Pricing](https://docs.hud.ai/train-agents/quickstart#pricing). Manage billing at the [HUD billing dashboard](https://hud.ai/project/billing).

## Architecture

```mermaid
%%{init: {"theme": "neutral", "themeVariables": {"fontSize": "14px"}} }%%
graph LR
    subgraph "Platform"
        Dashboard["üìä hud.ai"]
        API["üîå mcp.hud.ai"]
    end
  
    subgraph "hud"
        Agent["ü§ñ Agent"]
        Task["üìã Task"]
        SDK["üì¶ SDK"]
    end
  
    subgraph "Environments"
        LocalEnv["üñ•Ô∏è Local Docker<br/>(Development)"]
        RemoteEnv["‚òÅÔ∏è Remote Docker<br/>(100s Parallel)"]
    end
  
    subgraph "otel"
        Trace["üì° Traces & Metrics"]
    end
  
    Dataset["üìö Dataset<br/>(HuggingFace)"]
  
    AnyMCP["üîó Any MCP Client<br/>(Cursor, Claude, Custom)"]
  
    Agent <--> SDK
    Task --> SDK
    Dataset <-.-> Task
    SDK <-->|"MCP"| LocalEnv
    SDK <-->|"MCP"| API
    API  <-->|"MCP"| RemoteEnv
    SDK  --> Trace
    Trace --> Dashboard
    AnyMCP -->|"MCP"| API
  
```

## CLI reference

| Command                 | Purpose                                    | Docs |
| ----------------------- | ------------------------------------------ | ---- |
| [`hud init`](https://docs.hud.ai/reference/cli/init)            | Create new environment with boilerplate.  | [üìñ](https://docs.hud.ai/reference/cli/init) |
| [`hud dev`](https://docs.hud.ai/reference/cli/dev)              | Hot-reload development with Docker.        | [üìñ](https://docs.hud.ai/reference/cli/dev) |
| [`hud build`](https://docs.hud.ai/reference/cli/build)          | Build image and generate lock file.       | [üìñ](https://docs.hud.ai/reference/cli/build) |
| [`hud push`](https://docs.hud.ai/reference/cli/push)            | Share environment to registry.            | [üìñ](https://docs.hud.ai/reference/cli/push) |
| [`hud pull <target>`](https://docs.hud.ai/reference/cli/pull)   | Get environment from registry.            | [üìñ](https://docs.hud.ai/reference/cli/pull) |
| [`hud analyze <image>`](https://docs.hud.ai/reference/cli/analyze) | Discover tools, resources, and metadata.   | [üìñ](https://docs.hud.ai/reference/cli/analyze) |
| [`hud debug <image>`](https://docs.hud.ai/reference/cli/debug)   | Five-phase health check of an environment. | [üìñ](https://docs.hud.ai/reference/cli/debug) |
| [`hud run <image>`](https://docs.hud.ai/reference/cli/run)       | Run MCP server locally or remotely.       | [üìñ](https://docs.hud.ai/reference/cli/run) |

## Roadmap

- Merging our forks in to the main `mcp`, `mcp_use` repositories
- Helpers for building new environments (see [current guide](environments/README.md))
- Integrations with every major agent framework
- Evaluation environment registry
- MCP opentelemetry standard

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

Key areas:
- [Environment examples](environments/) - Add new MCP environments
- [Agent implementations](hud/agents/) - Add support for new LLM providers
- [Tool library](hud/tools/) - Extend the built-in tool collection
- [RL training](hud/rl/) - Improve reinforcement learning pipelines

Thanks to all our contributors!

<a href="https://github.com/hud-evals/hud-python/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=hud-evals/hud-python&max=50" />
</a>

## Citation

```bibtex
@software{hud2025agentevalplatform,
  author = {HUD and Jay Ram and Lorenss Martinsons and Parth Patel and Govind Pimpale and Dylan Bowman and Jaideep and Nguyen Nhat Minh},
  title  = {HUD: An Evaluation and RL Envrionments Platform for Agents},
  date   = {2025-04},
  url    = {https://github.com/hud-evals/hud-python},
  langid = {en}
}
```

> **License**: HUD is released under the MIT License ‚Äì see the [LICENSE](LICENSE) file for details.
