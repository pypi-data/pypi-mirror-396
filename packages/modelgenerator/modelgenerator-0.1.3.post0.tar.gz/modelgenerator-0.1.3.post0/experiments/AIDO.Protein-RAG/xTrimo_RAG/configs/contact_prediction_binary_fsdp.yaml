# lightning.pytorch==2.4.0
seed_everything: 42
trainer:
  accelerator: auto
  strategy:
    class_path: lightning.pytorch.strategies.FSDPStrategy
    init_args:
      auto_wrap_policy: modelgenerator.distributed.fsdp.wrap.AutoWrapPolicy
      sharding_strategy: HYBRID_SHARD
  devices: auto
  num_nodes: 1
  precision: 32
  callbacks:
  - class_path: lightning.pytorch.callbacks.LearningRateMonitor
    init_args:
      logging_interval: step
      log_momentum: false
      log_weight_decay: true
  - class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      # dirpath: logs/xtrimo_benchmark/cp_AIDO.Protein_16B
      filename: best_val:{epoch}-{val_top_L5_acc:.3f}
      monitor: val_top_L5_acc
      save_top_k: 1
      mode: max
      every_n_epochs: 1
      save_last: true
  max_epochs: 20
  max_steps: -1
  log_every_n_steps: 50
  accumulate_grad_batches: 1
  gradient_clip_val: 0.001
  gradient_clip_algorithm: value
  default_root_dir: null
model:
  class_path: modelgenerator.tasks.PairwiseTokenClassification
  init_args:
    backbone:
      class_path: modelgenerator.backbones.aido_protein_rag_16b
      init_args:
        max_length: 12800
        use_peft: true
        save_peft_only: true
        lora_r: 16
        lora_alpha: 16
        lora_dropout: 0.0
        lora_target_modules:
        - query
        - value
        - key
        - dense
        - router
        lora_use_rslora: true
        config_overwrites:
          hidden_dropout_prob: 0
          attention_probs_dropout_prob: 0
          gradient_checkpointing: true
        model_init_args: null
    adapter:
      class_path: modelgenerator.adapters.MLPAdapterWithoutOutConcat
      init_args:
        hidden_sizes:
        - 128
        activation_layer:
          class_path: torch.nn.ReLU
        bias: true
        dropout: 0
        dropout_in_middle: false
    n_classes: 2
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 0.0002
        betas:
        - 0.9
        - 0.95
        eps: 1.0e-08
        weight_decay: 0.01
    lr_scheduler:
      class_path: modelgenerator.lr_schedulers.ConstantWithWarmup
      init_args:
        warmup_ratio: 0
    strict_loading: true
    reset_optimizer_states: false
    use_legacy_adapter: false
    adapter_dim_multiplier: 1
data:
  class_path: modelgenerator.data.ContactPredictionBinary
  init_args:
    path: genbio-ai/contact_prediction_binary_rag
    is_rag_dataset: true
    train_split_files: null
    config_name: null
    x_col: seq
    extra_cols:
    - msa
    - str_emb
    batch_size: 1
    truncate_extra_cols: true
    max_length: 512
    train_split_name: train
    test_split_name: test
    valid_split_name: validation
    random_seed: 42
    shuffle: true
    max_context_length: 12800
    msa_random_seed: 1
ckpt_path: null
