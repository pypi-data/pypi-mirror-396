# tests/test_core.py

import os
import tempfile
from collections import Counter

from wordfreq_cn.core import (
    TfIdfResult,
    extract_keywords_tfidf,
    extract_keywords_tfidf_per_doc,
    count_word_frequency,
    load_stopwords,
    generate_trend_wordcloud, clean_text
)


class TestTFIDFKeywords:
    def test_tfidf_basic(self, sample_news):
        """æµ‹è¯• TF-IDF åŸºç¡€åŠŸèƒ½"""
        result = extract_keywords_tfidf(sample_news, top_k=5)

        assert isinstance(result, TfIdfResult)
        assert len(result.keywords) <= 5
        for key_item in result.keywords:
            assert isinstance(key_item.word, str)
            assert isinstance(key_item.weight, float)
            assert isinstance(key_item.count, int | None)
            assert key_item.weight > 0

    def test_tfidf_with_stopwords(self, sample_news, stopwords_file):
        """æµ‹è¯• TF-IDF åœç”¨è¯è¿‡æ»¤"""
        stopwords = load_stopwords(custom_file=stopwords_file)
        result = extract_keywords_tfidf(sample_news, stopwords=stopwords, top_k=10)

        stopwords_list = ["çš„", "äº†", "æ˜¯", "åœ¨", "ä¸"]
        for keyword_item in result.keywords:
            assert keyword_item.word not in stopwords_list

    def test_tfidf_empty_input(self):
        """æµ‹è¯•ç©ºè¾“å…¥"""
        result = extract_keywords_tfidf([], top_k=5)
        assert result.keywords == []

    def test_tfidf_single_document(self):
        """æµ‹è¯•å•æ–‡æ¡£è¾“å…¥"""
        result = extract_keywords_tfidf(["å•ä¸€æ–‡æ¡£æµ‹è¯•"], top_k=3)
        assert len(result.keywords) <= 3

    def test_tfidf__per_doc(self):
        """æµ‹è¯•å•æ–‡æ¡£è¾“å…¥"""
        result = extract_keywords_tfidf_per_doc(["å•ä¸€æ–‡æ¡£æµ‹è¯•"], top_k=3)
        assert len(result) <= 1
        assert len(result[0]) <= 3

        result1 = extract_keywords_tfidf_per_doc(["äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å–å¾—çªç ´","å…¨çƒæ°”å€™å˜åŒ–åŠ å‰§"], top_k=3)
        assert len(result1[0]) <= 3
        assert len(result1[1]) <= 3

class TestCountWords:
    def test_count_words_basic(self, sample_news):
        """æµ‹è¯•è¯é¢‘ç»Ÿè®¡åŸºç¡€åŠŸèƒ½"""
        counter = count_word_frequency(sample_news)

        assert isinstance(counter, Counter)
        assert len(counter) > 0

        common_words = counter.most_common(3)
        for word, count in common_words:
            assert isinstance(word, str)
            assert isinstance(count, int)
            assert count > 0

    def test_count_words_with_stopwords(self, sample_news, stopwords_file):
        """æµ‹è¯•å¸¦åœç”¨è¯çš„è¯é¢‘ç»Ÿè®¡"""
        stopwords = load_stopwords(custom_file=stopwords_file)
        counter = count_word_frequency(sample_news, stopwords=stopwords)

        stopwords_list = ["çš„", "äº†", "æ˜¯"]
        for stopword in stopwords_list:
            assert counter.get(stopword, 0) == 0

    def test_count_words_empty(self):
        """æµ‹è¯•ç©ºè¾“å…¥"""
        counter = count_word_frequency([])
        assert len(counter) == 0


class TestLoadStopwords:
    def test_load_default_stopwords(self):
        """æµ‹è¯•åŠ è½½é»˜è®¤åœç”¨è¯"""
        stopwords = load_stopwords()
        assert isinstance(stopwords, set)
        assert len(stopwords) > 0
        assert "çš„" in stopwords
        assert "äº†" in stopwords

    def test_load_custom_stopwords(self, stopwords_file):
        """æµ‹è¯•åŠ è½½è‡ªå®šä¹‰åœç”¨è¯"""
        stopwords = load_stopwords(custom_file=stopwords_file)
        assert isinstance(stopwords, set)
        assert "çš„" in stopwords
        assert "äº†" in stopwords
        assert "æ˜¯" in stopwords

    def test_load_nonexistent_file(self):
        """æµ‹è¯•åŠ è½½ä¸å­˜åœ¨çš„æ–‡ä»¶"""
        stopwords = load_stopwords(custom_file="nonexistent.txt")
        assert isinstance(stopwords, set)
        assert len(stopwords) > 0


class TestGenerateTrendWordcloud:
    def test_generate_wordcloud_basic(self, mock_news_by_date, tmp_path):
        """æµ‹è¯•ç”Ÿæˆè¯äº‘åŸºç¡€åŠŸèƒ½"""
        with tempfile.TemporaryDirectory() as temp_dir:
            files = generate_trend_wordcloud(
                mock_news_by_date,
                stopwords=set(),
                output_dir=temp_dir
            )

            assert isinstance(files, list)
            for file_path in files:
                assert os.path.exists(file_path)
                assert file_path.endswith('.png')

    def test_generate_wordcloud_custom_dir(self, mock_news_by_date, tmp_path):
        """æµ‹è¯•è‡ªå®šä¹‰è¾“å‡ºç›®å½•"""
        custom_dir = tmp_path / "wordclouds"
        custom_dir.mkdir()

        files = generate_trend_wordcloud(
            mock_news_by_date,
            stopwords=set(),
            output_dir=str(custom_dir)
        )

        for file_path in files:
            assert str(custom_dir) in file_path
            assert os.path.exists(file_path)

    def test_generate_wordcloud_empty_data(self, tmp_path):
        """æµ‹è¯•ç©ºæ•°æ®"""
        files = generate_trend_wordcloud(
            {},
            stopwords=set(),
            output_dir=str(tmp_path)
        )
        assert files == []

    def test_generate_wordcloud_bytes(self, mock_news_by_date, tmp_path):
        """æµ‹è¯•ç”Ÿæˆè¯äº‘å›¾bytes"""
        files = generate_trend_wordcloud(
            mock_news_by_date,
            stopwords=set(),
            return_bytes=True
        )

        # 1. è¿”å›ç±»å‹æ˜¯ list
        assert isinstance(files, list)
        assert len(files) > 0

        # 2. æ¯ä¸€é¡¹æ˜¯ bytes
        for item in files:
            assert isinstance(item, bytes)
            assert len(item) > 50   # PNG è‡³å°‘å‡ åå­—èŠ‚ï¼Œé¿å…ç©ºå­—èŠ‚ä¸²

            # 3. å¼€å¤´å¿…é¡»æ˜¯ PNG å¤´ï¼ˆéªŒè¯æ ¼å¼æ­£ç¡®ï¼‰
            assert item.startswith(b"\x89PNG\r\n\x1a\n")

class TestCleanText:
    def test_clean_text_basic_english_chinese(self):
        """ä¸­è‹±æ–‡æ··åˆ + å¤šç©ºæ ¼å‹ç¼©"""
        text = "  Hello   ä¸–ç•Œ!  "
        assert clean_text(text) == "hello ä¸–ç•Œ"


    def test_clean_text_remove_url_email(self):
        """åˆ é™¤ URL å’Œ email"""
        text = "è®¿é—® https://example.com æˆ–è”ç³» test@example.com"
        assert clean_text(text) == "è®¿é—® æˆ–è”ç³»"


    def test_clean_text_apostrophe_handling(self):
        """ä¿ç•™è‹±æ–‡å•è¯ä¸­çš„æ’‡å·ï¼Œä½†å»æ‰å­¤ç«‹æ’‡å·"""
        text = "Don't do it â€™ now ' test"
        assert clean_text(text) == "don't do it now test"


    def test_clean_text_remove_digits(self):
        """åˆ é™¤æ•°å­—"""
        text = "AI 2025 æŠ€æœ¯ 123"
        assert clean_text(text, remove_digits=True) == "ai æŠ€æœ¯"


    def test_clean_text_special_symbols(self):
        """è¿‡æ»¤ç‰¹æ®Šç¬¦å·ï¼ˆemojiã€è´§å¸ç¬¦å·ã€æ ‡ç‚¹ï¼‰"""
        text = "ä»·æ ¼Â£100 ğŸ˜Š éæ³•å­—ç¬¦#*& ä¿ç•™ä¸­æ–‡English123"
        assert clean_text(text) == "ä»·æ ¼ 100 éæ³•å­—ç¬¦ ä¿ç•™ä¸­æ–‡english123"