{
    "patch_size": 8,
    "input_size": 16,
    "data_dim": 128,
    "d_model": 128,
    "nhead": 16,
    "num_encoder_layers": 2,
    "dim_feedforward": 384,
    "max_seq_len": 30,
    "dropout": 0.2,
    "activation": "relu"
}
