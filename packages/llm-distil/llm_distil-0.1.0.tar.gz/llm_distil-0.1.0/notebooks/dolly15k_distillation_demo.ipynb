{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44229da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llm_distil from GitHub\n",
    "!pip install git+https://github.com/yashpatel2010/llm_distil.git\n",
    "\n",
    "# Import libraries\n",
    "from llm_distil import (\n",
    "    KnowledgeDistillation,\n",
    "    ReverseKnowledgeDistillation,\n",
    "    GeneralizedKnowledgeDistillation,\n",
    "    DistillationConfig\n",
    ")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba492fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dolly-15k dataset (small subset for demo)\n",
    "print(\"Loading Dolly-15k dataset...\")\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train[:1000]\")\n",
    "\n",
    "print(f\"âœ“ Loaded {len(dataset)} examples\")\n",
    "print(\"\\nExample:\")\n",
    "print(f\"Instruction: {dataset[0]['instruction'][:100]}...\")\n",
    "print(f\"Response: {dataset[0]['response'][:100]}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    # Combine instruction and response\n",
    "    texts = [f\"{inst}\\n{resp}\" for inst, resp in zip(examples[\"instruction\"], examples[\"response\"])]\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Split into train/eval\n",
    "train_dataset = tokenized_dataset.select(range(800))\n",
    "eval_dataset = tokenized_dataset.select(range(800, 1000))\n",
    "\n",
    "print(f\"âœ“ Train: {len(train_dataset)} examples\")\n",
    "print(f\"âœ“ Eval: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89524581",
   "metadata": {},
   "source": [
    "## 2. Load Dataset and Prepare Data\n",
    "\n",
    "We'll use a subset of **Databricks Dolly-15k**, an instruction-following dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f13a1",
   "metadata": {},
   "source": [
    "## 3. Configure Training\n",
    "\n",
    "Set up the distillation configuration with shared hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared configuration\n",
    "base_config = DistillationConfig(\n",
    "    teacher_model_name=\"gpt2-medium\",\n",
    "    student_model_name=\"gpt2\",\n",
    "    temperature=2.0,\n",
    "    kd_loss_weight=0.5,\n",
    "    epochs=2,  # Reduced for demo\n",
    "    batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    max_length=256,\n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    output_dir=\"./distil_output\"\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Temperature: {base_config.temperature}\")\n",
    "print(f\"  KD Loss Weight: {base_config.kd_loss_weight}\")\n",
    "print(f\"  Epochs: {base_config.epochs}\")\n",
    "print(f\"  Batch Size: {base_config.batch_size}\")\n",
    "print(f\"  Learning Rate: {base_config.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ec62d",
   "metadata": {},
   "source": [
    "## 4. Load Models\n",
    "\n",
    "Load the teacher (GPT2-medium) and student (GPT2) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teacher model (larger)\n",
    "print(\"Loading teacher model (GPT2-medium)...\")\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
    "print(f\"âœ“ Teacher params: {teacher.num_parameters():,}\")\n",
    "\n",
    "# Load student model (smaller) - we'll train multiple copies\n",
    "print(\"\\nLoading student model (GPT2)...\")\n",
    "student_baseline = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "student_kd = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "student_revkd = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "student_gkd = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "print(f\"âœ“ Student params: {student_baseline.num_parameters():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Compression ratio: {teacher.num_parameters() / student_baseline.num_parameters():.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training with KD (Forward KL Divergence)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "kd = KnowledgeDistillation(teacher, student_kd, base_config)\n",
    "kd.train(train_dataset, eval_dataset)\n",
    "\n",
    "print(\"âœ“ KD training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb14c4",
   "metadata": {},
   "source": [
    "## 5. Train with Standard KD (Forward KL)\n",
    "\n",
    "Train student using **Knowledge Distillation** with forward KL divergence.\n",
    "\n",
    "**Loss:** `L = (1-Î±)Â·CE + Î±Â·TÂ²Â·KL(Teacher || Student)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f01292",
   "metadata": {},
   "source": [
    "## 6. Train with RevKD (Reverse KL)\n",
    "\n",
    "Train student using **Reverse Knowledge Distillation** with reverse KL divergence (mode-seeking).\n",
    "\n",
    "**Loss:** `L = (1-Î±)Â·CE + Î±Â·TÂ²Â·KL(Student || Teacher)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ec05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training with RevKD (Reverse KL Divergence)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "revkd = ReverseKnowledgeDistillation(teacher, student_revkd, base_config)\n",
    "revkd.train(train_dataset, eval_dataset)\n",
    "\n",
    "print(\"âœ“ RevKD training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f1ed3",
   "metadata": {},
   "source": [
    "## 7. Train with GKD (Generalized JSD)\n",
    "\n",
    "Train student using **Generalized Knowledge Distillation** with JSD and on-policy generation.\n",
    "\n",
    "**Loss:** `L = Î»Â·JSD(Teacher, Student) + (1-Î»)Â·JSD(Teacher, Student_generated)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training with GKD (Generalized JSD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gkd_config = DistillationConfig(\n",
    "    teacher_model_name=\"gpt2-medium\",\n",
    "    student_model_name=\"gpt2\",\n",
    "    lambda_gkd=0.5,\n",
    "    beta_gkd=0.5,\n",
    "    epochs=2,\n",
    "    batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    max_length=256,\n",
    "    output_dir=\"./distil_output\"\n",
    ")\n",
    "\n",
    "gkd = GeneralizedKnowledgeDistillation(teacher, student_gkd, gkd_config)\n",
    "gkd.train(train_dataset, eval_dataset)\n",
    "\n",
    "print(\"âœ“ GKD training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944389a7",
   "metadata": {},
   "source": [
    "## 8. Evaluate All Methods\n",
    "\n",
    "Compare the perplexity of all distilled students on the evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce80d9a",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **All distillation methods outperform baseline** (student without distillation)\n",
    "2. **KD (Forward KL)** provides balanced performance (mean-seeking)\n",
    "3. **RevKD (Reverse KL)** focuses on high-confidence predictions (mode-seeking)\n",
    "4. **GKD (JSD)** leverages on-policy generation for robust distillation\n",
    "\n",
    "### When to Use Each Method:\n",
    "\n",
    "| Method | Best For | Behavior |\n",
    "|--------|----------|----------|\n",
    "| **KD** | General-purpose distillation | Covers all teacher modes |\n",
    "| **RevKD** | High-confidence tasks | Focuses on peaks |\n",
    "| **GKD** | Generative tasks | On-policy robustness |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try different temperatures (1.0-5.0)\n",
    "- Adjust `kd_loss_weight` (balance CE vs KD loss)\n",
    "- Train for more epochs for better convergence\n",
    "- Evaluate on downstream tasks (classification, generation, etc.)\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **GitHub**: https://github.com/yashpatel2010/llm_distil\n",
    "- **API Guide**: `docs/API_GUIDE.md`\n",
    "- **Examples**: `examples/distill_dolly15k.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270df636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "prompt = \"What is machine learning?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Tokenize prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate from each model\n",
    "models = {\n",
    "    \"Teacher\": teacher,\n",
    "    \"KD\": student_kd,\n",
    "    \"RevKD\": student_revkd,\n",
    "    \"GKD\": student_gkd\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(text)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11585cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Evaluating all methods...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate teacher\n",
    "print(\"\\n[1/5] Evaluating teacher...\")\n",
    "from llm_distil.metrics import compute_perplexity\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=4, shuffle=False)\n",
    "teacher_ppl = compute_perplexity(teacher, eval_dataset, tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Evaluate students\n",
    "print(\"[2/5] Evaluating baseline student...\")\n",
    "baseline_ppl = compute_perplexity(student_baseline, eval_dataset, tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"[3/5] Evaluating KD student...\")\n",
    "kd_metrics = kd.evaluate(eval_dataset)\n",
    "\n",
    "print(\"[4/5] Evaluating RevKD student...\")\n",
    "revkd_metrics = revkd.evaluate(eval_dataset)\n",
    "\n",
    "print(\"[5/5] Evaluating GKD student...\")\n",
    "gkd_metrics = gkd.evaluate(eval_dataset)\n",
    "\n",
    "# Create comparison table\n",
    "results_df = pd.DataFrame({\n",
    "    \"Method\": [\"Teacher (GPT2-medium)\", \"Baseline (no distill)\", \"KD (Forward KL)\", \"RevKD (Reverse KL)\", \"GKD (JSD)\"],\n",
    "    \"Perplexity\": [\n",
    "        teacher_ppl,\n",
    "        baseline_ppl,\n",
    "        kd_metrics['perplexity'],\n",
    "        revkd_metrics['perplexity'],\n",
    "        gkd_metrics['perplexity']\n",
    "    ],\n",
    "    \"Model Size\": [\"355M\", \"124M\", \"124M\", \"124M\", \"124M\"]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
