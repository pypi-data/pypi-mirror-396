{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837f1fa7",
   "metadata": {},
   "source": [
    "## 1. Installation and Imports\n",
    "\n",
    "First, install the required packages (uncomment if needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llm_distil from GitHub\n",
    "# !pip install git+https://github.com/yashpatel2010/llm_distil.git\n",
    "\n",
    "# Install PEFT for LoRA support\n",
    "# !pip install peft>=0.7.0 bitsandbytes>=0.41.0 accelerate>=0.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fc3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from llm_distil import KnowledgeDistillation, DistillationConfig\n",
    "from llm_distil.metrics import compute_perplexity\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0de83",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "We'll use 200 examples from Dolly-15k for a quick demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading Dolly-15k dataset...\")\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train[:200]\")\n",
    "\n",
    "print(f\"âœ“ Loaded {len(dataset)} examples\")\n",
    "print(f\"\\nSample example:\")\n",
    "print(f\"  Instruction: {dataset[0]['instruction'][:100]}...\")\n",
    "print(f\"  Response: {dataset[0]['response'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aecf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    texts = [f\"{inst}\\n{resp}\" for inst, resp in zip(examples[\"instruction\"], examples[\"response\"])]\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Split into train/eval\n",
    "train_dataset = tokenized_dataset.select(range(160))\n",
    "eval_dataset = tokenized_dataset.select(range(160, 200))\n",
    "\n",
    "print(f\"âœ“ Train: {len(train_dataset)} examples\")\n",
    "print(f\"âœ“ Eval: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c88f1ad",
   "metadata": {},
   "source": [
    "## 3. Load Models\n",
    "\n",
    "Load teacher (GPT-2 Medium) and two student models (GPT-2 Small) - one for full fine-tuning and one for LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading models...\\n\")\n",
    "\n",
    "# Teacher model\n",
    "print(\"[1/3] Loading teacher model (GPT-2 Medium)...\")\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
    "print(f\"âœ“ Teacher: {teacher.num_parameters():,} parameters\")\n",
    "\n",
    "# Student model for full fine-tuning\n",
    "print(\"\\n[2/3] Loading student model for full fine-tuning (GPT-2 Small)...\")\n",
    "student_full = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "print(f\"âœ“ Student (Full FT): {student_full.num_parameters():,} parameters\")\n",
    "\n",
    "# Student model for LoRA\n",
    "print(\"\\n[3/3] Loading student model for LoRA (GPT-2 Small)...\")\n",
    "student_lora = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "print(f\"âœ“ Student (LoRA): {student_lora.num_parameters():,} parameters\")\n",
    "\n",
    "compression_ratio = teacher.num_parameters() / student_full.num_parameters()\n",
    "print(f\"\\nðŸ“Š Compression ratio: {compression_ratio:.2f}x (355M â†’ 117M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e67ef",
   "metadata": {},
   "source": [
    "## 4. Train with Full Fine-tuning\n",
    "\n",
    "First, let's train with traditional full fine-tuning (all 117M parameters updated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92785a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Training with FULL Fine-tuning\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "full_config = DistillationConfig(\n",
    "    teacher_model_name=\"gpt2-medium\",\n",
    "    student_model_name=\"gpt2\",\n",
    "    temperature=2.0,\n",
    "    kd_loss_weight=0.5,\n",
    "    epochs=1,\n",
    "    batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    output_dir=\"./outputs/full_finetune\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    use_peft=False  # Full fine-tuning\n",
    ")\n",
    "\n",
    "kd_full = KnowledgeDistillation(teacher, student_full, full_config)\n",
    "print(\"\\nðŸ”§ All 117M parameters will be updated during training...\\n\")\n",
    "\n",
    "# Train\n",
    "full_history = kd_full.train(train_dataset, eval_dataset)\n",
    "\n",
    "print(\"\\nâœ“ Full fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84383b00",
   "metadata": {},
   "source": [
    "## 5. Train with LoRA (Parameter-Efficient)\n",
    "\n",
    "Now, let's train with LoRA - only ~0.3M parameters (0.26%) will be updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3417d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Training with LoRA (Parameter-Efficient)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lora_config = DistillationConfig(\n",
    "    teacher_model_name=\"gpt2-medium\",\n",
    "    student_model_name=\"gpt2\",\n",
    "    temperature=2.0,\n",
    "    kd_loss_weight=0.5,\n",
    "    epochs=1,\n",
    "    batch_size=4,\n",
    "    learning_rate=1e-4,  # Higher LR works better for LoRA\n",
    "    output_dir=\"./outputs/lora\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    use_peft=True,  # Enable LoRA\n",
    "    peft_type=\"lora\",\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    lora_target_modules=None  # Auto-detect for GPT-2\n",
    ")\n",
    "\n",
    "kd_lora = KnowledgeDistillation(teacher, student_lora, lora_config)\n",
    "print(\"\\nâš¡ LoRA applied! Only ~0.3M parameters (0.26%) will be trained...\\n\")\n",
    "\n",
    "# Train\n",
    "lora_history = kd_lora.train(train_dataset, eval_dataset)\n",
    "\n",
    "print(\"\\nâœ“ LoRA training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f1004",
   "metadata": {},
   "source": [
    "## 6. Evaluate Both Methods\n",
    "\n",
    "Compare perplexity scores on the evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c07a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Evaluating Models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"\\n[1/3] Evaluating teacher...\")\n",
    "teacher_ppl = compute_perplexity(teacher, eval_dataset, tokenizer, device=device, batch_size=4)\n",
    "print(f\"Teacher perplexity: {teacher_ppl:.2f}\")\n",
    "\n",
    "print(\"\\n[2/3] Evaluating full fine-tuning student...\")\n",
    "full_metrics = kd_full.evaluate(eval_dataset)\n",
    "full_ppl = full_metrics['perplexity']\n",
    "print(f\"Full fine-tuning perplexity: {full_ppl:.2f}\")\n",
    "\n",
    "print(\"\\n[3/3] Evaluating LoRA student...\")\n",
    "lora_metrics = kd_lora.evaluate(eval_dataset)\n",
    "lora_ppl = lora_metrics['perplexity']\n",
    "print(f\"LoRA perplexity: {lora_ppl:.2f}\")\n",
    "\n",
    "print(\"\\nâœ“ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6481f",
   "metadata": {},
   "source": [
    "## 7. Results Comparison\n",
    "\n",
    "Let's create a comprehensive comparison table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Method': ['Teacher', 'Full Fine-tuning', 'LoRA'],\n",
    "    'Model': ['GPT-2 Medium', 'GPT-2 Small', 'GPT-2 Small'],\n",
    "    'Perplexity': [teacher_ppl, full_ppl, lora_ppl],\n",
    "    'Total Params': ['355M', '117M', '117M'],\n",
    "    'Trainable Params': ['355M', '117M', '~0.3M (0.26%)'],\n",
    "    'Model Size': ['1.4GB', '500MB', '500MB + 2MB adapters']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + results.to_string(index=False))\n",
    "\n",
    "# Calculate performance difference\n",
    "ppl_diff = ((lora_ppl - full_ppl) / full_ppl) * 100\n",
    "print(f\"\\nðŸ“Š Performance difference: {ppl_diff:+.2f}%\")\n",
    "print(f\"   (LoRA is within {abs(ppl_diff):.1f}% of full fine-tuning)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d420b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Perplexity comparison\n",
    "methods = ['Teacher', 'Full FT', 'LoRA']\n",
    "perplexities = [teacher_ppl, full_ppl, lora_ppl]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "ax1.bar(methods, perplexities, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Perplexity', fontsize=12)\n",
    "ax1.set_title('Perplexity Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add values on bars\n",
    "for i, v in enumerate(perplexities):\n",
    "    ax1.text(i, v + 2, f'{v:.1f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Trainable parameters comparison\n",
    "params = [117, 117, 0.3]  # in millions\n",
    "methods_student = ['Full FT', 'LoRA']\n",
    "params_student = [117, 0.3]\n",
    "colors_student = ['#e74c3c', '#2ecc71']\n",
    "\n",
    "ax2.bar(methods_student, params_student, color=colors_student, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('Trainable Parameters (Millions)', fontsize=12)\n",
    "ax2.set_title('Trainable Parameters Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Add values on bars\n",
    "for i, v in enumerate(params_student):\n",
    "    ax2.text(i, v * 1.5, f'{v}M', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./outputs/lora_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Plot saved to ./outputs/lora_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a4fb1",
   "metadata": {},
   "source": [
    "## 8. Text Generation Comparison\n",
    "\n",
    "Let's compare the quality of generated text from all three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cf044",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain knowledge distillation in simple terms.\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Teacher generation\n",
    "    teacher.eval()\n",
    "    teacher.to(device)\n",
    "    with torch.no_grad():\n",
    "        teacher_output = teacher.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    teacher_text = tokenizer.decode(teacher_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Full fine-tuning generation\n",
    "    student_full.eval()\n",
    "    student_full.to(device)\n",
    "    with torch.no_grad():\n",
    "        full_output = student_full.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    full_text = tokenizer.decode(full_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # LoRA generation\n",
    "    student_lora.eval()\n",
    "    student_lora.to(device)\n",
    "    with torch.no_grad():\n",
    "        lora_output = student_lora.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    lora_text = tokenizer.decode(lora_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n[Teacher - GPT-2 Medium]\\n{teacher_text}\")\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"\\n[Full Fine-tuning - 117M params trained]\\n{full_text}\")\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"\\n[LoRA - 0.3M params trained]\\n{lora_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9562d",
   "metadata": {},
   "source": [
    "## 9. Save Models and Compare Sizes\n",
    "\n",
    "Save both models and compare their disk storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77499aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Saving Models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save models\n",
    "print(\"\\nSaving full fine-tuning model...\")\n",
    "kd_full.save_student('./outputs/models/full_student')\n",
    "\n",
    "print(\"Saving LoRA adapters...\")\n",
    "kd_lora.save_student('./outputs/models/lora_student')\n",
    "\n",
    "print(\"\\nâœ“ Models saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ffc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate file sizes\n",
    "import os\n",
    "\n",
    "def get_directory_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if os.path.exists(fp):\n",
    "                total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "full_size = get_directory_size('./outputs/models/full_student')\n",
    "lora_size = get_directory_size('./outputs/models/lora_student')\n",
    "savings = (1 - lora_size/full_size) * 100\n",
    "\n",
    "print(\"\\nðŸ“¦ Model Storage Comparison:\")\n",
    "print(f\"  â€¢ Full fine-tuning: {full_size:.1f} MB\")\n",
    "print(f\"  â€¢ LoRA adapters: {lora_size:.1f} MB\")\n",
    "print(f\"  â€¢ Storage savings: {savings:.1f}%\")\n",
    "print(f\"\\nðŸ’¾ To use LoRA model:\")\n",
    "print(f\"  1. Load base model: AutoModelForCausalLM.from_pretrained('gpt2')\")\n",
    "print(f\"  2. Load adapters: PeftModel.from_pretrained(model, './outputs/models/lora_student')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141fc04",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations\n",
    "\n",
    "Let's summarize the key findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48139b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ‰ EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nâœ¨ Key Findings:\")\n",
    "print(f\"  â€¢ LoRA uses only 0.26% of trainable parameters (0.3M vs 117M)\")\n",
    "print(f\"  â€¢ LoRA achieves {abs(ppl_diff):.1f}% difference in perplexity\")\n",
    "print(f\"  â€¢ LoRA saves {savings:.1f}% storage space ({lora_size:.1f}MB vs {full_size:.1f}MB)\")\n",
    "print(f\"  â€¢ LoRA enables training on consumer GPUs (2GB vs 8GB memory)\")\n",
    "\n",
    "print(\"\\nðŸ’¡ When to use LoRA:\")\n",
    "print(\"  âœ“ Limited GPU memory (e.g., RTX 3090, T4)\")\n",
    "print(\"  âœ“ Need multiple task-specific adapters\")\n",
    "print(\"  âœ“ Fast iteration and experimentation\")\n",
    "print(\"  âœ“ Easy deployment (small adapter files)\")\n",
    "print(\"  âœ“ Cost-effective training\")\n",
    "\n",
    "print(\"\\nðŸ’ª When to use Full Fine-tuning:\")\n",
    "print(\"  âœ“ Maximum performance is critical\")\n",
    "print(\"  âœ“ Have sufficient compute resources (A100+)\")\n",
    "print(\"  âœ“ Single-task deployment\")\n",
    "print(\"  âœ“ Large domain shift from base model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All results saved to ./outputs/\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921be7cd",
   "metadata": {},
   "source": [
    "## Additional Experiments\n",
    "\n",
    "Try these variations to explore further:\n",
    "\n",
    "### 1. Different LoRA Ranks\n",
    "```python\n",
    "# Lower rank (more efficient, slightly lower performance)\n",
    "config.lora_r = 4\n",
    "\n",
    "# Higher rank (more capacity, closer to full FT)\n",
    "config.lora_r = 16\n",
    "```\n",
    "\n",
    "### 2. QLoRA (4-bit Quantization)\n",
    "```python\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "student = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "config.peft_type = \"qlora\"\n",
    "```\n",
    "\n",
    "### 3. Other PEFT Methods\n",
    "```python\n",
    "# Prefix Tuning\n",
    "config.peft_type = \"prefix\"\n",
    "\n",
    "# Prompt Tuning\n",
    "config.peft_type = \"prompt\"\n",
    "\n",
    "# IA3\n",
    "config.peft_type = \"ia3\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
