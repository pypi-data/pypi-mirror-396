from typing import Optional, Union
from ..generation import GenerationConfig
from ..utils import add_end_docstrings, is_torch_available, is_vision_available, logging
from .base import Pipeline, build_pipeline_init_args
if is_vision_available():
    from PIL import Image
    from ..image_utils import load_image
if is_torch_available():
    from ..models.auto.modeling_auto import MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING_NAMES
    from .pt_utils import KeyDataset
logger = logging.get_logger(__name__)
@add_end_docstrings(build_pipeline_init_args(has_tokenizer=True, has_image_processor=True))
class VisualQuestionAnsweringPipeline(Pipeline):
    _load_processor = False
    _load_image_processor = True
    _load_feature_extractor = False
    _load_tokenizer = True
    _pipeline_calls_generate = True
    _default_generation_config = GenerationConfig(
        max_new_tokens=256,
    )
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.check_model_type(MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING_NAMES)
    def _sanitize_parameters(self, top_k=None, padding=None, truncation=None, timeout=None, **kwargs):
        preprocess_params, postprocess_params = {}, {}
        if padding is not None:
            preprocess_params["padding"] = padding
        if truncation is not None:
            preprocess_params["truncation"] = truncation
        if timeout is not None:
            preprocess_params["timeout"] = timeout
        if top_k is not None:
            postprocess_params["top_k"] = top_k
        forward_params = {}
        if getattr(self, "assistant_model", None) is not None:
            forward_params["assistant_model"] = self.assistant_model
        if getattr(self, "assistant_tokenizer", None) is not None:
            forward_params["tokenizer"] = self.tokenizer
            forward_params["assistant_tokenizer"] = self.assistant_tokenizer
        return preprocess_params, forward_params, postprocess_params
    def __call__(
        self,
        image: Union["Image.Image", str, list["Image.Image"], list[str], "KeyDataset"],
        question: Optional[Union[str, list[str]]] = None,
        **kwargs,
    ):
        is_dataset = isinstance(image, KeyDataset)
        is_image_batch = isinstance(image, list) and all(isinstance(item, (Image.Image, str)) for item in image)
        is_question_batch = isinstance(question, list) and all(isinstance(item, str) for item in question)
        if isinstance(image, (Image.Image, str)) and isinstance(question, str):
            inputs = {"image": image, "question": question}
        elif (is_image_batch or is_dataset) and isinstance(question, str):
            inputs = [{"image": im, "question": question} for im in image]
        elif isinstance(image, (Image.Image, str)) and is_question_batch:
            inputs = [{"image": image, "question": q} for q in question]
        elif (is_image_batch or is_dataset) and is_question_batch:
            question_image_pairs = []
            for q in question:
                for im in image:
                    question_image_pairs.append({"image": im, "question": q})
            inputs = question_image_pairs
        else:
            inputs = image
        results = super().__call__(inputs, **kwargs)
        return results
    def preprocess(self, inputs, padding=False, truncation=False, timeout=None):
        image = load_image(inputs["image"], timeout=timeout)
        model_inputs = self.tokenizer(
            inputs["question"],
            return_tensors=self.framework,
            padding=padding,
            truncation=truncation,
        )
        image_features = self.image_processor(images=image, return_tensors=self.framework)
        if self.framework == "pt":
            image_features = image_features.to(self.dtype)
        model_inputs.update(image_features)
        return model_inputs
    def _forward(self, model_inputs, **generate_kwargs):
        if self.model.can_generate():
            if "generation_config" not in generate_kwargs:
                generate_kwargs["generation_config"] = self.generation_config
            model_outputs = self.model.generate(**model_inputs, **generate_kwargs)
        else:
            model_outputs = self.model(**model_inputs)
        return model_outputs
    def postprocess(self, model_outputs, top_k=5):
        if self.model.can_generate():
            return [
                {"answer": self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()}
                for output_ids in model_outputs
            ]
        else:
            if top_k > self.model.config.num_labels:
                top_k = self.model.config.num_labels
            if self.framework == "pt":
                probs = model_outputs.logits.sigmoid()[0]
                scores, ids = probs.topk(top_k)
            else:
                raise ValueError(f"Unsupported framework: {self.framework}")
            scores = scores.tolist()
            ids = ids.tolist()
            return [{"score": score, "answer": self.model.config.id2label[_id]} for score, _id in zip(scores, ids)]