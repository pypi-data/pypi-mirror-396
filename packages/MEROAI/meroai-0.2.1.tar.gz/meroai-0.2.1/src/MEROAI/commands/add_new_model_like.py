import difflib
import os
import re
import subprocess
import textwrap
from argparse import ArgumentParser, Namespace
from datetime import date
from pathlib import Path
from typing import Any, Callable, Optional, Union
from ..models.auto.configuration_auto import CONFIG_MAPPING_NAMES, MODEL_NAMES_MAPPING
from ..models.auto.feature_extraction_auto import FEATURE_EXTRACTOR_MAPPING_NAMES
from ..models.auto.image_processing_auto import IMAGE_PROCESSOR_MAPPING_NAMES
from ..models.auto.processing_auto import PROCESSOR_MAPPING_NAMES
from ..models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES
from ..models.auto.video_processing_auto import VIDEO_PROCESSOR_MAPPING_NAMES
from ..utils import is_libcst_available
from . import BaseMEROAICLICommand
from .add_fast_image_processor import add_fast_image_processor
if is_libcst_available():
    import libcst as cst
    from libcst import CSTVisitor
    from libcst import matchers as m
    class ClassFinder(CSTVisitor):
        def __init__(self):
            self.classes: list = []
            self.public_classes: list = []
            self.is_in_class = False
        def visit_ClassDef(self, node: cst.ClassDef) -> None:
            self.classes.append(node.name.value)
            self.is_in_class = True
        def leave_ClassDef(self, node: cst.ClassDef):
            self.is_in_class = False
        def visit_SimpleStatementLine(self, node: cst.SimpleStatementLine):
            simple_top_level_assign_structure = m.SimpleStatementLine(
                body=[m.Assign(targets=[m.AssignTarget(target=m.Name())])]
            )
            if not self.is_in_class and m.matches(node, simple_top_level_assign_structure):
                assigned_variable = node.body[0].targets[0].target.value
                if assigned_variable == "__all__":
                    elements = node.body[0].value.elements
                    self.public_classes = [element.value.value for element in elements]
CURRENT_YEAR = date.today().year
MEROAI_PATH = Path(__file__).parent.parent
REPO_PATH = MEROAI_PATH.parent.parent
.lstrip()
class ModelInfos:
    def __init__(self, lowercase_name: str):
        self.lowercase_name = lowercase_name.lower().replace(" ", "_").replace("-", "_")
        if self.lowercase_name not in CONFIG_MAPPING_NAMES:
            self.lowercase_name.replace("_", "-")
        if self.lowercase_name not in CONFIG_MAPPING_NAMES:
            raise ValueError(f"{lowercase_name} is not a valid model name")
        self.paper_name = MODEL_NAMES_MAPPING[self.lowercase_name]
        self.config_class = CONFIG_MAPPING_NAMES[self.lowercase_name]
        self.camelcase_name = self.config_class.replace("Config", "")
        if self.lowercase_name in TOKENIZER_MAPPING_NAMES:
            self.tokenizer_class, self.fast_tokenizer_class = TOKENIZER_MAPPING_NAMES[self.lowercase_name]
            self.fast_tokenizer_class = (
                None if self.fast_tokenizer_class == "PreTrainedTokenizerFast" else self.fast_tokenizer_class
            )
        else:
            self.tokenizer_class, self.fast_tokenizer_class = None, None
        self.image_processor_class, self.fast_image_processor_class = IMAGE_PROCESSOR_MAPPING_NAMES.get(
            self.lowercase_name, (None, None)
        )
        self.video_processor_class = VIDEO_PROCESSOR_MAPPING_NAMES.get(self.lowercase_name, None)
        self.feature_extractor_class = FEATURE_EXTRACTOR_MAPPING_NAMES.get(self.lowercase_name, None)
        self.processor_class = PROCESSOR_MAPPING_NAMES.get(self.lowercase_name, None)
def add_content_to_file(file_name: Union[str, os.PathLike], new_content: str, add_after: str):
    with open(file_name, "r", encoding="utf-8") as f:
        old_content = f.read()
    before, after = old_content.split(add_after, 1)
    new_content = before + add_after + new_content + after
    with open(file_name, "w", encoding="utf-8") as f:
        f.write(new_content)
def add_model_to_auto_mappings(
    old_model_infos: ModelInfos,
    new_lowercase_name: str,
    new_model_paper_name: str,
    filenames_to_add: list[tuple[str, bool]],
):
    new_cased_name = "".join(x.title() for x in new_lowercase_name.replace("-", "_").split("_"))
    old_lowercase_name = old_model_infos.lowercase_name
    old_cased_name = old_model_infos.camelcase_name
    filenames_to_add = [
        (filename.replace(old_lowercase_name, "auto"), to_add) for filename, to_add in filenames_to_add[1:]
    ]
    corrected_filenames_to_add = []
    for file, to_add in filenames_to_add:
        if re.search(r"(?:tokenization)|(?:image_processing)_auto_fast.py", file):
            previous_file, previous_to_add = corrected_filenames_to_add[-1]
            corrected_filenames_to_add[-1] = (previous_file, previous_to_add or to_add)
        else:
            corrected_filenames_to_add.append((file, to_add))
    add_content_to_file(
        MEROAI_PATH / "models" / "auto" / "configuration_auto.py",
        new_content=f'        ("{new_lowercase_name}", "{new_cased_name}Config"),\n',
        add_after="CONFIG_MAPPING_NAMES = OrderedDict[str, str](\n    [\n        # Add configs here\n",
    )
    add_content_to_file(
        MEROAI_PATH / "models" / "auto" / "configuration_auto.py",
        new_content=f'        ("{new_lowercase_name}", "{new_model_paper_name}"),\n',
        add_after="MODEL_NAMES_MAPPING = OrderedDict[str, str](\n    [\n        # Add full (and cased) model names here\n",
    )
    for filename, to_add in corrected_filenames_to_add:
        if to_add:
            filename = filename.replace("_fast.py", ".py")
            with open(MEROAI_PATH / "models" / "auto" / filename) as f:
                file = f.read()
            matching_lines = re.findall(
                rf'( {{8,12}}\(\s*"{old_lowercase_name}",.*?\),\n)(?: {{4,12}}\(|\])', file, re.DOTALL
            )
            for match in matching_lines:
                add_content_to_file(
                    MEROAI_PATH / "models" / "auto" / filename,
                    new_content=match.replace(old_lowercase_name, new_lowercase_name).replace(
                        old_cased_name, new_cased_name
                    ),
                    add_after=match,
                )
def create_doc_file(new_paper_name: str, public_classes: list[str]):
    added_note = (
        "\n\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that "
        "may not be rendered properly in your Markdown viewer.\n\n-->\n\n"
    )
    copyright_for_markdown = re.sub(r"# ?", "", COPYRIGHT).replace("coding=utf-8\n", "<!--") + added_note
    doc_template = textwrap.dedent(
    )
    doc_for_classes = []
    for class_ in public_classes:
        doc = f"## {class_}\n\n[[autodoc]] {class_}"
        if "Model" in class_:
            doc += "\n    - forward"
        doc_for_classes.append(doc)
    class_doc = "\n\n".join(doc_for_classes)
    return copyright_for_markdown + doc_template + class_doc
def insert_model_in_doc_toc(old_lowercase_name: str, new_lowercase_name: str, new_model_paper_name: str):
    toc_file = REPO_PATH / "docs" / "source" / "en" / "_toctree.yml"
    with open(toc_file, "r") as f:
        content = f.read()
    old_model_toc = re.search(rf"- local: model_doc/{old_lowercase_name}\n {{8}}title: .*?\n", content).group(0)
    new_toc = f"      - local: model_doc/{new_lowercase_name}\n        title: {new_model_paper_name}\n"
    add_content_to_file(
        REPO_PATH / "docs" / "source" / "en" / "_toctree.yml", new_content=new_toc, add_after=old_model_toc
    )
def create_init_file(old_lowercase_name: str, new_lowercase_name: str, filenames_to_add: list[tuple[str, bool]]):
    filenames_to_add = [
        (filename.replace(old_lowercase_name, new_lowercase_name).replace(".py", ""), to_add)
        for filename, to_add in filenames_to_add
    ]
    imports = "\n            ".join(f"from .{file} import *" for file, to_add in filenames_to_add if to_add)
    init_file = COPYRIGHT + textwrap.dedent(
    )
    return init_file
def find_all_classes_from_file(module_name: str) -> set:
    with open(module_name, "r", encoding="utf-8") as file:
        source_code = file.read()
    module = cst.parse_module(source_code)
    visitor = ClassFinder()
    module.visit(visitor)
    return visitor.classes, visitor.public_classes
def find_modular_structure(
    module_name: str, old_model_infos: ModelInfos, new_cased_name: str
) -> tuple[str, str, list]:
    all_classes, public_classes = find_all_classes_from_file(module_name)
    import_location = ".".join(module_name.parts[-2:]).replace(".py", "")
    old_cased_name = old_model_infos.camelcase_name
    imports = f"from ..{import_location} import {', '.join(class_ for class_ in all_classes)}"
    modular_classes = "\n\n".join(
        f"class {class_.replace(old_cased_name, new_cased_name)}({class_}):\n    pass" for class_ in all_classes
    )
    public_classes = [class_.replace(old_cased_name, new_cased_name) for class_ in public_classes]
    return imports, modular_classes, public_classes
def create_modular_file(
    old_model_infos: ModelInfos,
    new_lowercase_name: str,
    filenames_to_add: list[tuple[str, bool]],
) -> str:
    new_cased_name = "".join(x.title() for x in new_lowercase_name.replace("-", "_").split("_"))
    old_lowercase_name = old_model_infos.lowercase_name
    old_folder_root = MEROAI_PATH / "models" / old_lowercase_name
    all_imports = ""
    all_bodies = ""
    all_public_classes = []
    for filename, to_add in filenames_to_add:
        if to_add:
            imports, body, public_classes = find_modular_structure(
                old_folder_root / filename, old_model_infos, new_cased_name
            )
            all_imports += f"\n{imports}"
            all_bodies += f"\n\n{body}"
            all_public_classes.extend(public_classes)
    public_classes_formatted = "\n            ".join(f"{public_class}," for public_class in all_public_classes)
    all_statement = textwrap.dedent(
    )
    modular_file = COPYRIGHT + all_imports + all_bodies + all_statement
    all_public_classes = [public_class.replace('"', "") for public_class in all_public_classes]
    return modular_file, all_public_classes
def create_test_files(old_model_infos: ModelInfos, new_lowercase_name, filenames_to_add: list[tuple[str, bool]]):
    new_cased_name = "".join(x.title() for x in new_lowercase_name.replace("-", "_").split("_"))
    old_lowercase_name = old_model_infos.lowercase_name
    old_cased_name = old_model_infos.camelcase_name
    filenames_to_add = [
        ("test_" + filename.replace(old_lowercase_name, new_lowercase_name), to_add)
        for filename, to_add in filenames_to_add[1:]
    ]
    corrected_filenames_to_add = []
    for file, to_add in filenames_to_add:
        if re.search(rf"test_(?:tokenization)|(?:image_processing)_{new_lowercase_name}_fast.py", file):
            previous_file, previous_to_add = corrected_filenames_to_add[-1]
            corrected_filenames_to_add[-1] = (previous_file, previous_to_add or to_add)
        else:
            corrected_filenames_to_add.append((file, to_add))
    test_files = {}
    for new_file, to_add in corrected_filenames_to_add:
        if to_add:
            original_test_file = new_file.replace(new_lowercase_name, old_lowercase_name)
            original_test_path = REPO_PATH / "tests" / "models" / old_lowercase_name / original_test_file
            if not original_test_path.is_file():
                continue
            with open(original_test_path, "r") as f:
                test_code = f.read()
            test_lines = test_code.split("\n")
            idx = 0
            while test_lines[idx].startswith("#"):
                idx += 1
            test_code = COPYRIGHT + "\n".join(test_lines[idx:])
            test_files[new_file] = test_code.replace(old_cased_name, new_cased_name)
    return test_files
def create_new_model_like(
    old_model_infos: ModelInfos,
    new_lowercase_name: str,
    new_model_paper_name: str,
    filenames_to_add: list[tuple[str, bool]],
    create_fast_image_processor: bool,
):
    if not is_libcst_available():
        raise ValueError("You need to install `libcst` to run this command -> `pip install libcst`")
    old_lowercase_name = old_model_infos.lowercase_name
    new_module_folder = MEROAI_PATH / "models" / new_lowercase_name
    os.makedirs(new_module_folder, exist_ok=True)
    modular_file, public_classes = create_modular_file(old_model_infos, new_lowercase_name, filenames_to_add)
    with open(new_module_folder / f"modular_{new_lowercase_name}.py", "w") as f:
        f.write(modular_file)
    init_file = create_init_file(old_lowercase_name, new_lowercase_name, filenames_to_add)
    with open(new_module_folder / "__init__.py", "w") as f:
        f.write(init_file)
    add_content_to_file(
        MEROAI_PATH / "models" / "__init__.py",
        new_content=f"    from .{new_lowercase_name} import *\n",
        add_after="if TYPE_CHECKING:\n",
    )
    add_model_to_auto_mappings(old_model_infos, new_lowercase_name, new_model_paper_name, filenames_to_add)
    tests_folder = REPO_PATH / "tests" / "models" / new_lowercase_name
    os.makedirs(tests_folder, exist_ok=True)
    with open(tests_folder / "__init__.py", "w"):
        pass
    test_files = create_test_files(old_model_infos, new_lowercase_name, filenames_to_add)
    for filename, content in test_files.items():
        with open(tests_folder / filename, "w") as f:
            f.write(content)
    doc_file = create_doc_file(new_model_paper_name, public_classes)
    with open(REPO_PATH / "docs" / "source" / "en" / "model_doc" / f"{new_lowercase_name}.md", "w") as f:
        f.write(doc_file)
    insert_model_in_doc_toc(old_lowercase_name, new_lowercase_name, new_model_paper_name)
    if create_fast_image_processor:
        add_fast_image_processor(model_name=new_lowercase_name)
    model_init_file = MEROAI_PATH / "models" / "__init__.py"
    subprocess.run(
        ["ruff", "check", new_module_folder, tests_folder, model_init_file, "--fix"],
        cwd=REPO_PATH,
        stdout=subprocess.DEVNULL,
    )
    subprocess.run(
        ["ruff", "format", new_module_folder, tests_folder, model_init_file],
        cwd=REPO_PATH,
        stdout=subprocess.DEVNULL,
    )
    subprocess.run(
        ["python", "utils/check_doc_toc.py", "--fix_and_overwrite"], cwd=REPO_PATH, stdout=subprocess.DEVNULL
    )
    subprocess.run(["python", "utils/sort_auto_mappings.py"], cwd=REPO_PATH, stdout=subprocess.DEVNULL)
    subprocess.run(
        ["python", "utils/modular_model_converter.py", new_lowercase_name], cwd=REPO_PATH, stdout=subprocess.DEVNULL
    )
def get_user_field(
    question: str,
    default_value: Optional[str] = None,
    convert_to: Optional[Callable] = None,
    fallback_message: Optional[str] = None,
) -> Any:
    if not question.endswith(" "):
        question = question + " "
    if default_value is not None:
        question = f"{question} [{default_value}] "
    valid_answer = False
    while not valid_answer:
        answer = input(question)
        if default_value is not None and len(answer) == 0:
            answer = default_value
        if convert_to is not None:
            try:
                answer = convert_to(answer)
                valid_answer = True
            except Exception:
                valid_answer = False
        else:
            valid_answer = True
        if not valid_answer:
            print(fallback_message)
    return answer
def convert_to_bool(x: str) -> bool:
    if x.lower() in ["1", "y", "yes", "true"]:
        return True
    if x.lower() in ["0", "n", "no", "false"]:
        return False
    raise ValueError(f"{x} is not a value that can be converted to a bool.")
def get_user_input():
    model_types = list(MODEL_NAMES_MAPPING.keys())
    valid_model_type = False
    while not valid_model_type:
        old_model_type = input(
            "What model would you like to duplicate? Please provide it as lowercase, e.g. `llama`): "
        )
        if old_model_type in model_types:
            valid_model_type = True
        else:
            print(f"{old_model_type} is not a valid model type.")
            near_choices = difflib.get_close_matches(old_model_type, model_types)
            if len(near_choices) >= 1:
                if len(near_choices) > 1:
                    near_choices = " or ".join(near_choices)
                print(f"Did you mean {near_choices}?")
    old_model_infos = ModelInfos(old_model_type)
    new_lowercase_name = get_user_field(
        "What is the new model name? Please provide it as snake lowercase, e.g. `new_model`?"
    )
    new_model_paper_name = get_user_field(
        "What is the fully cased name you would like to appear in the doc (e.g. `NeW ModEl`)? ",
        default_value="".join(x.title() for x in new_lowercase_name.split("_")),
    )
    add_tokenizer = False
    add_fast_tokenizer = False
    add_image_processor = False
    add_fast_image_processor = False
    add_video_processor = False
    add_feature_extractor = False
    add_processor = False
    if old_model_infos.tokenizer_class is not None:
        add_tokenizer = get_user_field(
            f"Do you want to create a new tokenizer? If `no`, it will use the same as {old_model_type} (y/n)?",
            convert_to=convert_to_bool,
            fallback_message="Please answer yes/no, y/n, true/false or 1/0. ",
        )
    if old_model_infos.fast_tokenizer_class is not None:
        add_fast_tokenizer = get_user_field(
            f"Do you want to create a new fast tokenizer? If `no`, it will use the same as {old_model_type} (y/n)?",
            convert_to=convert_to_bool,
            fallback_message="Please answer yes/no, y/n, true/false or 1/0. ",
        )
    if old_model_infos.image_processor_class is not None:
        add_image_processor = get_user_field(
            f"Do you want to create a new image processor? If `no`, it will use the same as {old_model_type} (y/n)?",
            convert_to=convert_to_bool,
            fallback_message="Please answer yes/no, y/n, true/false or 1/0. ",
        )
    if old_model_infos.fast_image_processor_class is not None:
        add_fast_image_processor = get_user_field(
            f"Do you want to create a new fast image processor? If `no`, it will use the same as {old_model_type} (y/n)?",
            convert_to=convert_to_bool,
            fallback_message="Please answer yes/no, y/n, true/false or 1/0. ",
        )
    if old_model_infos.video_processor_class is not None:
        add_video_processor = get_user_field(
            f"Do you want to create a new video processor? If `no`, it will use the same as {old_model_type} (y/n)?",
            convert_to=convert_to_bool,
            fallback_message="Please answer yes/no, y/n, true/false or 1/0. ",
        )
    if old_model_infos.feature_extractor_class is not None:
        add_feature_extractor = get_user_field(
            f"Do you want to create a new feature extractor? If `no`, it will use the same as {old_model_type} (y/n)?",
            convert_to=convert_to_bool,
            fallback_message="Please answer yes/no, y/n, true/false or 1/0. ",
        )
    if old_model_infos.processor_class is not None:
        add_processor = get_user_field(
            f"Do you want to create a new processor? If `no`, it will use the same as {old_model_type} (y/n)?",
            convert_to=convert_to_bool,
            fallback_message="Please answer yes/no, y/n, true/false or 1/0. ",
        )
    old_lowercase_name = old_model_infos.lowercase_name
    filenames_to_add = (
        (f"configuration_{old_lowercase_name}.py", True),
        (f"modeling_{old_lowercase_name}.py", True),
        (f"tokenization_{old_lowercase_name}.py", add_tokenizer),
        (f"tokenization_{old_lowercase_name}_fast.py", add_fast_tokenizer),
        (f"image_processing_{old_lowercase_name}.py", add_image_processor),
        (f"image_processing_{old_lowercase_name}_fast.py", add_fast_image_processor),
        (f"video_processing_{old_lowercase_name}.py", add_video_processor),
        (f"feature_extraction_{old_lowercase_name}.py", add_feature_extractor),
        (f"processing_{old_lowercase_name}.py", add_processor),
    )
    create_fast_image_processor = False
    if add_image_processor and not add_fast_image_processor:
        create_fast_image_processor = get_user_field(
            "A fast image processor can be created from the slow one, but modifications might be needed. "
            "Should we add a fast image processor class for this model (recommended) (y/n)? ",
            convert_to=convert_to_bool,
            default_value="y",
            fallback_message="Please answer yes/no, y/n, true/false or 1/0.",
        )
    return old_model_infos, new_lowercase_name, new_model_paper_name, filenames_to_add, create_fast_image_processor
def add_new_model_like_command_factory(args: Namespace):
    return AddNewModelLikeCommand(path_to_repo=args.path_to_repo)
class AddNewModelLikeCommand(BaseMEROAICLICommand):
    @staticmethod
    def register_subcommand(parser: ArgumentParser):
        add_new_model_like_parser = parser.add_parser("add-new-model-like")
        add_new_model_like_parser.add_argument(
            "--path_to_repo", type=str, help="When not using an editable install, the path to the MEROAI repo."
        )
        add_new_model_like_parser.set_defaults(func=add_new_model_like_command_factory)
    def __init__(self, path_to_repo=None, **kwargs):
        (
            self.old_model_infos,
            self.new_lowercase_name,
            self.new_model_paper_name,
            self.filenames_to_add,
            self.create_fast_image_processor,
        ) = get_user_input()
        self.path_to_repo = path_to_repo
    def run(self):
        if self.path_to_repo is not None:
            global MEROAI_PATH
            global REPO_PATH
            REPO_PATH = Path(self.path_to_repo)
            MEROAI_PATH = REPO_PATH / "src" / "MEROAI"
        create_new_model_like(
            old_model_infos=self.old_model_infos,
            new_lowercase_name=self.new_lowercase_name,
            new_model_paper_name=self.new_model_paper_name,
            filenames_to_add=self.filenames_to_add,
            create_fast_image_processor=self.create_fast_image_processor,
        )