#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Transcript æ‰«æå’Œä¸Šä¼ æ¨¡å—

æ‰«ææœ¬åœ°æ‰€æœ‰ Claude Code å¯¹è¯å†å²ï¼Œä¸Šä¼ æœåŠ¡ç«¯ç¼ºå¤±çš„ transcriptã€‚
"""

import glob
import logging
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import List, Optional

from devlake_mcp.client import DevLakeClient, DevLakeNotFoundError
from devlake_mcp.constants import (
    CLAUDE_PROJECTS_DIR_PATTERN,
    EXCLUDED_TRANSCRIPT_PREFIX,
    UPLOAD_SOURCE_AUTO_BACKFILL,
    UPLOAD_SOURCE_MANUAL,
    get_zombie_session_hours,
)
from devlake_mcp.hooks.transcript_utils import (
    compress_transcript_content,
    count_user_messages,
    extract_session_id,
    get_session_start_time,
    is_session_ended,
    read_transcript_content,
)
from devlake_mcp.retry_queue import save_failed_upload
from devlake_mcp.transcript_cache import TranscriptCache
from devlake_mcp.git_utils import get_git_info
import os

logger = logging.getLogger(__name__)


@dataclass
class TranscriptMetadata:
    """Transcript å…ƒæ•°æ®"""

    file_path: Path
    session_id: str
    start_time: Optional[datetime]
    is_ended: bool
    should_upload: bool
    upload_source: Optional[str]
    skip_reason: Optional[str] = None


@dataclass
class SyncReport:
    """åŒæ­¥æŠ¥å‘Š"""

    total_scanned: int = 0  # æ‰«æçš„æ–‡ä»¶æ€»æ•°
    skipped_excluded: int = 0  # è·³è¿‡ï¼ˆæ–‡ä»¶åè¢«æ’é™¤ï¼‰
    skipped_cached: int = 0  # è·³è¿‡ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
    skipped_ongoing: int = 0  # è·³è¿‡ï¼ˆæ­£åœ¨è¿›è¡Œä¸­çš„ä¼šè¯ï¼‰
    skipped_server_exists: int = 0  # è·³è¿‡ï¼ˆæœåŠ¡ç«¯å·²å­˜åœ¨ï¼‰
    skipped_error: int = 0  # è·³è¿‡ï¼ˆè§£æé”™è¯¯ï¼‰
    uploaded_success: int = 0  # ä¸Šä¼ æˆåŠŸ
    uploaded_failed: int = 0  # ä¸Šä¼ å¤±è´¥

    def get_summary(self) -> str:
        """è·å–æ‘˜è¦æ–‡æœ¬"""
        return f"""
ğŸ“Š åŒæ­¥ç»Ÿè®¡:
  â€¢ æ‰«ææ–‡ä»¶: {self.total_scanned} ä¸ª
  â€¢ è·³è¿‡æ’é™¤: {self.skipped_excluded} ä¸ª
  â€¢ è·³è¿‡ç¼“å­˜: {self.skipped_cached} ä¸ª
  â€¢ è·³è¿‡è¿›è¡Œä¸­: {self.skipped_ongoing} ä¸ª
  â€¢ è·³è¿‡å·²å­˜åœ¨: {self.skipped_server_exists} ä¸ª
  â€¢ è·³è¿‡é”™è¯¯: {self.skipped_error} ä¸ª
  âœ… ä¸Šä¼ æˆåŠŸ: {self.uploaded_success} ä¸ª
  âŒ ä¸Šä¼ å¤±è´¥: {self.uploaded_failed} ä¸ª
"""


def scan_claude_projects_dir() -> List[Path]:
    """
    æ‰«æ Claude Code projects ç›®å½•ä¸‹çš„æ‰€æœ‰ jsonl æ–‡ä»¶

    æ‰«æè·¯å¾„ï¼š~/.claude/projects* ï¼ˆåŒ…å«æ‰€æœ‰å­ç›®å½•ï¼‰
    æ’é™¤è§„åˆ™ï¼šæ–‡ä»¶åä»¥ 'agent-' å¼€å¤´çš„æ–‡ä»¶

    Returns:
        æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ jsonl æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    logger.info("å¼€å§‹æ‰«æ Claude Code projects ç›®å½•...")

    # å±•å¼€è·¯å¾„æ¨¡å¼
    pattern_path = Path(CLAUDE_PROJECTS_DIR_PATTERN.replace('~', str(Path.home())))
    base_pattern = str(pattern_path)

    # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„ç›®å½•
    matching_dirs = glob.glob(base_pattern)

    if not matching_dirs:
        logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„ç›®å½•: {base_pattern}")
        return []

    logger.debug(f"æ‰¾åˆ° {len(matching_dirs)} ä¸ª projects ç›®å½•")

    # æ‰«ææ‰€æœ‰ç›®å½•ä¸‹çš„ jsonl æ–‡ä»¶
    jsonl_files = []
    for project_dir in matching_dirs:
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ .jsonl æ–‡ä»¶
        pattern = str(Path(project_dir) / '**' / '*.jsonl')
        files = glob.glob(pattern, recursive=True)

        for file_path in files:
            file_name = Path(file_path).name

            # æ’é™¤ agent-*.jsonl
            if file_name.startswith(EXCLUDED_TRANSCRIPT_PREFIX):
                logger.debug(f"æ’é™¤æ–‡ä»¶: {file_name}")
                continue

            jsonl_files.append(Path(file_path))

    logger.info(f"æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(jsonl_files)} ä¸ª transcript æ–‡ä»¶")
    return jsonl_files


def should_upload_transcript(metadata: TranscriptMetadata, zombie_hours: int) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸Šä¼  transcript

    è¿‡æ»¤è§„åˆ™ï¼š
    1. ä¼šè¯å·²ç»“æŸ â†’ ä¸Šä¼ 
    2. ä¼šè¯æœªç»“æŸ + å¼€å§‹æ—¶é—´ > zombie_hours â†’ ä¸Šä¼ ï¼ˆåƒµå°¸ä¼šè¯ï¼‰
    3. ä¼šè¯æœªç»“æŸ + å¼€å§‹æ—¶é—´ â‰¤ zombie_hours â†’ è·³è¿‡

    Args:
        metadata: Transcript å…ƒæ•°æ®
        zombie_hours: åƒµå°¸ä¼šè¯é˜ˆå€¼ï¼ˆå°æ—¶ï¼‰

    Returns:
        True è¡¨ç¤ºåº”è¯¥ä¸Šä¼ ï¼ŒFalse è¡¨ç¤ºè·³è¿‡
    """
    # è§„åˆ™ 1: ä¼šè¯å·²ç»“æŸ
    if metadata.is_ended:
        metadata.should_upload = True
        metadata.upload_source = UPLOAD_SOURCE_MANUAL
        logger.debug(f"ä¼šè¯å·²ç»“æŸï¼Œåº”è¯¥ä¸Šä¼ : {metadata.session_id}")
        return True

    # è§„åˆ™ 2 & 3: ä¼šè¯æœªç»“æŸï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯åƒµå°¸ä¼šè¯
    if metadata.start_time is None:
        logger.warning(f"æ— æ³•è·å–ä¼šè¯å¼€å§‹æ—¶é—´ï¼Œè·³è¿‡: {metadata.session_id}")
        metadata.should_upload = False
        metadata.skip_reason = "æ— æ³•è·å–å¼€å§‹æ—¶é—´"
        return False

    # è®¡ç®—ä¼šè¯å·²æŒç»­æ—¶é—´
    now = datetime.now(timezone.utc)
    elapsed_hours = (now - metadata.start_time).total_seconds() / 3600

    if elapsed_hours > zombie_hours:
        # åƒµå°¸ä¼šè¯
        metadata.should_upload = True
        metadata.upload_source = UPLOAD_SOURCE_AUTO_BACKFILL
        logger.debug(
            f"åƒµå°¸ä¼šè¯ï¼ˆå·²æŒç»­ {elapsed_hours:.1f} å°æ—¶ï¼‰ï¼Œåº”è¯¥ä¸Šä¼ : {metadata.session_id}"
        )
        return True
    else:
        # æ­£åœ¨è¿›è¡Œä¸­
        metadata.should_upload = False
        metadata.skip_reason = f"æ­£åœ¨è¿›è¡Œä¸­ï¼ˆå·²æŒç»­ {elapsed_hours:.1f} å°æ—¶ï¼‰"
        logger.debug(f"ä¼šè¯æ­£åœ¨è¿›è¡Œä¸­ï¼Œè·³è¿‡: {metadata.session_id}")
        return False


def upload_single_transcript(
    file_path: Path,
    session_id: str,
    upload_source: str,
    client: DevLakeClient,
    cache: TranscriptCache,
) -> bool:
    """
    ä¸Šä¼ å•ä¸ª transcript

    Args:
        file_path: Transcript æ–‡ä»¶è·¯å¾„
        session_id: ä¼šè¯ ID
        upload_source: ä¸Šä¼ æ¥æºï¼ˆauto/auto_backfill/manualï¼‰
        client: API å®¢æˆ·ç«¯
        cache: ç¼“å­˜ç®¡ç†å™¨

    Returns:
        True è¡¨ç¤ºä¸Šä¼ æˆåŠŸï¼ŒFalse è¡¨ç¤ºå¤±è´¥
    """
    try:
        logger.info(f"å¼€å§‹ä¸Šä¼  transcript: {session_id}")

        # 1. è¯»å– transcript å†…å®¹
        transcript_content = read_transcript_content(str(file_path))
        if not transcript_content:
            logger.error(f"è¯»å– transcript å†…å®¹å¤±è´¥: {file_path}")
            return False

        # 2. å‹ç¼©å†…å®¹
        compression_result = compress_transcript_content(transcript_content)

        # 3. ç»Ÿè®¡æ¶ˆæ¯æ•°é‡
        message_count = count_user_messages(str(file_path))

        # 4. è·å– git ç”¨æˆ·ä¿¡æ¯
        cwd = os.getcwd()
        git_info = get_git_info(cwd, include_user_info=True)
        git_author = git_info.get('git_author')
        git_email = git_info.get('git_email')
        # å¦‚æœè·å–å¤±è´¥æˆ–ä¸º 'unknown',åˆ™è®¾ä¸º None
        if git_author == 'unknown':
            git_author = None
        if git_email == 'unknown':
            git_email = None

        # 5. æ„å»ºä¸Šä¼ æ•°æ®
        upload_data = {
            'session_id': session_id,
            'transcript_path': str(file_path),
            'transcript_content': compression_result['content'],
            'compression': compression_result['compression'],
            'original_size': compression_result['original_size'],
            'compressed_size': compression_result['compressed_size'],
            'compression_ratio': compression_result['compression_ratio'],
            'message_count': message_count,
            'upload_time': datetime.now().isoformat(),
            'upload_source': upload_source,
            'git_author': git_author,
            'git_email': git_email,
        }

        # 5. è°ƒç”¨ API ä¸Šä¼ 
        response = client.create_transcript(upload_data)

        if response.get('success'):
            logger.info(f"ä¸Šä¼ æˆåŠŸ: {session_id}")
            # æ·»åŠ åˆ°ç¼“å­˜
            cache.add(session_id)
            return True
        else:
            error_msg = response.get('message', 'æœªçŸ¥é”™è¯¯')
            logger.error(f"ä¸Šä¼ å¤±è´¥: {session_id} - {error_msg}")
            # ä¿å­˜åˆ°é‡è¯•é˜Ÿåˆ—
            save_failed_upload(
                queue_type='transcript',
                api_endpoint='/api/ai-coding/transcripts',
                data=upload_data,
                error=error_msg,
            )
            return False

    except Exception as e:
        logger.error(f"ä¸Šä¼  transcript å¼‚å¸¸: {session_id} - {e}", exc_info=True)
        # ä¿å­˜åˆ°é‡è¯•é˜Ÿåˆ—
        save_failed_upload(
            queue_type='transcript',
            api_endpoint='/api/ai-coding/transcripts',
            data={
                'session_id': session_id,
                'transcript_path': str(file_path),
                'upload_source': upload_source,
            },
            error=str(e),
        )
        return False


def scan_local_transcripts(
    cache: TranscriptCache,
    client: DevLakeClient,
    check_server: bool = False,
    force: bool = False,
    session_id_filter: Optional[str] = None,
    dry_run: bool = False,
) -> SyncReport:
    """
    æ‰«æå¹¶ä¸Šä¼ æœ¬åœ° transcript

    Args:
        cache: ç¼“å­˜å®ä¾‹
        client: API å®¢æˆ·ç«¯
        check_server: æ˜¯å¦å‘æœåŠ¡ç«¯æ£€æŸ¥ï¼ˆFalse æ—¶åªä¾èµ–ç¼“å­˜ï¼‰
        force: å¼ºåˆ¶ä¸Šä¼ ï¼Œå¿½ç•¥ç¼“å­˜
        session_id_filter: åªå¤„ç†ç‰¹å®š session_id
        dry_run: é¢„è§ˆæ¨¡å¼ï¼Œåªæ‰«æä¸ä¸Šä¼ 

    Returns:
        åŒæ­¥æŠ¥å‘Š
    """
    report = SyncReport()
    zombie_hours = get_zombie_session_hours()

    logger.info("=" * 60)
    logger.info("å¼€å§‹æ‰«ææœ¬åœ° transcript æ–‡ä»¶")
    logger.info(f"é…ç½®: check_server={check_server}, force={force}, dry_run={dry_run}")
    logger.info(f"åƒµå°¸ä¼šè¯é˜ˆå€¼: {zombie_hours} å°æ—¶")
    logger.info("=" * 60)

    # 1. æ‰«ææ–‡ä»¶
    jsonl_files = scan_claude_projects_dir()
    report.total_scanned = len(jsonl_files)

    if report.total_scanned == 0:
        logger.info("æœªæ‰¾åˆ°ä»»ä½• transcript æ–‡ä»¶")
        return report

    # 2. éå†æ–‡ä»¶ï¼Œæå–å…ƒæ•°æ®å¹¶è¿‡æ»¤
    total_files = len(jsonl_files)
    for idx, file_path in enumerate(jsonl_files, start=1):
        try:
            # æ˜¾ç¤ºæ•´ä½“è¿›åº¦
            print(f"ğŸ“„ å¤„ç†ä¸­ ({idx}/{total_files}): {file_path.name}")

            # 2.1 æå– session_id
            session_id = extract_session_id(str(file_path))
            if not session_id:
                logger.warning(f"æ— æ³•æå– session_idï¼Œè·³è¿‡: {file_path}")
                print(f"   âš ï¸  æ— æ³•æå– session_idï¼Œå·²è·³è¿‡")
                report.skipped_error += 1
                continue

            # 2.2 ä¼šè¯è¿‡æ»¤
            if session_id_filter and session_id != session_id_filter:
                continue

            # 2.3 æ£€æŸ¥ç¼“å­˜ï¼ˆé™¤é forceï¼‰
            if not force and cache.is_uploaded(session_id):
                logger.debug(f"ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡: {session_id}")
                print(f"   â­ï¸  ç¼“å­˜å‘½ä¸­ï¼Œå·²è·³è¿‡")
                report.skipped_cached += 1
                continue

            # 2.4 æå–å…ƒæ•°æ®
            start_time = get_session_start_time(str(file_path))
            is_ended = is_session_ended(str(file_path))

            metadata = TranscriptMetadata(
                file_path=file_path,
                session_id=session_id,
                start_time=start_time,
                is_ended=is_ended,
                should_upload=False,
                upload_source=None,
            )

            # 2.5 åº”ç”¨è¿‡æ»¤è§„åˆ™
            if not should_upload_transcript(metadata, zombie_hours):
                if metadata.skip_reason:
                    logger.debug(f"è·³è¿‡: {session_id} - {metadata.skip_reason}")
                    print(f"   â­ï¸  {metadata.skip_reason}")
                report.skipped_ongoing += 1
                continue

            # 2.6 æ£€æŸ¥æœåŠ¡ç«¯æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if check_server:
                print(f"   ğŸŒ æ­£åœ¨æ£€æŸ¥æœåŠ¡ç«¯: {session_id[:8]}...")
                try:
                    exists = client.check_transcript_exists(session_id)
                    if exists:
                        logger.info(f"æœåŠ¡ç«¯å·²å­˜åœ¨ï¼Œè·³è¿‡: {session_id}")
                        print(f"   âœ… æœåŠ¡ç«¯å·²å­˜åœ¨ï¼Œå·²è·³è¿‡")
                        report.skipped_server_exists += 1
                        # æ·»åŠ åˆ°ç¼“å­˜ï¼ˆé¿å…ä¸‹æ¬¡å†æ£€æŸ¥ï¼‰
                        cache.add(session_id)
                        continue
                    else:
                        print(f"   ğŸ“¤ æœåŠ¡ç«¯ä¸å­˜åœ¨ï¼Œå‡†å¤‡ä¸Šä¼ ")
                except Exception as e:
                    logger.warning(f"æ£€æŸ¥æœåŠ¡ç«¯å¤±è´¥: {session_id} - {e}")
                    print(f"   âš ï¸  æ£€æŸ¥æœåŠ¡ç«¯å¤±è´¥: {e}ï¼Œç»§ç»­ä¸Šä¼ ")
                    # ç»§ç»­æ‰§è¡Œä¸Šä¼ 

            # 2.7 ä¸Šä¼  transcript
            if dry_run:
                logger.info(f"[DRY RUN] å°†ä¸Šä¼ : {session_id} ({metadata.upload_source})")
                print(f"   ğŸ” [é¢„è§ˆ] å°†ä¸Šä¼  ({metadata.upload_source})")
                report.uploaded_success += 1
            else:
                print(f"   ğŸ“¤ å¼€å§‹ä¸Šä¼ : {session_id[:8]}...")
                success = upload_single_transcript(
                    file_path=file_path,
                    session_id=session_id,
                    upload_source=metadata.upload_source,
                    client=client,
                    cache=cache,
                )

                if success:
                    print(f"   âœ… ä¸Šä¼ æˆåŠŸ")
                    report.uploaded_success += 1
                else:
                    print(f"   âŒ ä¸Šä¼ å¤±è´¥")
                    report.uploaded_failed += 1

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path} - {e}", exc_info=True)
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
            report.skipped_error += 1

    logger.info("=" * 60)
    logger.info("æ‰«æå®Œæˆ")
    logger.info(report.get_summary())
    logger.info("=" * 60)

    return report
