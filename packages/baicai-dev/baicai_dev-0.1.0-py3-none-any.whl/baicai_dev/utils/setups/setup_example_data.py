# Baseline Builder
BASELINE_CODES_HOUSE = [
    {
        "code": '\nimport warnings\nfrom datetime import datetime\nfrom pathlib import Path\nfrom joblib import dump\n\nimport numpy as np\nimport pandas as pd\nfrom fastai.tabular.all import (\n    Categorify,\n    FillMissing,\n    TabularDataLoaders,\n    cont_cat_split,\n    add_datepart,\n)\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import accuracy_score, r2_score\n\nfrom baicai_base.utils.data import load_data, time_series_train_condition\n\n# 加载数据并进行预处理\ndf = load_data(r"D:\\programming\\baicai\\examples\\data\\house.csv", delimiter=",")\n\n# 忽略特征\ndf = df.drop(columns=[])\n\n# 时间序列按照年月日等划分\nsplits = None\nif "" != "":\n    df = add_datepart(df, "", time=False)\n\n    if False:\n        cond = time_series_train_condition(df,\n                                          {},\n                                          {})\n        train_index = df[cond].index\n        valid_index = df[~cond].index\n        splits = (list(train_index), list(valid_index))\n        \n\n# 有序特征\nordinal_features = []\nif len(ordinal_features) > 0:\n    for feature_dict in ordinal_features:\n        for ordinal_feature, order in feature_dict.items():\n            df[ordinal_feature] = df[ordinal_feature].astype("category").cat.set_categories(order, ordered=True)\n\nif not False:\n    # 如果是回归任务，对目标变量进行对数变换以处理偏态\n    df["MedHouseVal"] = np.log1p(df["MedHouseVal"])\n\n# 自动识别连续变量和分类变量\ncont, cat = cont_cat_split(df, dep_var="MedHouseVal")\n\n\n\n# 创建 TabularDataLoaders，用于数据加载和预处理\ndls = TabularDataLoaders.from_df(\n    df,\n    y_names="MedHouseVal",  # 指定目标变量\n    cont_names=cont,  # 指定连续变量\n    cat_names=cat,   # 指定分类变量\n    procs=[\n        Categorify,  # 将分类变量转换为数值编码\n        FillMissing, # 处理缺失值\n    ],\n    splits=splits,\n)\n\n# 根据任务类型选择评估指标和模型\nif False:\n    metrics = accuracy_score\n    RandomForest = RandomForestClassifier  # noqa: N806\nelse:\n    metrics = r2_score\n    RandomForest = RandomForestRegressor  # noqa: N806\n\n# 初始化随机森林模型\nrf_model = RandomForest(n_estimators=10, random_state=42, max_depth=5, min_samples_leaf=20)\n\n# 训练随机森林模型\nrf_model.fit(dls.train.dataset.xs, dls.train.dataset.y)\n\n# 评估模型性能\ntrain_score = metrics(dls.train.dataset.y, rf_model.predict(dls.train.dataset.xs))\nvalid_score = metrics(dls.valid.dataset.y, rf_model.predict(dls.valid.dataset.xs))\n\n# 打印评估结果\nprint(f"Training {metrics.__name__}: {train_score}")\nprint(f"Validation {metrics.__name__}: {valid_score}")\n\n# 特征重要性分析\nfeature_names = dls.train.dataset.xs.columns\n# 计算训练集的特征重要性\ntrain_permutation_importance = permutation_importance(rf_model, dls.train.dataset.xs, dls.train.dataset.y, n_repeats=10, random_state=42)\n# 计算验证集的特征重要性\nvalid_permutation_importance = permutation_importance(rf_model, dls.valid.dataset.xs, dls.valid.dataset.y, n_repeats=10, random_state=42)\n\n# 创建特征重要性数据框\ntrain_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": np.round(train_permutation_importance.importances_mean, 4)})\nvalid_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": np.round(valid_permutation_importance.importances_mean, 4)})\n\n# 打印特征重要性结果\nprint("Training Set Feature Importance:")\nprint(train_importance_df.sort_values("Importance", ascending=False).to_string(index=False))\nprint("Validation Set Feature Importance:")\nprint(valid_importance_df.sort_values("Importance", ascending=False).to_string(index=False))\n\n# 保存结果\n\n# 生成时间戳\ntimestamp = datetime.now().strftime("%Y%m%d-%H%M%S")\n\n# 文件夹名在当前用户路径，如果没有则创建\ndata_folder = Path.home() / ".baicai" / "tmp"/ "data" / "house"\nif not data_folder.exists():\n    data_folder.mkdir(parents=True, exist_ok=True)\n\n# 模型文件夹名在当前用户路径，如果没有则创建\nmodel_folder = Path.home() / ".baicai" / "tmp"/ "models" / "house"\nif not model_folder.exists():\n    model_folder.mkdir(parents=True, exist_ok=True)\n\n# 生成带时间戳的文件名\nsaved_file_name = data_folder / f"baseline_{timestamp}.pkl"\n\nmodel_path = model_folder / f"baseline_{timestamp}.pkl"\ndump(rf_model, model_path)\nprint(f"The model is saved to {model_path}")\n\n# 将数据保存为pickle文件\npd.to_pickle(dls, saved_file_name)\nprint(f"The pickled data is saved to {saved_file_name}")\n',
        "success": True,
        "result": "Training r2_score: 0.6729263271730264\nValidation r2_score: 0.6459683128806339\n,Training Set Feature Importance:\n   Feature  Importance\n    MedInc      1.0930\n  AveOccup      0.1433\n  Latitude      0.0769\n Longitude      0.0461\n  AveRooms      0.0439\n  HouseAge      0.0275\nPopulation      0.0004\n AveBedrms      0.0002\nValidation Set Feature Importance:\n   Feature  Importance\n    MedInc      1.0642\n  AveOccup      0.1322\n  Latitude      0.0787\n Longitude      0.0469\n  AveRooms      0.0409\n  HouseAge      0.0239\nPopulation      0.0005\n AveBedrms      0.0001\nThe model is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\house\\baseline_20250419-155204.pkl\nThe pickled data is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\data\\house\\baseline_20250419-155204.pkl\n",
        "error": "",
    }
]

BASELINE_CODES_IRIS = [
    {
        "code": '\nimport warnings\nfrom datetime import datetime\nfrom pathlib import Path\nfrom joblib import dump\n\nimport numpy as np\nimport pandas as pd\nfrom fastai.tabular.all import (\n    Categorify,\n    FillMissing,\n    TabularDataLoaders,\n    cont_cat_split,\n    add_datepart,\n)\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import accuracy_score, r2_score\n\nfrom baicai_base.utils.data import load_data, time_series_train_condition\n\n# 加载数据并进行预处理\ndf = load_data(r"D:\\programming\\baicai\\examples\\data\\iris.csv", delimiter=",")\n\n# 忽略特征\ndf = df.drop(columns=[])\n\n# 时间序列按照年月日等划分\nsplits = None\nif "" != "":\n    df = add_datepart(df, "", time=False)\n\n    if False:\n        cond = time_series_train_condition(df,\n                                          {},\n                                          {})\n        train_index = df[cond].index\n        valid_index = df[~cond].index\n        splits = (list(train_index), list(valid_index))\n        \n\n# 有序特征\nordinal_features = []\nif len(ordinal_features) > 0:\n    for feature_dict in ordinal_features:\n        for ordinal_feature, order in feature_dict.items():\n            df[ordinal_feature] = df[ordinal_feature].astype("category").cat.set_categories(order, ordered=True)\n\nif not True:\n    # 如果是回归任务，对目标变量进行对数变换以处理偏态\n    df["iris"] = np.log1p(df["iris"])\n\n# 自动识别连续变量和分类变量\ncont, cat = cont_cat_split(df, dep_var="iris")\n\n\n\n# 创建 TabularDataLoaders，用于数据加载和预处理\ndls = TabularDataLoaders.from_df(\n    df,\n    y_names="iris",  # 指定目标变量\n    cont_names=cont,  # 指定连续变量\n    cat_names=cat,   # 指定分类变量\n    procs=[\n        Categorify,  # 将分类变量转换为数值编码\n        FillMissing, # 处理缺失值\n    ],\n    splits=splits,\n)\n\n# 根据任务类型选择评估指标和模型\nif True:\n    metrics = accuracy_score\n    RandomForest = RandomForestClassifier  # noqa: N806\nelse:\n    metrics = r2_score\n    RandomForest = RandomForestRegressor  # noqa: N806\n\n# 初始化随机森林模型\nrf_model = RandomForest(n_estimators=10, random_state=42, max_depth=5, min_samples_leaf=20)\n\n# 训练随机森林模型\nrf_model.fit(dls.train.dataset.xs, dls.train.dataset.y)\n\n# 评估模型性能\ntrain_score = metrics(dls.train.dataset.y, rf_model.predict(dls.train.dataset.xs))\nvalid_score = metrics(dls.valid.dataset.y, rf_model.predict(dls.valid.dataset.xs))\n\n# 打印评估结果\nprint(f"Training {metrics.__name__}: {train_score}")\nprint(f"Validation {metrics.__name__}: {valid_score}")\n\n# 特征重要性分析\nfeature_names = dls.train.dataset.xs.columns\n# 计算训练集的特征重要性\ntrain_permutation_importance = permutation_importance(rf_model, dls.train.dataset.xs, dls.train.dataset.y, n_repeats=10, random_state=42)\n# 计算验证集的特征重要性\nvalid_permutation_importance = permutation_importance(rf_model, dls.valid.dataset.xs, dls.valid.dataset.y, n_repeats=10, random_state=42)\n\n# 创建特征重要性数据框\ntrain_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": np.round(train_permutation_importance.importances_mean, 4)})\nvalid_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": np.round(valid_permutation_importance.importances_mean, 4)})\n\n# 打印特征重要性结果\nprint("Training Set Feature Importance:")\nprint(train_importance_df.sort_values("Importance", ascending=False).to_string(index=False))\nprint("Validation Set Feature Importance:")\nprint(valid_importance_df.sort_values("Importance", ascending=False).to_string(index=False))\n\n# 保存结果\n\n# 生成时间戳\ntimestamp = datetime.now().strftime("%Y%m%d-%H%M%S")\n\n# 文件夹名在当前用户路径，如果没有则创建\ndata_folder = Path.home() / ".baicai" / "tmp"/ "data" / "iris"\nif not data_folder.exists():\n    data_folder.mkdir(parents=True, exist_ok=True)\n\n# 模型文件夹名在当前用户路径，如果没有则创建\nmodel_folder = Path.home() / ".baicai" / "tmp"/ "models" / "iris"\nif not model_folder.exists():\n    model_folder.mkdir(parents=True, exist_ok=True)\n\n# 生成带时间戳的文件名\nsaved_file_name = data_folder / f"baseline_{timestamp}.pkl"\n\nmodel_path = model_folder / f"baseline_{timestamp}.pkl"\ndump(rf_model, model_path)\nprint(f"The model is saved to {model_path}")\n\n# 将数据保存为pickle文件\npd.to_pickle(dls, saved_file_name)\nprint(f"The pickled data is saved to {saved_file_name}")\n',
        "success": True,
        "result": "Training accuracy_score: 0.9583333333333334\nValidation accuracy_score: 0.9666666666666667\nTraining Set Feature Importance:\n          Feature  Importance\npetal length (cm)       0.275\n petal width (cm)       0.240\nsepal length (cm)       0.000\n sepal width (cm)       0.000\nValidation Set Feature Importance:\n          Feature  Importance\npetal length (cm)      0.3033\n petal width (cm)      0.2200\nsepal length (cm)      0.0000\n sepal width (cm)      0.0000\nThe model is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\iris\\baseline_20250419-163142.pkl\nThe pickled data is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\data\\iris\\baseline_20250419-163142.pkl\n",
        "error": "",
    }
]

BASELINE_CODES_TITANIC = [
    {
        "code": '\nimport warnings\nfrom datetime import datetime\nfrom pathlib import Path\nfrom joblib import dump\n\nimport numpy as np\nimport pandas as pd\nfrom fastai.tabular.all import (\n    Categorify,\n    FillMissing,\n    TabularDataLoaders,\n    cont_cat_split,\n    add_datepart,\n)\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import accuracy_score, r2_score\n\nfrom baicai_base.utils.data import load_data, time_series_train_condition\n\n# 加载数据并进行预处理\ndf = load_data(r"D:\\programming\\baicai\\examples\\data\\titanic.csv", delimiter=",")\n\n# 忽略特征\ndf = df.drop(columns=[\'deck\', \'alive\'])\n\n# 时间序列按照年月日等划分\nsplits = None\nif "" != "":\n    df = add_datepart(df, "", time=False)\n\n    if False:\n        cond = time_series_train_condition(df,\n                                          {},\n                                          {})\n        train_index = df[cond].index\n        valid_index = df[~cond].index\n        splits = (list(train_index), list(valid_index))\n        \n\n# 有序特征\nordinal_features = []\nif len(ordinal_features) > 0:\n    for feature_dict in ordinal_features:\n        for ordinal_feature, order in feature_dict.items():\n            df[ordinal_feature] = df[ordinal_feature].astype("category").cat.set_categories(order, ordered=True)\n\nif not True:\n    # 如果是回归任务，对目标变量进行对数变换以处理偏态\n    df["survived"] = np.log1p(df["survived"])\n\n# 自动识别连续变量和分类变量\ncont, cat = cont_cat_split(df, dep_var="survived")\n\n\n\n# 创建 TabularDataLoaders，用于数据加载和预处理\ndls = TabularDataLoaders.from_df(\n    df,\n    y_names="survived",  # 指定目标变量\n    cont_names=cont,  # 指定连续变量\n    cat_names=cat,   # 指定分类变量\n    procs=[\n        Categorify,  # 将分类变量转换为数值编码\n        FillMissing, # 处理缺失值\n    ],\n    splits=splits,\n)\n\n# 根据任务类型选择评估指标和模型\nif True:\n    metrics = accuracy_score\n    RandomForest = RandomForestClassifier  # noqa: N806\nelse:\n    metrics = r2_score\n    RandomForest = RandomForestRegressor  # noqa: N806\n\n# 初始化随机森林模型\nrf_model = RandomForest(n_estimators=10, random_state=42, max_depth=5, min_samples_leaf=20)\n\n# 训练随机森林模型\nrf_model.fit(dls.train.dataset.xs, dls.train.dataset.y)\n\n# 评估模型性能\ntrain_score = metrics(dls.train.dataset.y, rf_model.predict(dls.train.dataset.xs))\nvalid_score = metrics(dls.valid.dataset.y, rf_model.predict(dls.valid.dataset.xs))\n\n# 打印评估结果\nprint(f"Training {metrics.__name__}: {train_score}")\nprint(f"Validation {metrics.__name__}: {valid_score}")\n\n# 特征重要性分析\nfeature_names = dls.train.dataset.xs.columns\n# 计算训练集的特征重要性\ntrain_permutation_importance = permutation_importance(rf_model, dls.train.dataset.xs, dls.train.dataset.y, n_repeats=10, random_state=42)\n# 计算验证集的特征重要性\nvalid_permutation_importance = permutation_importance(rf_model, dls.valid.dataset.xs, dls.valid.dataset.y, n_repeats=10, random_state=42)\n\n# 创建特征重要性数据框\ntrain_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": np.round(train_permutation_importance.importances_mean, 4)})\nvalid_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": np.round(valid_permutation_importance.importances_mean, 4)})\n\n# 打印特征重要性结果\nprint("Training Set Feature Importance:")\nprint(train_importance_df.sort_values("Importance", ascending=False).to_string(index=False))\nprint("Validation Set Feature Importance:")\nprint(valid_importance_df.sort_values("Importance", ascending=False).to_string(index=False))\n\n# 保存结果\n\n# 生成时间戳\ntimestamp = datetime.now().strftime("%Y%m%d-%H%M%S")\n\n# 文件夹名在当前用户路径，如果没有则创建\ndata_folder = Path.home() / ".baicai" / "tmp"/ "data" / "titanic"\nif not data_folder.exists():\n    data_folder.mkdir(parents=True, exist_ok=True)\n\n# 模型文件夹名在当前用户路径，如果没有则创建\nmodel_folder = Path.home() / ".baicai" / "tmp"/ "models" / "titanic"\nif not model_folder.exists():\n    model_folder.mkdir(parents=True, exist_ok=True)\n\n# 生成带时间戳的文件名\nsaved_file_name = data_folder / f"baseline_{timestamp}.pkl"\n\nmodel_path = model_folder / f"baseline_{timestamp}.pkl"\ndump(rf_model, model_path)\nprint(f"The model is saved to {model_path}")\n\n# 将数据保存为pickle文件\npd.to_pickle(dls, saved_file_name)\nprint(f"The pickled data is saved to {saved_file_name}")\n',
        "success": True,
        "result": "c:\\Users\\gengyabc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\baicai-6ddbNBXo-py3.12\\Lib\\site-packages\\fastai\\tabular\\core.py:314: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  to[n].fillna(self.na_dict[n], inplace=True)\n,Training accuracy_score: 0.8373071528751753\nValidation accuracy_score: 0.7808988764044944\n,Training Set Feature Importance:\n    Feature  Importance\n adult_male      0.0993\n      sibsp      0.0125\n        sex      0.0115\n      class      0.0097\n        age      0.0060\n     pclass      0.0049\n   embarked      0.0022\nembark_town      0.0013\n      parch      0.0000\n        who      0.0000\n      alone      0.0000\n     age_na     -0.0004\n       fare     -0.0011\nValidation Set Feature Importance:\n    Feature  Importance\n adult_male      0.0753\n   embarked      0.0096\n        who      0.0096\nembark_town      0.0090\n     pclass      0.0051\n      class      0.0051\n        sex      0.0034\n      parch      0.0000\n      alone      0.0000\n     age_na      0.0000\n       fare     -0.0006\n      sibsp     -0.0039\n        age     -0.0051\nThe model is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\titanic\\baseline_20250419-163212.pkl\nThe pickled data is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\data\\titanic\\baseline_20250419-163212.pkl\n",
        "error": "",
    }
]

BASELINE_CODES_GARMENT = [
    {
        "code": '\nimport warnings\nfrom datetime import datetime\nfrom pathlib import Path\nfrom joblib import dump\n\nimport numpy as np\nimport pandas as pd\nfrom fastai.tabular.all import (\n    Categorify,\n    FillMissing,\n    TabularDataLoaders,\n    cont_cat_split,\n    add_datepart,\n)\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import accuracy_score, r2_score\n\nfrom baicai_base.utils.data import load_data, time_series_train_condition\n\n# 加载数据并进行预处理\ndf = load_data(r"D:\\programming\\baicai\\examples\\data\\garment.csv", delimiter=",")\n\n# 忽略特征\ndf = df.drop(columns=[])\n\n# 时间序列按照年月日等划分\nsplits = None\nif "date" != "":\n    df = add_datepart(df, "date", time=False)\n\n    if True:\n        cond = time_series_train_condition(df,\n                                          {\'year\': \'Year\', \'month\': \'Month\', \'day\': \'Day\'},\n                                          {\'year\': 2015, \'month\': 9, \'day\': 1})\n        train_index = df[cond].index\n        valid_index = df[~cond].index\n        splits = (list(train_index), list(valid_index))\n        \n\n# 有序特征\nordinal_features = [{\'quarter\': [\'Quarter1\', \'Quarter2\', \'Quarter3\', \'Quarter4\', \'Quarter5\']}, {\'day\': [\'Monday\', \'Tuesday\', \'Wednesday\', \'Thursday\', \'Friday\', \'Saturday\', \'Sunday\']}]\nif len(ordinal_features) > 0:\n    for feature_dict in ordinal_features:\n        for ordinal_feature, order in feature_dict.items():\n            df[ordinal_feature] = df[ordinal_feature].astype("category").cat.set_categories(order, ordered=True)\n\nif not False:\n    # 如果是回归任务，对目标变量进行对数变换以处理偏态\n    df["actual_productivity"] = np.log1p(df["actual_productivity"])\n\n# 自动识别连续变量和分类变量\ncont, cat = cont_cat_split(df, dep_var="actual_productivity")\n\n\n\n# 创建 TabularDataLoaders，用于数据加载和预处理\ndls = TabularDataLoaders.from_df(\n    df,\n    y_names="actual_productivity",  # 指定目标变量\n    cont_names=cont,  # 指定连续变量\n    cat_names=cat,   # 指定分类变量\n    procs=[\n        Categorify,  # 将分类变量转换为数值编码\n        FillMissing, # 处理缺失值\n    ],\n    splits=splits,\n)\n\n# 根据任务类型选择评估指标和模型\nif False:\n    metrics = accuracy_score\n    RandomForest = RandomForestClassifier  # noqa: N806\nelse:\n    metrics = r2_score\n    RandomForest = RandomForestRegressor  # noqa: N806\n\n# 初始化随机森林模型\nrf_model = RandomForest(n_estimators=10, random_state=42, max_depth=5, min_samples_leaf=20)\n\n# 训练随机森林模型\nrf_model.fit(dls.train.dataset.xs, dls.train.dataset.y)\n\n# 评估模型性能\ntrain_score = metrics(dls.train.dataset.y, rf_model.predict(dls.train.dataset.xs))\nvalid_score = metrics(dls.valid.dataset.y, rf_model.predict(dls.valid.dataset.xs))\n\n# 打印评估结果\nprint(f"Training {metrics.__name__}: {train_score}")\nprint(f"Validation {metrics.__name__}: {valid_score}")\n\n# 特征重要性分析\nfeature_names = dls.train.dataset.xs.columns\n# 计算训练集的特征重要性\ntrain_permutation_importance = permutation_importance(rf_model, dls.train.dataset.xs, dls.train.dataset.y, n_repeats=10, random_state=42)\n# 计算验证集的特征重要性\nvalid_permutation_importance = permutation_importance(rf_model, dls.valid.dataset.xs, dls.valid.dataset.y, n_repeats=10, random_state=42)\n\n# 创建特征重要性数据框\ntrain_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": np.round(train_permutation_importance.importances_mean, 4)})\nvalid_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": np.round(valid_permutation_importance.importances_mean, 4)})\n\n# 打印特征重要性结果\nprint("Training Set Feature Importance:")\nprint(train_importance_df.sort_values("Importance", ascending=False).to_string(index=False))\nprint("Validation Set Feature Importance:")\nprint(valid_importance_df.sort_values("Importance", ascending=False).to_string(index=False))\n\n# 保存结果\n\n# 生成时间戳\ntimestamp = datetime.now().strftime("%Y%m%d-%H%M%S")\n\n# 文件夹名在当前用户路径，如果没有则创建\ndata_folder = Path.home() / ".baicai" / "tmp"/ "data" / "garment"\nif not data_folder.exists():\n    data_folder.mkdir(parents=True, exist_ok=True)\n\n# 模型文件夹名在当前用户路径，如果没有则创建\nmodel_folder = Path.home() / ".baicai" / "tmp"/ "models" / "garment"\nif not model_folder.exists():\n    model_folder.mkdir(parents=True, exist_ok=True)\n\n# 生成带时间戳的文件名\nsaved_file_name = data_folder / f"baseline_{timestamp}.pkl"\n\nmodel_path = model_folder / f"baseline_{timestamp}.pkl"\ndump(rf_model, model_path)\nprint(f"The model is saved to {model_path}")\n\n# 将数据保存为pickle文件\npd.to_pickle(dls, saved_file_name)\nprint(f"The pickled data is saved to {saved_file_name}")\n',
        "success": True,
        "result": "c:\\Users\\gengyabc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\baicai-6ddbNBXo-py3.12\\Lib\\site-packages\\fastai\\tabular\\core.py:25: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n  df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)\nc:\\Users\\gengyabc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\baicai-6ddbNBXo-py3.12\\Lib\\site-packages\\fastai\\tabular\\core.py:314: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  to[n].fillna(self.na_dict[n], inplace=True)\n,Training r2_score: 0.49188335851444087\nValidation r2_score: 0.4068070359770055\n,Training Set Feature Importance:\n              Feature  Importance\ntargeted_productivity      0.3230\n            incentive      0.1683\n                  smv      0.1017\n        no_of_workers      0.0929\n                 team      0.0721\n            over_time      0.0288\n              Elapsed      0.0163\n                  wip      0.0136\n            Dayofyear      0.0121\n                  Day      0.0052\n            Dayofweek      0.0008\n                 Week      0.0005\n              quarter      0.0001\n           department      0.0000\n          Is_year_end      0.0000\n        Is_year_start      0.0000\n               wip_na      0.0000\n       Is_quarter_end      0.0000\n       Is_month_start      0.0000\n         Is_month_end      0.0000\n                Month      0.0000\n                 Year      0.0000\n            idle_time      0.0000\n   no_of_style_change      0.0000\n             idle_men      0.0000\n                  day      0.0000\n     Is_quarter_start      0.0000\nValidation Set Feature Importance:\n              Feature  Importance\ntargeted_productivity      0.3420\n            incentive      0.1354\n                  smv      0.0634\n        no_of_workers      0.0513\n                  wip      0.0126\n              Elapsed      0.0085\n            over_time      0.0066\n            Dayofyear      0.0055\n                  Day      0.0029\n                 team      0.0005\n                 Week      0.0001\n        Is_year_start      0.0000\n            idle_time      0.0000\n               wip_na      0.0000\n     Is_quarter_start      0.0000\n          Is_year_end      0.0000\n           department      0.0000\n       Is_quarter_end      0.0000\n       Is_month_start      0.0000\n         Is_month_end      0.0000\n                Month      0.0000\n                 Year      0.0000\n   no_of_style_change      0.0000\n             idle_men      0.0000\n                  day      0.0000\n              quarter     -0.0002\n            Dayofweek     -0.0006\nThe model is saved to C:\\Use",
        "error": "",
    }
]
# Action Builder
ACTION_MAKER_RESULT_IRIS = [
    {
        "id": 1,
        "action": "Create a new feature for sepal area by multiplying sepal length and width.",
        "features": ["sepal length (cm)", "sepal width (cm)"],
        "business_justification": "Sepal area provides a combined measure of sepal size, which may be more informative than individual dimensions for species classification.",
        "expected_impact": "Improved model accuracy by capturing the overall sepal size.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 2,
        "action": "Create a new feature for petal area by multiplying petal length and width.",
        "features": ["petal length (cm)", "petal width (cm)"],
        "business_justification": "Petal area provides a combined measure of petal size, which may be more informative than individual dimensions for species classification.",
        "expected_impact": "Improved model accuracy by capturing the overall petal size.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 3,
        "action": "Compute the ratio of petal length to sepal length.",
        "features": ["petal length (cm)", "sepal length (cm)"],
        "business_justification": "The ratio of petal to sepal length is a known discriminative feature in iris classification.",
        "expected_impact": "Improved model accuracy by capturing proportional differences between petal and sepal lengths.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 4,
        "action": "Compute the ratio of petal width to sepal width.",
        "features": ["petal width (cm)", "sepal width (cm)"],
        "business_justification": "The ratio of petal to sepal width is a known discriminative feature in iris classification.",
        "expected_impact": "Improved model accuracy by capturing proportional differences between petal and sepal widths.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 5,
        "action": "Create interaction features by multiplying sepal length with petal length and sepal width with petal width.",
        "features": ["sepal length (cm)", "petal length (cm)", "sepal width (cm)", "petal width (cm)"],
        "business_justification": "Interaction features capture non-linear relationships between sepal and petal dimensions, which may be important for classification.",
        "expected_impact": "Improved model accuracy by capturing interaction effects between features.",
        "domain_rules_compliance": ["Mathematical operations", "Feature interactions"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 6,
        "action": "Calculate the sum of sepal and petal lengths and widths.",
        "features": ["sepal length (cm)", "sepal width (cm)", "petal length (cm)", "petal width (cm)"],
        "business_justification": "Summing measurements provides a total size measure that may be more informative than individual dimensions.",
        "expected_impact": "Improved model accuracy by capturing overall size differences.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 7,
        "action": "Compute the difference between petal and sepal lengths and widths.",
        "features": ["petal length (cm)", "sepal length (cm)", "petal width (cm)", "sepal width (cm)"],
        "business_justification": "Differences in measurements may highlight shape differences between species.",
        "expected_impact": "Improved model accuracy by capturing shape differences.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 8,
        "action": "Create a feature for the perimeter of the flower by summing all four measurements.",
        "features": ["sepal length (cm)", "sepal width (cm)", "petal length (cm)", "petal width (cm)"],
        "business_justification": "Perimeter provides a measure of overall flower size, which may be informative for classification.",
        "expected_impact": "Improved model accuracy by capturing overall size differences.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 9,
        "action": "Compute the ratio of sepal area to petal area.",
        "features": ["sepal length (cm)", "sepal width (cm)", "petal length (cm)", "petal width (cm)"],
        "business_justification": "The ratio of sepal to petal area may indicate resource allocation differences between species.",
        "expected_impact": "Improved model accuracy by capturing area proportion differences.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 10,
        "action": "Create a composite feature combining all four measurements through addition.",
        "features": ["sepal length (cm)", "sepal width (cm)", "petal length (cm)", "petal width (cm)"],
        "business_justification": "A composite feature may capture overall size and shape differences between species.",
        "expected_impact": "Improved model accuracy by providing a holistic measure of flower dimensions.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
]

ACTION_MAKER_RESULT_HOUSE = [
    {
        "id": 1,
        "action": "Create a new feature 'LogPopulation' by taking the natural logarithm of the Population feature to account for non-linear relationships.",
        "features": ["Population"],
        "business_justification": "Population may have a non-linear relationship with house value, and log transformation can help linearize it.",
        "expected_impact": "Improved model fit by better capturing the relationship between population size and house value.",
        "domain_rules_compliance": ["Mathematical operations", "Distribution analysis"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 2,
        "action": "Create a new feature 'RoomsPerBedroom' by dividing AveRooms by AveBedrms to capture the ratio of rooms to bedrooms.",
        "features": ["AveRooms", "AveBedrms"],
        "business_justification": "The ratio of rooms to bedrooms can indicate the layout and functionality of the house, which may impact value.",
        "expected_impact": "Provides additional information about house structure that may improve predictive accuracy.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 3,
        "action": "Create a new feature 'PopulationDensity' by dividing Population by AveRooms to estimate population density per room.",
        "features": ["Population", "AveRooms"],
        "business_justification": "Population density can be a proxy for neighborhood congestion and demand for housing.",
        "expected_impact": "Adds a measure of crowding that may explain variations in house prices.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 4,
        "action": "Create a new feature 'HouseAgeGroup' by categorizing HouseAge into quantiles (e.g., 4 groups) to capture non-linear age effects.",
        "features": ["HouseAge"],
        "business_justification": "House age may have a non-linear relationship with value, and grouping can capture this better than raw values.",
        "expected_impact": "Improves the model's ability to capture non-linear effects of house age on value.",
        "domain_rules_compliance": ["Distribution analysis", "Business rule application"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 5,
        "action": "Create a new feature 'LatitudeLongitudeInteraction' by multiplying Latitude and Longitude to capture spatial interactions.",
        "features": ["Latitude", "Longitude"],
        "business_justification": "Geographic location interactions can capture area-specific effects on house value.",
        "expected_impact": "Adds a feature that may explain regional variations in house prices.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 6,
        "action": "Create a new feature 'IncomePerRoom' by dividing MedInc by AveRooms to measure income per room.",
        "features": ["MedInc", "AveRooms"],
        "business_justification": "Income per room can indicate the economic status and value of the household.",
        "expected_impact": "Provides a measure of economic capacity that may correlate with house value.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 7,
        "action": "Create a new feature 'OccupancyPerRoom' by dividing AveOccup by AveRooms to measure occupancy per room.",
        "features": ["AveOccup", "AveRooms"],
        "business_justification": "Occupancy per room can indicate how fully utilized the house is, which may relate to value.",
        "expected_impact": "Adds a measure of how occupied each room is on average.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 8,
        "action": "Create a new feature 'HouseValuePerPerson' by dividing MedHouseVal by Population to estimate value per person.",
        "features": ["MedHouseVal", "Population"],
        "business_justification": "Value per person can indicate the economic value allocated per individual in the household.",
        "expected_impact": "Provides a measure of value distribution across household members.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 9,
        "action": "Create a new feature 'RoomDensity' by dividing AveRooms by Population to estimate rooms per person.",
        "features": ["AveRooms", "Population"],
        "business_justification": "Room density can indicate the spaciousness of the house relative to the population.",
        "expected_impact": "Adds a measure of how roomy the house is per person.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 10,
        "action": "Create a new feature 'AgeIncomeInteraction' by multiplying HouseAge and MedInc to capture the combined effect of age and income on house value.",
        "features": ["HouseAge", "MedInc"],
        "business_justification": "The interaction of house age and income can capture the premium for older homes in higher-income areas.",
        "expected_impact": "Adds a feature that may explain how age and income jointly affect house value.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
    },
]

ACTION_MAKER_RESULT_TITANIC = [
    {
        "id": 1,
        "action": "Create a new feature 'family_size' by adding 'sibsp' and 'parch' and then adding 1 to account for the passenger themselves.",
        "features": ["sibsp", "parch"],
        "business_justification": "Family size can indicate survival priority, as larger families might have had more difficulty evacuating.",
        "expected_impact": "Improved model understanding of family dynamics.",
        "domain_rules_compliance": ["Family size is a logical combination of sibsp and parch."],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 2,
        "action": "Create a new feature 'fare_per_person' by dividing 'fare' by 'family_size'.",
        "features": ["fare", "family_size"],
        "business_justification": "Indicates the economic status per person, which might influence survival chances.",
        "expected_impact": "Better understanding of economic status impact.",
        "domain_rules_compliance": ["Economic status is relevant to survival."],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 3,
        "action": "Create a new feature 'age_group' by binning 'age' into categories: infant (0-5), child (6-12), adult (13-64), senior (65+).",
        "features": ["age"],
        "business_justification": "Age groups can capture non-linear relationships with survival rates.",
        "expected_impact": "Improved handling of age-related survival trends.",
        "domain_rules_compliance": ["Age grouping aligns with survival patterns."],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 4,
        "action": "Extract 'title' from the 'name' feature and create a new categorical feature indicating social status.",
        "features": ["name"],
        "business_justification": "Titles like 'Mr.', 'Mrs.', 'Dr.' indicate social status, which can affect survival.",
        "expected_impact": "Captures social status influence on survival.",
        "domain_rules_compliance": ["Social status is a known factor in disasters."],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 5,
        "action": "Create a new feature 'deck_level' based on the first letter of 'cabin', indicating the deck level (A, B, C, etc.).",
        "features": ["cabin"],
        "business_justification": "Deck level can indicate proximity to lifeboats, affecting survival.",
        "expected_impact": "Better understanding of physical location on the ship.",
        "domain_rules_compliance": ["Deck location impacts evacuation."],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 6,
        "action": "Create a new feature 'is_alone' which is 1 if 'family_size' is 1, else 0.",
        "features": ["family_size"],
        "business_justification": "Traveling alone might reduce survival chances due to lack of assistance.",
        "expected_impact": "Highlights solo travelers' vulnerability.",
        "domain_rules_compliance": ["Solo travelers are a known risk group."],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 7,
        "action": "Create an interaction feature 'class_sex_survival' combining 'pclass' and 'sex' to capture survival probabilities.",
        "features": ["pclass", "sex"],
        "business_justification": "Survival rates vary significantly by class and gender.",
        "expected_impact": "Captures class and gender survival interactions.",
        "domain_rules_compliance": ["Class and gender are key factors."],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 8,
        "action": "Create a new feature 'embarked_port_importance' based on historical survival rates of each port.",
        "features": ["embarked"],
        "business_justification": "Certain ports might have different survival rates due to loading practices.",
        "expected_impact": "Incorporates port-specific survival trends.",
        "domain_rules_compliance": ["Embarkation port affects survival."],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 9,
        "action": "Create an interaction feature 'age_class' combining 'age' and 'pclass' to capture age-related survival within classes.",
        "features": ["age", "pclass"],
        "business_justification": "Age effects on survival may vary by class.",
        "expected_impact": "Better captures age-class survival dynamics.",
        "domain_rules_compliance": ["Age impacts vary by class."],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 10,
        "action": "Create a new feature 'large_family' which is 1 if 'family_size' is greater than 4, else 0.",
        "features": ["family_size"],
        "business_justification": "Large families might face more survival challenges.",
        "expected_impact": "Highlights challenges for large families.",
        "domain_rules_compliance": ["Family size impacts survival."],
        "rejected": "false",
        "rejection_reason": "",
    },
]

ACTION_MAKER_RESULT_GARMENT = [
    {
        "id": 1,
        "action": "Create a new feature 'log_over_time' by taking the natural logarithm of over_time to stabilize variance.",
        "features": ["over_time"],
        "business_justification": "Over_time has a high variance, and log transformation can make the distribution more normal, improving model performance.",
        "expected_impact": "Improved model fit due to reduced skewness.",
        "domain_rules_compliance": ["Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 2,
        "action": "Create a new feature 'smv_per_worker' by dividing smv by no_of_workers to capture productivity per worker.",
        "features": ["smv", "no_of_workers"],
        "business_justification": "This feature captures individual worker productivity, which is crucial for understanding team performance.",
        "expected_impact": "Better understanding of individual contributions to productivity.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 3,
        "action": "Create a new feature 'idle_time_per_worker' by dividing idle_time by no_of_workers to understand individual idle time.",
        "features": ["idle_time", "no_of_workers"],
        "business_justification": "This feature helps identify if idle time is evenly distributed or concentrated among specific workers.",
        "expected_impact": "Improved understanding of idle time distribution.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 4,
        "action": "Create a new feature 'productivity_rolling_avg' by taking a 7-day rolling average of actual_productivity to smooth out daily fluctuations.",
        "features": ["actual_productivity"],
        "business_justification": "This feature captures the trend in productivity over time, reducing noise.",
        "expected_impact": "Improved model stability by reducing daily fluctuations.",
        "domain_rules_compliance": ["Window functions"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 5,
        "action": "Create a new feature 'wip_to_workers_ratio' by dividing wip by no_of_workers to understand workload distribution.",
        "features": ["wip", "no_of_workers"],
        "business_justification": "This feature helps understand how workload is distributed among workers, impacting productivity.",
        "expected_impact": "Better understanding of workload distribution.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 6,
        "action": "Create a new feature 'incentive_effectiveness' by dividing incentive by over_time to measure the impact of incentives on productivity.",
        "features": ["incentive", "over_time"],
        "business_justification": "This feature quantifies how effective incentives are in increasing productivity.",
        "expected_impact": "Improved understanding of incentive effectiveness.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 7,
        "action": "Create a new feature 'idle_percentage' by dividing idle_time by (total available time) to understand idle time as a percentage of total time.",
        "features": ["idle_time"],
        "business_justification": "This feature provides a relative measure of idle time, making it easier to compare across different periods.",
        "expected_impact": "Improved understanding of idle time impact.",
        "domain_rules_compliance": ["Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 8,
        "action": "Create a new feature 'style_change_frequency' by dividing no_of_style_change by no_of_workers to understand the impact of style changes per worker.",
        "features": ["no_of_style_change", "no_of_workers"],
        "business_justification": "This feature captures how frequently style changes affect each worker, impacting productivity.",
        "expected_impact": "Better understanding of style change impact on workers.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 9,
        "action": "Create a new feature 'productivity_gap' by subtracting actual_productivity from targeted_productivity to measure performance gaps.",
        "features": ["actual_productivity", "targeted_productivity"],
        "business_justification": "This feature quantifies the difference between actual and targeted productivity, highlighting areas for improvement.",
        "expected_impact": "Improved identification of performance gaps.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
    {
        "id": 10,
        "action": "Create a new feature 'productivity_ratio' by dividing actual_productivity by targeted_productivity to measure productivity efficiency.",
        "features": ["actual_productivity", "targeted_productivity"],
        "business_justification": "This feature provides a relative measure of productivity efficiency, making it easier to compare across different periods.",
        "expected_impact": "Improved understanding of productivity efficiency.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
    },
]

ACTIONS_WITH_CODE_IRIS = [
    {
        "id": 1,
        "action": "Create a new feature for sepal area by multiplying sepal length and width.",
        "features": ["sepal length (cm)", "sepal width (cm)"],
        "business_justification": "Sepal area provides a combined measure of sepal size, which may be more informative than individual dimensions for species classification.",
        "expected_impact": "Improved model accuracy by capturing the overall sepal size.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 1: Create a new feature for sepal area by multiplying sepal length and width.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['sepal_area'] = X_train_copy['sepal length (cm)'] * X_train_copy['sepal width (cm)']\nX_test_copy['sepal_area'] = X_test_copy['sepal length (cm)'] * X_test_copy['sepal width (cm)']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 1\")\nprint(\"Training set accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set accuracy: \", accuracy_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 1\nTraining set accuracy:  1.0\nTesting set accuracy:  0.9666666666666667\n",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
    {
        "id": 2,
        "action": "Create a new feature for petal area by multiplying petal length and width.",
        "features": ["petal length (cm)", "petal width (cm)"],
        "business_justification": "Petal area provides a combined measure of petal size, which may be more informative than individual dimensions for species classification.",
        "expected_impact": "Improved model accuracy by capturing the overall petal size.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 2: Create a new feature for petal area by multiplying petal length and width.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['petal_area'] = X_train_copy['petal length (cm)'] * X_train_copy['petal width (cm)']\nX_test_copy['petal_area'] = X_test_copy['petal length (cm)'] * X_test_copy['petal width (cm)']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 2\")\nprint(\"Training set accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set accuracy: \", accuracy_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 2\nTraining set accuracy:  1.0\nTesting set accuracy:  1.0\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 3,
        "action": "Compute the ratio of petal length to sepal length.",
        "features": ["petal length (cm)", "sepal length (cm)"],
        "business_justification": "The ratio of petal to sepal length is a known discriminative feature in iris classification.",
        "expected_impact": "Improved model accuracy by capturing proportional differences between petal and sepal lengths.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 3: Compute the ratio of petal length to sepal length.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['petal_to_sepal_length'] = X_train_copy['petal length (cm)'] / X_train_copy['sepal length (cm)']\nX_test_copy['petal_to_sepal_length'] = X_test_copy['petal length (cm)'] / X_test_copy['sepal length (cm)']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 3\")\nprint(\"Training set accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set accuracy: \", accuracy_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 3\n,Training set accuracy:  1.0\nTesting set accuracy:  1.0\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 4,
        "action": "Compute the ratio of petal width to sepal width.",
        "features": ["petal width (cm)", "sepal width (cm)"],
        "business_justification": "The ratio of petal to sepal width is a known discriminative feature in iris classification.",
        "expected_impact": "Improved model accuracy by capturing proportional differences between petal and sepal widths.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 4: Compute the ratio of petal width to sepal width.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['petal_to_sepal_width'] = X_train_copy['petal width (cm)'] / X_train_copy['sepal width (cm)']\nX_test_copy['petal_to_sepal_width'] = X_test_copy['petal width (cm)'] / X_test_copy['sepal width (cm)']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 4\")\nprint(\"Training set accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set accuracy: \", accuracy_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 4\n,Training set accuracy:  1.0\nTesting set accuracy:  1.0\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 5,
        "action": "Create interaction features by multiplying sepal length with petal length and sepal width with petal width.",
        "features": ["sepal length (cm)", "petal length (cm)", "sepal width (cm)", "petal width (cm)"],
        "business_justification": "Interaction features capture non-linear relationships between sepal and petal dimensions, which may be important for classification.",
        "expected_impact": "Improved model accuracy by capturing interaction effects between features.",
        "domain_rules_compliance": ["Mathematical operations", "Feature interactions"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 5: Create interaction features by multiplying sepal length with petal length and sepal width with petal width.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['sepal_length_petal_length'] = X_train_copy['sepal length (cm)'] * X_train_copy['petal length (cm)']\nX_train_copy['sepal_width_petal_width'] = X_train_copy['sepal width (cm)'] * X_train_copy['petal width (cm)']\nX_test_copy['sepal_length_petal_length'] = X_test_copy['sepal length (cm)'] * X_test_copy['petal length (cm)']\nX_test_copy['sepal_width_petal_width'] = X_test_copy['sepal width (cm)'] * X_test_copy['petal width (cm)']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 5\")\nprint(\"Training set accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set accuracy: \", accuracy_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 5\n,Training set accuracy:  1.0\nTesting set accuracy:  0.9666666666666667\n",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
    {
        "id": 6,
        "action": "Calculate the sum of sepal and petal lengths and widths.",
        "features": ["sepal length (cm)", "sepal width (cm)", "petal length (cm)", "petal width (cm)"],
        "business_justification": "Summing measurements provides a total size measure that may be more informative than individual dimensions.",
        "expected_impact": "Improved model accuracy by capturing overall size differences.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 6: Calculate the sum of sepal and petal lengths and widths.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['total_length'] = X_train_copy['sepal length (cm)'] + X_train_copy['petal length (cm)']\nX_train_copy['total_width'] = X_train_copy['sepal width (cm)'] + X_train_copy['petal width (cm)']\nX_test_copy['total_length'] = X_test_copy['sepal length (cm)'] + X_test_copy['petal length (cm)']\nX_test_copy['total_width'] = X_test_copy['sepal width (cm)'] + X_test_copy['petal width (cm)']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 6\")\nprint(\"Training set accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set accuracy: \", accuracy_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 6\n,Training set accuracy:  1.0\nTesting set accuracy:  0.9666666666666667\n",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
    {
        "id": 7,
        "action": "Compute the difference between petal and sepal lengths and widths.",
        "features": ["petal length (cm)", "sepal length (cm)", "petal width (cm)", "sepal width (cm)"],
        "business_justification": "Differences in measurements may highlight shape differences between species.",
        "expected_impact": "Improved model accuracy by capturing shape differences.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 7: Compute the difference between petal and sepal lengths and widths.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['petal_length_diff'] = X_train_copy['petal length (cm)'] - X_train_copy['sepal length (cm)']\nX_train_copy['petal_width_diff'] = X_train_copy['petal width (cm)'] - X_train_copy['sepal width (cm)']\nX_test_copy['petal_length_diff'] = X_test_copy['petal length (cm)'] - X_test_copy['sepal length (cm)']\nX_test_copy['petal_width_diff'] = X_test_copy['petal width (cm)'] - X_test_copy['sepal width (cm)']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 7\")\nprint(\"Training set accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set accuracy: \", accuracy_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 7\n,Training set accuracy:  1.0\nTesting set accuracy:  0.9666666666666667\n",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
    {
        "id": 8,
        "action": "Create a feature for the perimeter of the flower by summing all four measurements.",
        "features": ["sepal length (cm)", "sepal width (cm)", "petal length (cm)", "petal width (cm)"],
        "business_justification": "Perimeter provides a measure of overall flower size, which may be informative for classification.",
        "expected_impact": "Improved model accuracy by capturing overall size differences.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 8: Create a feature for the perimeter of the flower by summing all four measurements.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['perimeter'] = X_train_copy['sepal length (cm)'] + X_train_copy['sepal width (cm)'] + X_train_copy['petal length (cm)'] + X_train_copy['petal width (cm)']\nX_test_copy['perimeter'] = X_test_copy['sepal length (cm)'] + X_test_copy['sepal width (cm)'] + X_test_copy['petal length (cm)'] + X_test_copy['petal width (cm)']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 8\")\nprint(\"Training set accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set accuracy: \", accuracy_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 8\n,Training set accuracy:  1.0\nTesting set accuracy:  1.0\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 9,
        "action": "Compute the ratio of sepal area to petal area.",
        "features": ["sepal length (cm)", "sepal width (cm)", "petal length (cm)", "petal width (cm)"],
        "business_justification": "The ratio of sepal to petal area may indicate resource allocation differences between species.",
        "expected_impact": "Improved model accuracy by capturing area proportion differences.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 9: Compute the ratio of sepal area to petal area.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['sepal_area'] = X_train_copy['sepal length (cm)'] * X_train_copy['sepal width (cm)']\nX_train_copy['petal_area'] = X_train_copy['petal length (cm)'] * X_train_copy['petal width (cm)']\nX_train_copy['sepal_to_petal_area'] = X_train_copy['sepal_area'] / X_train_copy['petal_area']\n\nX_test_copy['sepal_area'] = X_test_copy['sepal length (cm)'] * X_test_copy['sepal width (cm)']\nX_test_copy['petal_area'] = X_test_copy['petal length (cm)'] * X_test_copy['petal width (cm)']\nX_test_copy['sepal_to_petal_area'] = X_test_copy['sepal_area'] / X_test_copy['petal_area']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 9\")\nprint(\"Training set accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set accuracy: \", accuracy_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 9\nTraining set accuracy:  1.0\nTesting set accuracy:  0.9666666666666667\n",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
    {
        "id": 10,
        "action": "Create a composite feature combining all four measurements through addition.",
        "features": ["sepal length (cm)", "sepal width (cm)", "petal length (cm)", "petal width (cm)"],
        "business_justification": "A composite feature may capture overall size and shape differences between species.",
        "expected_impact": "Improved model accuracy by providing a holistic measure of flower dimensions.",
        "domain_rules_compliance": ["Mathematical operations", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 10: Create a composite feature combining all four measurements through addition.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['composite_feature'] = X_train_copy['sepal length (cm)'] + X_train_copy['sepal width (cm)'] + X_train_copy['petal length (cm)'] + X_train_copy['petal width (cm)']\nX_test_copy['composite_feature'] = X_test_copy['sepal length (cm)'] + X_test_copy['sepal width (cm)'] + X_test_copy['petal length (cm)'] + X_test_copy['petal width (cm)']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 10\")\nprint(\"Training set accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set accuracy: \", accuracy_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n",
        "success": True,
        "result": "Action number: 10\nTraining set accuracy:  1.0\nTesting set accuracy:  1.0\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
]

ACTIONS_WITH_CODE_HOUSE = [
    {
        "id": 1,
        "action": "Create a new feature 'LogPopulation' by taking the natural logarithm of the Population feature to account for non-linear relationships.",
        "features": ["Population"],
        "business_justification": "Population may have a non-linear relationship with house value, and log transformation can help linearize it.",
        "expected_impact": "Improved model fit by better capturing the relationship between population size and house value.",
        "domain_rules_compliance": ["Mathematical operations", "Distribution analysis"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 1: Create a new feature 'LogPopulation' by taking the natural logarithm of the Population feature\n# First, copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Add LogPopulation feature\nX_train_copy['LogPopulation'] = np.log(X_train_copy['Population'])\nX_test_copy['LogPopulation'] = np.log(X_test_copy['Population'])\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_copy)\nX_test_scaled = scaler.transform(X_test_copy)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_scaled, y_train_copy)\n\n# Predict and evaluate\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n\nprint(\"Action 1: LogPopulation\")\nprint(\"Training R²:\", r2_score(y_train_copy, y_pred_train))\nprint(\"Testing R²:\", r2_score(y_test_copy, y_pred_test))\n\n# Clean up memory\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred_train, y_pred_test\n\n",
        "success": True,
        "result": "Action 1: LogPopulation\nTraining R²: 0.9762822759970576\nTesting R²: 0.8265219033086286\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 2,
        "action": "Create a new feature 'RoomsPerBedroom' by dividing AveRooms by AveBedrms to capture the ratio of rooms to bedrooms.",
        "features": ["AveRooms", "AveBedrms"],
        "business_justification": "The ratio of rooms to bedrooms can indicate the layout and functionality of the house, which may impact value.",
        "expected_impact": "Provides additional information about house structure that may improve predictive accuracy.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 2: Create a new feature 'RoomsPerBedroom' by dividing AveRooms by AveBedrms\n# First, copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Add RoomsPerBedroom feature\nX_train_copy['RoomsPerBedroom'] = X_train_copy['AveRooms'] / X_train_copy['AveBedrms']\nX_test_copy['RoomsPerBedroom'] = X_test_copy['AveRooms'] / X_test_copy['AveBedrms']\n\n# Scale features\nX_train_scaled = scaler.fit_transform(X_train_copy)\nX_test_scaled = scaler.transform(X_test_copy)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_scaled, y_train_copy)\n\n# Predict and evaluate\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n\nprint(\"Action 2: RoomsPerBedroom\")\nprint(\"Training R²:\", r2_score(y_train_copy, y_pred_train))\nprint(\"Testing R²:\", r2_score(y_test_copy, y_pred_test))\n\n# Clean up memory\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred_train, y_pred_test\n\n",
        "success": True,
        "result": "Action 2: RoomsPerBedroom\nTraining R²: 0.9763043738372944\nTesting R²: 0.8277313931079371\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 3,
        "action": "Create a new feature 'PopulationDensity' by dividing Population by AveRooms to estimate population density per room.",
        "features": ["Population", "AveRooms"],
        "business_justification": "Population density can be a proxy for neighborhood congestion and demand for housing.",
        "expected_impact": "Adds a measure of crowding that may explain variations in house prices.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 3: Create a new feature 'PopulationDensity' by dividing Population by AveRooms\n# First, copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Add PopulationDensity feature\nX_train_copy['PopulationDensity'] = X_train_copy['Population'] / X_train_copy['AveRooms']\nX_test_copy['PopulationDensity'] = X_test_copy['Population'] / X_test_copy['AveRooms']\n\n# Scale features\nX_train_scaled = scaler.fit_transform(X_train_copy)\nX_test_scaled = scaler.transform(X_test_copy)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_scaled, y_train_copy)\n\n# Predict and evaluate\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n\nprint(\"Action 3: PopulationDensity\")\nprint(\"Training R²:\", r2_score(y_train_copy, y_pred_train))\nprint(\"Testing R²:\", r2_score(y_test_copy, y_pred_test))\n\n# Clean up memory\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred_train, y_pred_test\n\n",
        "success": True,
        "result": "Action 3: PopulationDensity\nTraining R²: 0.9761957735019345\nTesting R²: 0.8258922559563494\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 4,
        "action": "Create a new feature 'HouseAgeGroup' by categorizing HouseAge into quantiles (e.g., 4 groups) to capture non-linear age effects.",
        "features": ["HouseAge"],
        "business_justification": "House age may have a non-linear relationship with value, and grouping can capture this better than raw values.",
        "expected_impact": "Improves the model's ability to capture non-linear effects of house age on value.",
        "domain_rules_compliance": ["Distribution analysis", "Business rule application"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 4: Create a new feature 'HouseAgeGroup' by categorizing HouseAge into quantiles\n# First, copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Create HouseAgeGroup feature using quantiles\nX_train_copy['HouseAgeGroup'] = pd.qcut(X_train_copy['HouseAge'], q=4, labels=False)\nX_test_copy['HouseAgeGroup'] = pd.qcut(X_test_copy['HouseAge'], q=4, labels=False)\n\n# Scale features\nX_train_scaled = scaler.fit_transform(X_train_copy)\nX_test_scaled = scaler.transform(X_test_copy)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_scaled, y_train_copy)\n\n# Predict and evaluate\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n\nprint(\"Action 4: HouseAgeGroup\")\nprint(\"Training R²:\", r2_score(y_train_copy, y_pred_train))\nprint(\"Testing R²:\", r2_score(y_test_copy, y_pred_test))\n\n# Clean up memory\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred_train, y_pred_test\n\n",
        "success": True,
        "result": "Action 4: HouseAgeGroup\nTraining R²: 0.9762406282684831\nTesting R²: 0.8273191885855133\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 5,
        "action": "Create a new feature 'LatitudeLongitudeInteraction' by multiplying Latitude and Longitude to capture spatial interactions.",
        "features": ["Latitude", "Longitude"],
        "business_justification": "Geographic location interactions can capture area-specific effects on house value.",
        "expected_impact": "Adds a feature that may explain regional variations in house prices.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 5: Create a new feature 'LatitudeLongitudeInteraction' by multiplying Latitude and Longitude\n# First, copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Add LatitudeLongitudeInteraction feature\nX_train_copy['LatitudeLongitudeInteraction'] = X_train_copy['Latitude'] * X_train_copy['Longitude']\nX_test_copy['LatitudeLongitudeInteraction'] = X_test_copy['Latitude'] * X_test_copy['Longitude']\n\n# Scale features\nX_train_scaled = scaler.fit_transform(X_train_copy)\nX_test_scaled = scaler.transform(X_test_copy)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_scaled, y_train_copy)\n\n# Predict and evaluate\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n\nprint(\"Action 5: LatitudeLongitudeInteraction\")\nprint(\"Training R²:\", r2_score(y_train_copy, y_pred_train))\nprint(\"Testing R²:\", r2_score(y_test_copy, y_pred_test))\n\n# Clean up memory\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred_train, y_pred_test\n\n",
        "success": True,
        "result": "Action 5: LatitudeLongitudeInteraction\nTraining R²: 0.9767742366506389\nTesting R²: 0.8292830940240313\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 6,
        "action": "Create a new feature 'IncomePerRoom' by dividing MedInc by AveRooms to measure income per room.",
        "features": ["MedInc", "AveRooms"],
        "business_justification": "Income per room can indicate the economic status and value of the household.",
        "expected_impact": "Provides a measure of economic capacity that may correlate with house value.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 6: Create a new feature 'IncomePerRoom' by dividing MedInc by AveRooms\n# First, copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Add IncomePerRoom feature\nX_train_copy['IncomePerRoom'] = X_train_copy['MedInc'] / X_train_copy['AveRooms']\nX_test_copy['IncomePerRoom'] = X_test_copy['MedInc'] / X_test_copy['AveRooms']\n\n# Scale features\nX_train_scaled = scaler.fit_transform(X_train_copy)\nX_test_scaled = scaler.transform(X_test_copy)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_scaled, y_train_copy)\n\n# Predict and evaluate\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n\nprint(\"Action 6: IncomePerRoom\")\nprint(\"Training R²:\", r2_score(y_train_copy, y_pred_train))\nprint(\"Testing R²:\", r2_score(y_test_copy, y_pred_test))\n\n# Clean up memory\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred_train, y_pred_test\n\n",
        "success": True,
        "result": "Action 6: IncomePerRoom\nTraining R²: 0.9756375706101582\nTesting R²: 0.8241216347980237\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 7,
        "action": "Create a new feature 'OccupancyPerRoom' by dividing AveOccup by AveRooms to measure occupancy per room.",
        "features": ["AveOccup", "AveRooms"],
        "business_justification": "Occupancy per room can indicate how fully utilized the house is, which may relate to value.",
        "expected_impact": "Adds a measure of how occupied each room is on average.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 7: Create a new feature 'OccupancyPerRoom' by dividing AveOccup by AveRooms\n# First, copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Add OccupancyPerRoom feature\nX_train_copy['OccupancyPerRoom'] = X_train_copy['AveOccup'] / X_train_copy['AveRooms']\nX_test_copy['OccupancyPerRoom'] = X_test_copy['AveOccup'] / X_test_copy['AveRooms']\n\n# Scale features\nX_train_scaled = scaler.fit_transform(X_train_copy)\nX_test_scaled = scaler.transform(X_test_copy)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_scaled, y_train_copy)\n\n# Predict and evaluate\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n\nprint(\"Action 7: OccupancyPerRoom\")\nprint(\"Training R²:\", r2_score(y_train_copy, y_pred_train))\nprint(\"Testing R²:\", r2_score(y_test_copy, y_pred_test))\n\n# Clean up memory\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred_train, y_pred_test\n\n",
        "success": True,
        "result": "Action 7: OccupancyPerRoom\nTraining R²: 0.9764580753081237\nTesting R²: 0.826738777957388\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 8,
        "action": "Create a new feature 'HouseValuePerPerson' by dividing MedHouseVal by Population to estimate value per person.",
        "features": ["MedHouseVal", "Population"],
        "business_justification": "Value per person can indicate the economic value allocated per individual in the household.",
        "expected_impact": "Provides a measure of value distribution across household members.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 8: Create a new feature 'HouseValuePerPerson' by dividing MedHouseVal by Population\n# First, copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Add HouseValuePerPerson feature using target variable\nX_train_copy['HouseValuePerPerson'] = y_train_copy / X_train_copy['Population']\nX_test_copy['HouseValuePerPerson'] = y_test_copy / X_test_copy['Population']\n\n# Scale features\nX_train_scaled = scaler.fit_transform(X_train_copy)\nX_test_scaled = scaler.transform(X_test_copy)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_scaled, y_train_copy)\n\n# Predict and evaluate\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n\nprint(\"Action 8: HouseValuePerPerson\")\nprint(\"Training R²:\", r2_score(y_train_copy, y_pred_train))\nprint(\"Testing R²:\", r2_score(y_test_copy, y_pred_test))\n\n# Clean up memory\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred_train, y_pred_test\n",
        "success": True,
        "result": "Action 8: HouseValuePerPerson\nTraining R²: 0.9898577958105537\nTesting R²: 0.921112321537407\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 9,
        "action": "Create a new feature 'RoomDensity' by dividing AveRooms by Population to estimate rooms per person.",
        "features": ["AveRooms", "Population"],
        "business_justification": "Room density can indicate the spaciousness of the house relative to the population.",
        "expected_impact": "Adds a measure of how roomy the house is per person.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 9: Create a new feature 'RoomDensity' by dividing AveRooms by Population\n# First, copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Add RoomDensity feature\nX_train_copy['RoomDensity'] = X_train_copy['AveRooms'] / X_train_copy['Population']\nX_test_copy['RoomDensity'] = X_test_copy['AveRooms'] / X_test_copy['Population']\n\n# Scale features\nX_train_scaled = scaler.fit_transform(X_train_copy)\nX_test_scaled = scaler.transform(X_test_copy)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_scaled, y_train_copy)\n\n# Predict and evaluate\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n\nprint(\"Action 9: RoomDensity\")\nprint(\"Training R²:\", r2_score(y_train_copy, y_pred_train))\nprint(\"Testing R²:\", r2_score(y_test_copy, y_pred_test))\n\n# Clean up memory\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred_train, y_pred_test\n\n",
        "success": True,
        "result": "Action 9: RoomDensity\nTraining R²: 0.9761684403152814\nTesting R²: 0.8257146327788645\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 10,
        "action": "Create a new feature 'AgeIncomeInteraction' by multiplying HouseAge and MedInc to capture the combined effect of age and income on house value.",
        "features": ["HouseAge", "MedInc"],
        "business_justification": "The interaction of house age and income can capture the premium for older homes in higher-income areas.",
        "expected_impact": "Adds a feature that may explain how age and income jointly affect house value.",
        "domain_rules_compliance": ["Feature interactions", "Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 10: Create a new feature 'AgeIncomeInteraction' by multiplying HouseAge and MedInc\n# First, copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Add AgeIncomeInteraction feature\nX_train_copy['AgeIncomeInteraction'] = X_train_copy['HouseAge'] * X_train_copy['MedInc']\nX_test_copy['AgeIncomeInteraction'] = X_test_copy['HouseAge'] * X_test_copy['MedInc']\n\n# Scale features\nX_train_scaled = scaler.fit_transform(X_train_copy)\nX_test_scaled = scaler.transform(X_test_copy)\n\n# Train model\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_scaled, y_train_copy)\n\n# Predict and evaluate\ny_pred_train = model.predict(X_train_scaled)\ny_pred_test = model.predict(X_test_scaled)\n\nprint(\"Action 10: AgeIncomeInteraction\")\nprint(\"Training R²:\", r2_score(y_train_copy, y_pred_train))\nprint(\"Testing R²:\", r2_score(y_test_copy, y_pred_test))\n\n# Clean up memory\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred_train, y_pred_test\n",
        "success": True,
        "result": "Action 10: AgeIncomeInteraction\nTraining R²: 0.9758103759394826\nTesting R²: 0.8219095232288258\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
]

ACTIONS_WITH_CODE_TITANIC = [
    {
        "id": 1,
        "action": "Create a new feature 'family_size' by adding 'sibsp' and 'parch' and then adding 1 to account for the passenger themselves.",
        "features": ["sibsp", "parch"],
        "business_justification": "Family size can indicate survival priority, as larger families might have had more difficulty evacuating.",
        "expected_impact": "Improved model understanding of family dynamics.",
        "domain_rules_compliance": ["Family size is a logical combination of sibsp and parch."],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 1: Create family_size\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Create family_size feature\nX_train_copy['family_size'] = X_train_copy['sibsp'] + X_train_copy['parch'] + 1\nX_test_copy['family_size'] = X_test_copy['sibsp'] + X_test_copy['parch'] + 1\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action 1: Create family_size\")\nprint(\"Training Accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing Accuracy: \", accuracy_score(y_test_copy, y_pred))\nprint(\"\\n\")\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action 1: Create family_size\nTraining Accuracy:  0.9859747545582047\nTesting Accuracy:  0.7865168539325843\n\n\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 2,
        "action": "Create a new feature 'fare_per_person' by dividing 'fare' by 'family_size'.",
        "features": ["fare", "family_size"],
        "business_justification": "Indicates the economic status per person, which might influence survival chances.",
        "expected_impact": "Better understanding of economic status impact.",
        "domain_rules_compliance": ["Economic status is relevant to survival."],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 2: Create fare_per_person\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Create family_size feature first\nX_train_copy['family_size'] = X_train_copy['sibsp'] + X_train_copy['parch'] + 1\nX_test_copy['family_size'] = X_test_copy['sibsp'] + X_test_copy['parch'] + 1\n\n# Create fare_per_person feature\nX_train_copy['fare_per_person'] = X_train_copy['fare'] / X_train_copy['family_size']\nX_test_copy['fare_per_person'] = X_test_copy['fare'] / X_test_copy['family_size']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action 2: Create fare_per_person\")\nprint(\"Training Accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing Accuracy: \", accuracy_score(y_test_copy, y_pred))\nprint(\"\\n\")\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action 2: Create fare_per_person\nTraining Accuracy:  0.9859747545582047\nTesting Accuracy:  0.7640449438202247\n\n\n",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
    {
        "id": 3,
        "action": "Create a new feature 'age_group' by binning 'age' into categories: infant (0-5), child (6-12), adult (13-64), senior (65+).",
        "features": ["age"],
        "business_justification": "Age groups can capture non-linear relationships with survival rates.",
        "expected_impact": "Improved handling of age-related survival trends.",
        "domain_rules_compliance": ["Age grouping aligns with survival patterns."],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 3: Create age_group\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Create age_group feature\nbins = [0, 5, 13, 65, 100]\nlabels = ['infant', 'child', 'adult', 'senior']\nX_train_copy['age_group'] = pd.cut(X_train_copy['age'], bins=bins, labels=labels)\nX_test_copy['age_group'] = pd.cut(X_test_copy['age'], bins=bins, labels=labels)\n\n# Encode the categorical feature\nle = LabelEncoder()\nX_train_copy['age_group'] = le.fit_transform(X_train_copy['age_group'])\nX_test_copy['age_group'] = le.transform(X_test_copy['age_group'])\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action 3: Create age_group\")\nprint(\"Training Accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing Accuracy: \", accuracy_score(y_test_copy, y_pred))\nprint(\"\\n\")\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action 3: Create age_group\nTraining Accuracy:  0.9859747545582047\nTesting Accuracy:  0.7921348314606742\n\n\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 4,
        "action": "Extract 'title' from the 'name' feature and create a new categorical feature indicating social status.",
        "features": ["name"],
        "business_justification": "Titles like 'Mr.', 'Mrs.', 'Dr.' indicate social status, which can affect survival.",
        "expected_impact": "Captures social status influence on survival.",
        "domain_rules_compliance": ["Social status is a known factor in disasters."],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 4: Create title from name (Note: 'name' feature is not present in the data)\n# Skipping this action as 'name' feature is not available\n\n",
        "success": True,
        "result": "Success",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
    {
        "id": 5,
        "action": "Create a new feature 'deck_level' based on the first letter of 'cabin', indicating the deck level (A, B, C, etc.).",
        "features": ["cabin"],
        "business_justification": "Deck level can indicate proximity to lifeboats, affecting survival.",
        "expected_impact": "Better understanding of physical location on the ship.",
        "domain_rules_compliance": ["Deck location impacts evacuation."],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 5: Create deck_level (Note: 'cabin' feature is not present in the data)\n# Skipping this action as 'cabin' feature is not available\n\n",
        "success": True,
        "result": "Success",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
    {
        "id": 6,
        "action": "Create a new feature 'is_alone' which is 1 if 'family_size' is 1, else 0.",
        "features": ["family_size"],
        "business_justification": "Traveling alone might reduce survival chances due to lack of assistance.",
        "expected_impact": "Highlights solo travelers' vulnerability.",
        "domain_rules_compliance": ["Solo travelers are a known risk group."],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 6: Create is_alone\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Create family_size feature first\nX_train_copy['family_size'] = X_train_copy['sibsp'] + X_train_copy['parch'] + 1\nX_test_copy['family_size'] = X_test_copy['sibsp'] + X_test_copy['parch'] + 1\n\n# Create is_alone feature\nX_train_copy['is_alone'] = (X_train_copy['family_size'] == 1).astype(int)\nX_test_copy['is_alone'] = (X_test_copy['family_size'] == 1).astype(int)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action 6: Create is_alone\")\nprint(\"Training Accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing Accuracy: \", accuracy_score(y_test_copy, y_pred))\nprint(\"\\n\")\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action 6: Create is_alone\nTraining Accuracy:  0.9859747545582047\nTesting Accuracy:  0.7696629213483146\n\n\n",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
    {
        "id": 7,
        "action": "Create an interaction feature 'class_sex_survival' combining 'pclass' and 'sex' to capture survival probabilities.",
        "features": ["pclass", "sex"],
        "business_justification": "Survival rates vary significantly by class and gender.",
        "expected_impact": "Captures class and gender survival interactions.",
        "domain_rules_compliance": ["Class and gender are key factors."],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 7: Create class_sex_survival\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Create class_sex_survival feature\nX_train_copy['class_sex_survival'] = X_train_copy['pclass'] * X_train_copy['sex']\nX_test_copy['class_sex_survival'] = X_test_copy['pclass'] * X_test_copy['sex']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action 7: Create class_sex_survival\")\nprint(\"Training Accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing Accuracy: \", accuracy_score(y_test_copy, y_pred))\nprint(\"\\n\")\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action 7: Create class_sex_survival\nTraining Accuracy:  0.9859747545582047\nTesting Accuracy:  0.7865168539325843\n\n\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 8,
        "action": "Create a new feature 'embarked_port_importance' based on historical survival rates of each port.",
        "features": ["embarked"],
        "business_justification": "Certain ports might have different survival rates due to loading practices.",
        "expected_impact": "Incorporates port-specific survival trends.",
        "domain_rules_compliance": ["Embarkation port affects survival."],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 8: Create embarked_port_importance (Note: Historical data is not available)\n# Skipping this action as historical data is not provided\n\n",
        "success": True,
        "result": "Success",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
    {
        "id": 9,
        "action": "Create an interaction feature 'age_class' combining 'age' and 'pclass' to capture age-related survival within classes.",
        "features": ["age", "pclass"],
        "business_justification": "Age effects on survival may vary by class.",
        "expected_impact": "Better captures age-class survival dynamics.",
        "domain_rules_compliance": ["Age impacts vary by class."],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 9: Create age_class\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Create age_class feature\nX_train_copy['age_class'] = X_train_copy['age'] * X_train_copy['pclass']\nX_test_copy['age_class'] = X_test_copy['age'] * X_test_copy['pclass']\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action 9: Create age_class\")\nprint(\"Training Accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing Accuracy: \", accuracy_score(y_test_copy, y_pred))\nprint(\"\\n\")\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action 9: Create age_class\nTraining Accuracy:  0.9859747545582047\nTesting Accuracy:  0.7921348314606742\n\n\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 10,
        "action": "Create a new feature 'large_family' which is 1 if 'family_size' is greater than 4, else 0.",
        "features": ["family_size"],
        "business_justification": "Large families might face more survival challenges.",
        "expected_impact": "Highlights challenges for large families.",
        "domain_rules_compliance": ["Family size impacts survival."],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 10: Create large_family\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Create family_size feature first\nX_train_copy['family_size'] = X_train_copy['sibsp'] + X_train_copy['parch'] + 1\nX_test_copy['family_size'] = X_test_copy['sibsp'] + X_test_copy['parch'] + 1\n\n# Create large_family feature\nX_train_copy['large_family'] = (X_train_copy['family_size'] > 4).astype(int)\nX_test_copy['large_family'] = (X_test_copy['family_size'] > 4).astype(int)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action 10: Create large_family\")\nprint(\"Training Accuracy: \", accuracy_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing Accuracy: \", accuracy_score(y_test_copy, y_pred))\nprint(\"\\n\")\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n",
        "success": True,
        "result": "Action 10: Create large_family\nTraining Accuracy:  0.9859747545582047\nTesting Accuracy:  0.7808988764044944\n\n\n",
        "ignore": False,
        "error": "",
        "accepted": "false",
    },
]

ACTIONS_WITH_CODE_GARMENT = [
    {
        "id": 1,
        "action": "Create a new feature 'log_over_time' by taking the natural logarithm of over_time to stabilize variance.",
        "features": ["over_time"],
        "business_justification": "Over_time has a high variance, and log transformation can make the distribution more normal, improving model performance.",
        "expected_impact": "Improved model fit due to reduced skewness.",
        "domain_rules_compliance": ["Mathematical operations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 1: Create a new feature 'log_over_time' by taking the natural logarithm of over_time to stabilize variance.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['log_over_time'] = np.log(X_train_copy['over_time'] + 1e-8)\nX_test_copy['log_over_time'] = np.log(X_test_copy['over_time'] + 1e-8)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 1\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 1\nTraining set R²:  0.9313469683030731\nTesting set R²:  0.44654768311355697\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 2,
        "action": "Create a new feature 'smv_per_worker' by dividing smv by no_of_workers to capture productivity per worker.",
        "features": ["smv", "no_of_workers"],
        "business_justification": "This feature captures individual worker productivity, which is crucial for understanding team performance.",
        "expected_impact": "Better understanding of individual contributions to productivity.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 2: Create a new feature 'smv_per_worker' by dividing smv by no_of_workers to capture productivity per worker.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['smv_per_worker'] = X_train_copy['smv'] / (X_train_copy['no_of_workers'] + 1e-8)\nX_test_copy['smv_per_worker'] = X_test_copy['smv'] / (X_test_copy['no_of_workers'] + 1e-8)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 2\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 2\nTraining set R²:  0.9285805929700276\nTesting set R²:  0.4540975355340855\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 3,
        "action": "Create a new feature 'idle_time_per_worker' by dividing idle_time by no_of_workers to understand individual idle time.",
        "features": ["idle_time", "no_of_workers"],
        "business_justification": "This feature helps identify if idle time is evenly distributed or concentrated among specific workers.",
        "expected_impact": "Improved understanding of idle time distribution.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 3: Create a new feature 'idle_time_per_worker' by dividing idle_time by no_of_workers to understand individual idle time.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['idle_time_per_worker'] = X_train_copy['idle_time'] / (X_train_copy['no_of_workers'] + 1e-8)\nX_test_copy['idle_time_per_worker'] = X_test_copy['idle_time'] / (X_test_copy['no_of_workers'] + 1e-8)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 3\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 3\nTraining set R²:  0.930401618857855\nTesting set R²:  0.44837149360036654\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 4,
        "action": "Create a new feature 'productivity_rolling_avg' by taking a 7-day rolling average of actual_productivity to smooth out daily fluctuations.",
        "features": ["actual_productivity"],
        "business_justification": "This feature captures the trend in productivity over time, reducing noise.",
        "expected_impact": "Improved model stability by reducing daily fluctuations.",
        "domain_rules_compliance": ["Window functions"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Import necessary libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n# Action 4: Create a new feature 'productivity_rolling_avg' by taking a 7-day rolling average of actual_productivity\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Calculate rolling average on the target variable (y)\ny_train_rolling = y_train_copy.rolling(7).mean().fillna(y_train_copy.mean())\ny_test_rolling = y_test_copy.rolling(7).mean().fillna(y_test_copy.mean())\n\n# Add the new feature to X\nX_train_copy['productivity_rolling_avg'] = y_train_rolling\nX_test_copy['productivity_rolling_avg'] = y_test_rolling\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 4\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n",
        "success": False,
        "result": "",
        "ignore": False,
        "error": "You got some mistake in your previous code. Please re-complete the code to fix the error.\nHere is the previous version:\n```python\n\n# Import necessary libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n# Action 4: Create a new feature 'productivity_rolling_avg' by taking a 7-day rolling average of actual_productivity\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Calculate rolling average on the target variable (y)\ny_train_rolling = y_train_copy.rolling(7).mean().fillna(y_train_copy.mean())\ny_test_rolling = y_test_copy.rolling(7).mean().fillna(y_test_copy.mean())\n\n# Add the new feature to X\nX_train_copy['productivity_rolling_avg'] = y_train_rolling\nX_test_copy['productivity_rolling_avg'] = y_test_rolling\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 4\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n```\nWhen we run the above code, it raises this error:\n```sh\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[15], line 12\n      9 y_test_copy = y_test.copy()\n     11 # Calculate rolling average on the target variable (y)\n---> 12 y_train_rolling = y_train_copy.rolling(7).mean().fillna(y_train_copy.mean())\n     13 y_test_rolling = y_test_copy.rolling(7).mean().fillna(y_test_copy.mean())\n     15 # Add the new feature to X\n\nAttributeError: 'numpy.ndarray' object has no attribute 'rolling'\n```",
        "accepted": "false",
    },
    {
        "id": 5,
        "action": "Create a new feature 'wip_to_workers_ratio' by dividing wip by no_of_workers to understand workload distribution.",
        "features": ["wip", "no_of_workers"],
        "business_justification": "This feature helps understand how workload is distributed among workers, impacting productivity.",
        "expected_impact": "Better understanding of workload distribution.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 5: Create a new feature 'wip_to_workers_ratio' by dividing wip by no_of_workers to understand workload distribution.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['wip_to_workers_ratio'] = X_train_copy['wip'] / (X_train_copy['no_of_workers'] + 1e-8)\nX_test_copy['wip_to_workers_ratio'] = X_test_copy['wip'] / (X_test_copy['no_of_workers'] + 1e-8)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 5\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 5\nTraining set R²:  0.9299335668225199\nTesting set R²:  0.44826608973093884\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 6,
        "action": "Create a new feature 'incentive_effectiveness' by dividing incentive by over_time to measure the impact of incentives on productivity.",
        "features": ["incentive", "over_time"],
        "business_justification": "This feature quantifies how effective incentives are in increasing productivity.",
        "expected_impact": "Improved understanding of incentive effectiveness.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 6: Create a new feature 'incentive_effectiveness' by dividing incentive by over_time to measure the impact of incentives on productivity.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['incentive_effectiveness'] = X_train_copy['incentive'] / (X_train_copy['over_time'] + 1e-8)\nX_test_copy['incentive_effectiveness'] = X_test_copy['incentive'] / (X_test_copy['over_time'] + 1e-8)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 6\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 6\nTraining set R²:  0.9307830348331566\nTesting set R²:  0.44218353577088165\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 7,
        "action": "Create a new feature 'idle_percentage' by dividing idle_time by (total available time) to understand idle time as a percentage of total time.",
        "features": ["idle_time"],
        "business_justification": "This feature provides a relative measure of idle time, making it easier to compare across different periods.",
        "expected_impact": "Improved understanding of idle time impact.",
        "domain_rules_compliance": ["Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 7: Create a new feature 'idle_percentage' by dividing idle_time by (total available time) to understand idle time as a percentage of total time.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Assuming total available time is 24 hours (1 day)\nX_train_copy['idle_percentage'] = (X_train_copy['idle_time'] / 24) * 100\nX_test_copy['idle_percentage'] = (X_test_copy['idle_time'] / 24) * 100\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 7\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 7\nTraining set R²:  0.9304791922885336\nTesting set R²:  0.44810546366656345\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 8,
        "action": "Create a new feature 'style_change_frequency' by dividing no_of_style_change by no_of_workers to understand the impact of style changes per worker.",
        "features": ["no_of_style_change", "no_of_workers"],
        "business_justification": "This feature captures how frequently style changes affect each worker, impacting productivity.",
        "expected_impact": "Better understanding of style change impact on workers.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Action 8: Create a new feature 'style_change_frequency' by dividing no_of_style_change by no_of_workers to understand the impact of style changes per worker.\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\nX_train_copy['style_change_frequency'] = X_train_copy['no_of_style_change'] / (X_train_copy['no_of_workers'] + 1e-8)\nX_test_copy['style_change_frequency'] = X_test_copy['no_of_style_change'] / (X_test_copy['no_of_workers'] + 1e-8)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 8\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n\n",
        "success": True,
        "result": "Action number: 8\nTraining set R²:  0.930682372803769\nTesting set R²:  0.4499707034893986\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 9,
        "action": "Create a new feature 'productivity_gap' by subtracting actual_productivity from targeted_productivity to measure performance gaps.",
        "features": ["actual_productivity", "targeted_productivity"],
        "business_justification": "This feature quantifies the difference between actual and targeted productivity, highlighting areas for improvement.",
        "expected_impact": "Improved identification of performance gaps.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Import necessary libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n# Action 9: Create a new feature 'productivity_gap' by subtracting actual_productivity from targeted_productivity\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Calculate productivity gap using the target variable (y) instead of X\nX_train_copy['productivity_gap'] = X_train_copy['targeted_productivity'] - y_train_copy\nX_test_copy['productivity_gap'] = X_test_copy['targeted_productivity'] - y_test_copy\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 9\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n",
        "success": True,
        "result": "Action number: 9\nTraining set R²:  0.9976980292076344\nTesting set R²:  0.9813907605965453\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
    {
        "id": 10,
        "action": "Create a new feature 'productivity_ratio' by dividing actual_productivity by targeted_productivity to measure productivity efficiency.",
        "features": ["actual_productivity", "targeted_productivity"],
        "business_justification": "This feature provides a relative measure of productivity efficiency, making it easier to compare across different periods.",
        "expected_impact": "Improved understanding of productivity efficiency.",
        "domain_rules_compliance": ["Feature interactions", "Domain-specific calculations"],
        "rejected": "false",
        "rejection_reason": "",
        "code": "\n# Import necessary libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n# Action 10: Create a new feature 'productivity_ratio' by dividing actual_productivity by targeted_productivity\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\ny_train_copy = y_train.copy()\ny_test_copy = y_test.copy()\n\n# Create productivity_ratio using the target variable (y) instead of X\nX_train_copy['productivity_ratio'] = y_train_copy / (X_train_copy['targeted_productivity'] + 1e-8)\nX_test_copy['productivity_ratio'] = y_test_copy / (X_test_copy['targeted_productivity'] + 1e-8)\n\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_train_copy, y_train_copy)\ny_pred = model.predict(X_test_copy)\n\nprint(\"Action number: 10\")\nprint(\"Training set R²: \", r2_score(y_train_copy, model.predict(X_train_copy)))\nprint(\"Testing set R²: \", r2_score(y_test_copy, y_pred))\n\ndel X_train_copy, X_test_copy, y_train_copy, y_test_copy, model, y_pred\n",
        "success": True,
        "result": "Action number: 10\nTraining set R²:  0.9980668016614064\nTesting set R²:  0.9807302746356379\n",
        "ignore": False,
        "error": "",
        "accepted": "true",
    },
]
# Workflow Builder
WORKFLOW_CODE_IRIS = [
    {
        "code": "\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom pathlib import Path\nfrom joblib import dump\n\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    f1_score,\n    mean_absolute_error,\n    mean_squared_error,\n    precision_score,\n    r2_score,\n    recall_score,\n)\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# Load the pickle file.\ndata = pd.read_pickle(r\"C:\\Users\\gengyabc\\.baicai\\tmp\\data\\iris\\baseline_20250419-163142.pkl\")\nX_train, y_train = data.train.xs, data.train.ys.iloc[:, 0]\nX_test, y_test = data.valid.xs, data.valid.ys.iloc[:, 0]\ntry:\n    vocab = data.vocab\nexcept:\n    vocab = {i: v for i, v in enumerate(y_train.unique())}\n\n# if data length larger than 1000, random sample 1000 rows for fast testing the workflow\nif len(X_train) + len(X_test) > 1000:\n    train_test_ratio = len(X_train) / (len(X_train) + len(X_test))\n    X_train = X_train.sample(n=int(1000 * train_test_ratio), random_state=42)\n    X_test = X_test.sample(n=int(1000 * (1 - train_test_ratio)), random_state=42)\n    y_train = y_train.sample(n=int(1000 * train_test_ratio), random_state=42)\n    y_test = y_test.sample(n=int(1000 * (1 - train_test_ratio)), random_state=42)\n\n# Feature Engineering\n\n# Copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\n\n# Action 2: Create a new feature for petal area by multiplying petal length and width.\nX_train_copy['petal_area'] = X_train_copy['petal length (cm)'] * X_train_copy['petal width (cm)']\nX_test_copy['petal_area'] = X_test_copy['petal length (cm)'] * X_test_copy['petal width (cm)']\n# Drop rows with missing values if any\nX_train_copy.dropna(subset=['petal_area'], inplace=True)\nX_test_copy.dropna(subset=['petal_area'], inplace=True)\n\n# Action 3: Compute the ratio of petal length to sepal length.\nX_train_copy['petal_to_sepal_length'] = X_train_copy['petal length (cm)'] / X_train_copy['sepal length (cm)']\nX_test_copy['petal_to_sepal_length'] = X_test_copy['petal length (cm)'] / X_test_copy['sepal length (cm)']\n# Drop rows with missing values if any\nX_train_copy.dropna(subset=['petal_to_sepal_length'], inplace=True)\nX_test_copy.dropna(subset=['petal_to_sepal_length'], inplace=True)\n\n# Action 4: Compute the ratio of petal width to sepal width.\nX_train_copy['petal_to_sepal_width'] = X_train_copy['petal width (cm)'] / X_train_copy['sepal width (cm)']\nX_test_copy['petal_to_sepal_width'] = X_test_copy['petal width (cm)'] / X_test_copy['sepal width (cm)']\n# Drop rows with missing values if any\nX_train_copy.dropna(subset=['petal_to_sepal_width'], inplace=True)\nX_test_copy.dropna(subset=['petal_to_sepal_width'], inplace=True)\n\n# Action 8: Create a feature for the perimeter of the flower by summing all four measurements.\nX_train_copy['perimeter'] = X_train_copy['sepal length (cm)'] + X_train_copy['sepal width (cm)'] + X_train_copy['petal length (cm)'] + X_train_copy['petal width (cm)']\nX_test_copy['perimeter'] = X_test_copy['sepal length (cm)'] + X_test_copy['sepal width (cm)'] + X_test_copy['petal length (cm)'] + X_test_copy['petal width (cm)']\n# Drop rows with missing values if any\nX_train_copy.dropna(subset=['perimeter'], inplace=True)\nX_test_copy.dropna(subset=['perimeter'], inplace=True)\n\n# Action 10: Create a composite feature combining all four measurements through addition.\nX_train_copy['composite_feature'] = X_train_copy['sepal length (cm)'] + X_train_copy['sepal width (cm)'] + X_train_copy['petal length (cm)'] + X_train_copy['petal width (cm)']\nX_test_copy['composite_feature'] = X_test_copy['sepal length (cm)'] + X_test_copy['sepal width (cm)'] + X_test_copy['petal length (cm)'] + X_test_copy['petal width (cm)']\n# Drop rows with missing values if any\nX_train_copy.dropna(subset=['composite_feature'], inplace=True)\nX_test_copy.dropna(subset=['composite_feature'], inplace=True)\n\n# Save the processed data even if no action is implemented\n# 生成时间戳\ntimestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ndata_folder = Path.home() / \".baicai\" / \"tmp\"/ \"data\" / \"iris\"\nif not data_folder.exists():\n    data_folder.mkdir(parents=True, exist_ok=True)\n\nsaved_file_name = data_folder / f\"workflow_{timestamp}.pkl\"\npd.to_pickle({\"X_train\": X_train_copy, \"X_test\": X_test_copy, \"y_train\": y_train, \"y_test\": y_test, \"vocab\": vocab}, saved_file_name)\n\nprint(f\"The pickled data is saved to {saved_file_name}\")\n\n# Model Definitions -- You must use all the models below\nmodels = {\n     'LinearModel': {\n         'model': LogisticRegression(random_state=42),  # Solver lbfgs supports only 'l2' or None penalties, got l1 penalty\n         'param_grid': {\n             'max_iter': [5]  # note: LinearRegression doesn't have a 'max_iter' parameter\n         }\n     },\n     'RandomForest': {\n         'model': RandomForestClassifier(random_state=42),\n         'param_grid': {\n             'n_estimators': [5]\n         }\n     },\n     'LightGBM': {\n         'model': LGBMClassifier(random_state=42, verbose=-1),\n         'param_grid': {\n             'n_estimators': [5]\n         }\n     }\n }\n\n# Model Training and Evaluation using transformed data\nresults = {}\nbest_score = 0\nbest_model = None\n\nprint(\"Model Training Results:\")\nprint(\"-\" * 50)\n\n\n# 保存模型\nmodel_folder = Path.home() / \".baicai\" / \"tmp\"/ \"models\" / \"iris\"\nif not model_folder.exists():\n    model_folder.mkdir(parents=True, exist_ok=True)\n\nif False:\n    cv = TimeSeriesSplit(n_splits=5)\nelse:\n    cv = 2\n\nfor model_name, config in models.items():\n    search = GridSearchCV(config[\"model\"], config[\"param_grid\"], cv=cv, n_jobs=-1)\n    search.fit(X_train_copy, y_train)  # Use transformed training data\n\n    # Get training and test scores using transformed data\n    train_score = search.score(X_train_copy, y_train)\n    test_score = search.score(X_test_copy, y_test)\n    current_score = test_score\n\n    results[model_name] = {\"train_score\": train_score, \"test_score\": test_score, \"best_params\": search.best_params_}\n\n    # Print individual model results\n    print(f\"{model_name} Model:\")\n    print(f\"Training Score: {train_score:.4f}\")\n    print(f\"Testing Score:  {test_score:.4f}\")\n    print(f\"Best Parameters: {search.best_params_}\")\n\n    if current_score > best_score:\n        best_score = current_score\n        best_model = model_name\n\n    model_path = model_folder / f\"{model_name}_{timestamp}.pkl\"\n    dump(search.best_estimator_, model_path)\n    print(f\"The model is saved to {model_path}\")\n\n# Print best model results\nprint(\"=\" * 50)\nprint(f\"Best Model: {best_model}\")\nprint(f\"Best Model Training Score: {results[best_model]['train_score']:.4f}\")\nprint(f\"Best Model Testing Score:  {results[best_model]['test_score']:.4f}\")\nprint(f\"Best Parameters: {results[best_model]['best_params']}\")\nprint(\"=\" * 50)\n",
        "success": True,
        "result": "The pickled data is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\data\\iris\\workflow_20250419-203050.pkl\nModel Training Results:\n--------------------------------------------------\n,c:\\Users\\gengyabc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\baicai-6ddbNBXo-py3.12\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n,LinearModel Model:\nTraining Score: 0.9583\nTesting Score:  0.9667\nBest Parameters: {'max_iter': 5}\nThe model is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\iris\\LinearModel_20250419-203050.pkl\n,RandomForest Model:\nTraining Score: 0.9750\nTesting Score:  1.0000\nBest Parameters: {'n_estimators': 5}\nThe model is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\iris\\RandomForest_20250419-203050.pkl\n,LightGBM Model:\nTraining Score: 0.9667\nTesting Score:  1.0000\nBest Parameters: {'n_estimators': 5}\nThe model is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\iris\\LightGBM_20250419-203050.pkl\n==================================================\nBest Model: RandomForest\nBest Model Training Score: 0.9750\nBest Model Testing Score:  1.0000\nBest Parameters: {'n_estimators': 5}\n==================================================\n",
        "error": "",
    }
]

WORKFLOW_CODE_HOUSE = [
    {
        "code": "\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom pathlib import Path\nfrom joblib import dump\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the data\ndata = pd.read_pickle(r\"C:\\Users\\gengyabc\\.baicai\\tmp\\data\\house\\baseline_20250419-155204.pkl\")\nX_train, y_train = data.train.xs, data.train.ys.iloc[:, 0]\nX_test, y_test = data.valid.xs, data.valid.ys.iloc[:, 0]\n\n# Sample data if too large\nif len(X_train) + len(X_test) > 1000:\n    train_test_ratio = len(X_train) / (len(X_train) + len(X_test))\n    X_train = X_train.sample(n=int(1000 * train_test_ratio), random_state=42)\n    X_test = X_test.sample(n=int(1000 * (1 - train_test_ratio)), random_state=42)\n    y_train = y_train.sample(n=int(1000 * train_test_ratio), random_state=42)\n    y_test = y_test.sample(n=int(1000 * (1 - train_test_ratio)), random_state=42)\n\n# Feature Engineering\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\n\n# Action 1: LogPopulation\nX_train_copy['LogPopulation'] = np.log(X_train_copy['Population'])\nX_test_copy['LogPopulation'] = np.log(X_test_copy['Population'])\nX_train_copy.dropna(subset=['LogPopulation'], inplace=True)\nX_test_copy.dropna(subset=['LogPopulation'], inplace=True)\n\n# Action 2: RoomsPerBedroom\nX_train_copy['RoomsPerBedroom'] = X_train_copy['AveRooms'] / X_train_copy['AveBedrms']\nX_test_copy['RoomsPerBedroom'] = X_test_copy['AveRooms'] / X_test_copy['AveBedrms']\nX_train_copy.dropna(subset=['RoomsPerBedroom'], inplace=True)\nX_test_copy.dropna(subset=['RoomsPerBedroom'], inplace=True)\n\n# Action 3: PopulationDensity\nX_train_copy['PopulationDensity'] = X_train_copy['Population'] / X_train_copy['AveRooms']\nX_test_copy['PopulationDensity'] = X_test_copy['Population'] / X_test_copy['AveRooms']\nX_train_copy.dropna(subset=['PopulationDensity'], inplace=True)\nX_test_copy.dropna(subset=['PopulationDensity'], inplace=True)\n\n# Action 4: HouseAgeGroup\nX_train_copy['HouseAgeGroup'] = pd.qcut(X_train_copy['HouseAge'], q=4, labels=False)\nX_test_copy['HouseAgeGroup'] = pd.qcut(X_test_copy['HouseAge'], q=4, labels=False)\n\n# Action 5: LatitudeLongitudeInteraction\nX_train_copy['LatitudeLongitudeInteraction'] = X_train_copy['Latitude'] * X_train_copy['Longitude']\nX_test_copy['LatitudeLongitudeInteraction'] = X_test_copy['Latitude'] * X_test_copy['Longitude']\n\n# Action 6: IncomePerRoom\nX_train_copy['IncomePerRoom'] = X_train_copy['MedInc'] / X_train_copy['AveRooms']\nX_test_copy['IncomePerRoom'] = X_test_copy['MedInc'] / X_test_copy['AveRooms']\nX_train_copy.dropna(subset=['IncomePerRoom'], inplace=True)\nX_test_copy.dropna(subset=['IncomePerRoom'], inplace=True)\n\n# Action 7: OccupancyPerRoom\nX_train_copy['OccupancyPerRoom'] = X_train_copy['AveOccup'] / X_train_copy['AveRooms']\nX_test_copy['OccupancyPerRoom'] = X_test_copy['AveOccup'] / X_test_copy['AveRooms']\nX_train_copy.dropna(subset=['OccupancyPerRoom'], inplace=True)\nX_test_copy.dropna(subset=['OccupancyPerRoom'], inplace=True)\n\n# Action 8: HouseValuePerPerson\nX_train_copy['HouseValuePerPerson'] = y_train / X_train_copy['Population']\nX_test_copy['HouseValuePerPerson'] = y_test / X_test_copy['Population']\nX_train_copy.dropna(subset=['HouseValuePerPerson'], inplace=True)\nX_test_copy.dropna(subset=['HouseValuePerPerson'], inplace=True)\n\n# Action 9: RoomDensity\nX_train_copy['RoomDensity'] = X_train_copy['AveRooms'] / X_train_copy['Population']\nX_test_copy['RoomDensity'] = X_test_copy['AveRooms'] / X_test_copy['Population']\nX_train_copy.dropna(subset=['RoomDensity'], inplace=True)\nX_test_copy.dropna(subset=['RoomDensity'], inplace=True)\n\n# Action 10: AgeIncomeInteraction\nX_train_copy['AgeIncomeInteraction'] = X_train_copy['HouseAge'] * X_train_copy['MedInc']\nX_test_copy['AgeIncomeInteraction'] = X_test_copy['HouseAge'] * X_test_copy['MedInc']\n\n# Save processed data\ntimestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ndata_folder = Path.home() / \".baicai\" / \"tmp\" / \"data\" / \"house\"\nif not data_folder.exists():\n    data_folder.mkdir(parents=True, exist_ok=True)\nsaved_file_name = data_folder / f\"workflow_{timestamp}.pkl\"\npd.to_pickle({\"X_train\": X_train_copy, \"X_test\": X_test_copy, \"y_train\": y_train, \"y_test\": y_test}, saved_file_name)\n\n# Model Definitions\nmodels = {\n    'LinearModel': {\n        'model': LinearRegression(),\n        'param_grid': {}\n    },\n    'RandomForest': {\n        'model': RandomForestRegressor(random_state=42),\n        'param_grid': {\n            'n_estimators': [10]\n        }\n    },\n    'LightGBM': {\n        'model': LGBMRegressor(random_state=42),\n        'param_grid': {\n            'n_estimators': [10]\n        }\n    }\n}\n\n# Model Training and Evaluation\nresults = {}\nbest_score = 0\nbest_model = None\n\nprint(\"Model Training Results:\")\nprint(\"-\" * 50)\n\n# Save models\nmodel_folder = Path.home() / \".baicai\" / \"tmp\" / \"models\" / \"house\"\nif not model_folder.exists():\n    model_folder.mkdir(parents=True, exist_ok=True)\n\ncv = 2  # Using simple cross-validation\n\nfor model_name, config in models.items():\n    search = GridSearchCV(config[\"model\"], config[\"param_grid\"], cv=cv, n_jobs=-1)\n    search.fit(X_train_copy, y_train)\n\n    train_score = search.score(X_train_copy, y_train)\n    test_score = search.score(X_test_copy, y_test)\n    current_score = test_score\n\n    results[model_name] = {\"train_score\": train_score, \"test_score\": test_score, \"best_params\": search.best_params_}\n\n    print(f\"{model_name} Model:\")\n    print(f\"Training R²: {train_score:.4f}\")\n    print(f\"Testing R²:  {test_score:.4f}\")\n    print(f\"Best Parameters: {search.best_params_}\")\n\n    if current_score > best_score:\n        best_score = current_score\n        best_model = model_name\n\n    model_path = model_folder / f\"{model_name}_{timestamp}.pkl\"\n    dump(search.best_estimator_, model_path)\n    print(f\"Model saved to {model_path}\")\n\nprint(\"=\" * 50)\nprint(f\"Best Model: {best_model}\")\nprint(f\"Best Model Training Score: {results[best_model]['train_score']:.4f}\")\nprint(f\"Best Model Testing Score:  {results[best_model]['test_score']:.4f}\")\nprint(f\"Best Parameters: {results[best_model]['best_params']}\")\nprint(\"=\" * 50)\n",
        "success": True,
        "result": "Model Training Results:\n--------------------------------------------------\n,LinearModel Model:\nTraining R²: 0.7516\nTesting R²:  -1.2767\nBest Parameters: {}\nModel saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\house\\LinearModel_20250419-204055.pkl\n,RandomForest Model:\nTraining R²: 0.9674\nTesting R²:  0.8241\nBest Parameters: {'n_estimators': 10}\nModel saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\house\\RandomForest_20250419-204055.pkl\n,[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4127\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 18\n[LightGBM] [Info] Start training from score 1.044757\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nLightGBM Model:\nTraining R²: 0.7443\nTesting R²:  0.6813\nBest Parameters: {'n_estimators': 10}\nModel saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\house\\LightGBM_20250419-204055.pkl\n==================================================\nBest Model: RandomForest\nBest Model Training Score: 0.9674\nBest Model Testing Score:  0.8241\nBest Parameters: {'n_estimators': 10}\n==================================================\n",
        "error": "",
    }
]

WORKFLOW_CODE_TITANIC = [
    {
        "code": "\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom pathlib import Path\nfrom joblib import dump\n\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    f1_score,\n    mean_absolute_error,\n    mean_squared_error,\n    precision_score,\n    r2_score,\n    recall_score,\n)\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# Load the pickle file.\ndata = pd.read_pickle(r\"C:\\Users\\gengyabc\\.baicai\\tmp\\data\\titanic\\baseline_20250419-163212.pkl\")\nX_train, y_train = data.train.xs, data.train.ys.iloc[:, 0]\nX_test, y_test = data.valid.xs, data.valid.ys.iloc[:, 0]\ntry:\n    vocab = data.vocab\nexcept:\n    vocab = {i: v for i, v in enumerate(y_train.unique())}\n\n# if data length larger than 1000, random sample 1000 rows for fast testing the workflow\nif len(X_train) + len(X_test) > 1000:\n    train_test_ratio = len(X_train) / (len(X_train) + len(X_test))\n    X_train = X_train.sample(n=int(1000 * train_test_ratio), random_state=42)\n    X_test = X_test.sample(n=int(1000 * (1 - train_test_ratio)), random_state=42)\n    y_train = y_train.sample(n=int(1000 * train_test_ratio), random_state=42)\n    y_test = y_test.sample(n=int(1000 * (1 - train_test_ratio)), random_state=42)\n\n# Feature Engineering\n\n# Copy the data\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\n\n# Action 1: Create family_size\n# Create family_size feature\nX_train_copy['family_size'] = X_train_copy['sibsp'] + X_train_copy['parch'] + 1\nX_test_copy['family_size'] = X_test_copy['sibsp'] + X_test_copy['parch'] + 1\n\n# Action 3: Create age_group\n# Create age_group feature\nbins = [0, 5, 13, 65, 100]\nlabels = ['infant', 'child', 'adult', 'senior']\nX_train_copy['age_group'] = pd.cut(X_train_copy['age'], bins=bins, labels=labels)\nX_test_copy['age_group'] = pd.cut(X_test_copy['age'], bins=bins, labels=labels)\n\n# Encode the categorical feature\nle = LabelEncoder()\nX_train_copy['age_group'] = le.fit_transform(X_train_copy['age_group'])\nX_test_copy['age_group'] = le.transform(X_test_copy['age_group'])\n\n# Action 7: Create class_sex_survival\n# Create class_sex_survival feature\nX_train_copy['class_sex_survival'] = X_train_copy['pclass'] * X_train_copy['sex']\nX_test_copy['class_sex_survival'] = X_test_copy['pclass'] * X_test_copy['sex']\n\n# Action 9: Create age_class\n# Create age_class feature\nX_train_copy['age_class'] = X_train_copy['age'] * X_train_copy['pclass']\nX_test_copy['age_class'] = X_test_copy['age'] * X_test_copy['pclass']\n\n# Drop features with missing values if any\nX_train_copy.dropna(axis=1, inplace=True)\nX_test_copy.dropna(axis=1, inplace=True)\n\n# Save the processed data even if no action is implemented\n# 生成时间戳\ntimestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ndata_folder = Path.home() / \".baicai\" / \"tmp\"/ \"data\" / \"titanic\"\nif not data_folder.exists():\n    data_folder.mkdir(parents=True, exist_ok=True)\n\nsaved_file_name = data_folder / f\"workflow_{timestamp}.pkl\"\npd.to_pickle({\"X_train\": X_train_copy, \"X_test\": X_test_copy, \"y_train\": y_train, \"y_test\": y_test, \"vocab\": vocab}, saved_file_name)\n\nprint(f\"The pickled data is saved to {saved_file_name}\")\n\n# Model Definitions -- You must use all the models below\nmodels = {\n     'LinearModel': {\n         'model': LogisticRegression(random_state=42),\n         'param_grid': {\n             'max_iter': [5]\n         }\n     },\n     'RandomForest': {\n         'model': RandomForestClassifier(random_state=42),\n         'param_grid': {\n             'n_estimators': [5]\n         }\n     },\n     'LightGBM': {\n         'model': LGBMClassifier(random_state=42, verbose=-1),\n         'param_grid': {\n             'n_estimators': [5]\n         }\n     }\n }\n\n# Model Training and Evaluation using transformed data\nresults = {}\nbest_score = 0\nbest_model = None\n\nprint(\"Model Training Results:\")\nprint(\"-\" * 50)\n\n\n# 保存模型\nmodel_folder = Path.home() / \".baicai\" / \"tmp\"/ \"models\" / \"titanic\"\nif not model_folder.exists():\n    model_folder.mkdir(parents=True, exist_ok=True)\n\ncv = 2  # Using simple cross-validation\n\nfor model_name, config in models.items():\n    search = GridSearchCV(config[\"model\"], config[\"param_grid\"], cv=cv, n_jobs=-1)\n    search.fit(X_train_copy, y_train)  # Use transformed training data\n\n    # Get training and test scores using transformed data\n    train_score = search.score(X_train_copy, y_train)\n    test_score = search.score(X_test_copy, y_test)\n    current_score = test_score\n\n    results[model_name] = {\"train_score\": train_score, \"test_score\": test_score, \"best_params\": search.best_params_}\n\n    # Print individual model results\n    print(f\"{model_name} Model:\")\n    print(f\"Training Score: {train_score:.4f}\")\n    print(f\"Testing Score:  {test_score:.4f}\")\n    print(f\"Best Parameters: {search.best_params_}\")\n\n    if current_score > best_score:\n        best_score = current_score\n        best_model = model_name\n\n    model_path = model_folder / f\"{model_name}_{timestamp}.pkl\"\n    dump(search.best_estimator_, model_path)\n    print(f\"The model is saved to {model_path}\")\n\n# Print best model results\nprint(\"=\" * 50)\nprint(f\"Best Model: {best_model}\")\nprint(f\"Best Model Training Score: {results[best_model]['train_score']:.4f}\")\nprint(f\"Best Model Testing Score:  {results[best_model]['test_score']:.4f}\")\nprint(f\"Best Parameters: {results[best_model]['best_params']}\")\nprint(\"=\" * 50)\n",
        "success": True,
        "result": "The pickled data is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\data\\titanic\\workflow_20250419-203245.pkl\nModel Training Results:\n--------------------------------------------------\n,c:\\Users\\gengyabc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\baicai-6ddbNBXo-py3.12\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n,LinearModel Model:\nTraining Score: 0.7139\nTesting Score:  0.6461\nBest Parameters: {'max_iter': 5}\nThe model is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\titanic\\LinearModel_20250419-203245.pkl\n,RandomForest Model:\nTraining Score: 0.9551\nTesting Score:  0.7809\nBest Parameters: {'n_estimators': 5}\nThe model is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\titanic\\RandomForest_20250419-203245.pkl\n,LightGBM Model:\nTraining Score: 0.8527\nTesting Score:  0.7697\nBest Parameters: {'n_estimators': 5}\nThe model is saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\titanic\\LightGBM_20250419-203245.pkl\n==================================================\nBest Model: RandomForest\nBest Model Training Score: 0.9551\nBest Model Testing Score:  0.7809\nBest Parameters: {'n_estimators': 5}\n==================================================\n",
        "error": "",
    }
]

WORKFLOW_CODE_GARMENT = [
    {
        "code": "\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom pathlib import Path\nfrom joblib import dump\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the data\ndata = pd.read_pickle(r\"C:\\Users\\gengyabc\\.baicai\\tmp\\data\\garment\\baseline_20250419-163314.pkl\")\nX_train, y_train = data.train.xs, data.train.ys.iloc[:, 0]\nX_test, y_test = data.valid.xs, data.valid.ys.iloc[:, 0]\n\n# Sample data if too large\nif len(X_train) + len(X_test) > 1000:\n    train_test_ratio = len(X_train) / (len(X_train) + len(X_test))\n    X_train = X_train.sample(n=int(1000 * train_test_ratio), random_state=42)\n    X_test = X_test.sample(n=int(1000 * (1 - train_test_ratio)), random_state=42)\n    y_train = y_train.sample(n=int(1000 * train_test_ratio), random_state=42)\n    y_test = y_test.sample(n=int(1000 * (1 - train_test_ratio)), random_state=42)\n\n# Feature Engineering\nX_train_copy = X_train.copy()\nX_test_copy = X_test.copy()\n\n# Action 1: log_over_time\nX_train_copy['log_over_time'] = np.log(X_train_copy['over_time'] + 1e-8)\nX_test_copy['log_over_time'] = np.log(X_test_copy['over_time'] + 1e-8)\nif 'log_over_time' in X_train_copy.columns:\n    X_train_copy.dropna(subset=['log_over_time'], inplace=True)\n    X_test_copy.dropna(subset=['log_over_time'], inplace=True)\n\n# Action 2: smv_per_worker\nX_train_copy['smv_per_worker'] = X_train_copy['smv'] / (X_train_copy['no_of_workers'] + 1e-8)\nX_test_copy['smv_per_worker'] = X_test_copy['smv'] / (X_test_copy['no_of_workers'] + 1e-8)\nif 'smv_per_worker' in X_train_copy.columns:\n    X_train_copy.dropna(subset=['smv_per_worker'], inplace=True)\n    X_test_copy.dropna(subset=['smv_per_worker'], inplace=True)\n\n# Action 3: idle_time_per_worker\nX_train_copy['idle_time_per_worker'] = X_train_copy['idle_time'] / (X_train_copy['no_of_workers'] + 1e-8)\nX_test_copy['idle_time_per_worker'] = X_test_copy['idle_time'] / (X_test_copy['no_of_workers'] + 1e-8)\nif 'idle_time_per_worker' in X_train_copy.columns:\n    X_train_copy.dropna(subset=['idle_time_per_worker'], inplace=True)\n    X_test_copy.dropna(subset=['idle_time_per_worker'], inplace=True)\n\n# Action 5: wip_to_workers_ratio\nX_train_copy['wip_to_workers_ratio'] = X_train_copy['wip'] / (X_train_copy['no_of_workers'] + 1e-8)\nX_test_copy['wip_to_workers_ratio'] = X_test_copy['wip'] / (X_test_copy['no_of_workers'] + 1e-8)\nif 'wip_to_workers_ratio' in X_train_copy.columns:\n    X_train_copy.dropna(subset=['wip_to_workers_ratio'], inplace=True)\n    X_test_copy.dropna(subset=['wip_to_workers_ratio'], inplace=True)\n\n# Action 6: incentive_effectiveness\nX_train_copy['incentive_effectiveness'] = X_train_copy['incentive'] / (X_train_copy['over_time'] + 1e-8)\nX_test_copy['incentive_effectiveness'] = X_test_copy['incentive'] / (X_test_copy['over_time'] + 1e-8)\nif 'incentive_effectiveness' in X_train_copy.columns:\n    X_train_copy.dropna(subset=['incentive_effectiveness'], inplace=True)\n    X_test_copy.dropna(subset=['incentive_effectiveness'], inplace=True)\n\n# Action 7: idle_percentage\nX_train_copy['idle_percentage'] = (X_train_copy['idle_time'] / 24) * 100\nX_test_copy['idle_percentage'] = (X_test_copy['idle_time'] / 24) * 100\n\n# Action 8: style_change_frequency\nX_train_copy['style_change_frequency'] = X_train_copy['no_of_style_change'] / (X_train_copy['no_of_workers'] + 1e-8)\nX_test_copy['style_change_frequency'] = X_test_copy['no_of_style_change'] / (X_test_copy['no_of_workers'] + 1e-8)\nif 'style_change_frequency' in X_train_copy.columns:\n    X_train_copy.dropna(subset=['style_change_frequency'], inplace=True)\n    X_test_copy.dropna(subset=['style_change_frequency'], inplace=True)\n\n# Action 9: productivity_gap\nX_train_copy['productivity_gap'] = X_train_copy['targeted_productivity'] - y_train\nX_test_copy['productivity_gap'] = X_test_copy['targeted_productivity'] - y_test\n\n# Action 10: productivity_ratio\nX_train_copy['productivity_ratio'] = y_train / (X_train_copy['targeted_productivity'] + 1e-8)\nX_test_copy['productivity_ratio'] = y_test / (X_test_copy['targeted_productivity'] + 1e-8)\nif 'productivity_ratio' in X_train_copy.columns:\n    X_train_copy.dropna(subset=['productivity_ratio'], inplace=True)\n    X_test_copy.dropna(subset=['productivity_ratio'], inplace=True)\n\n# Save processed data\ntimestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ndata_folder = Path.home() / \".baicai\" / \"tmp\" / \"data\" / \"garment\"\nif not data_folder.exists():\n    data_folder.mkdir(parents=True, exist_ok=True)\nsaved_file_name = data_folder / f\"workflow_{timestamp}.pkl\"\npd.to_pickle({\"X_train\": X_train_copy, \"X_test\": X_test_copy, \"y_train\": y_train, \"y_test\": y_test}, saved_file_name)\n\n# Model Definitions\nmodels = {\n    'LinearModel': {\n        'model': LinearRegression(),\n        'param_grid': {}\n    },\n    'RandomForest': {\n        'model': RandomForestRegressor(random_state=42),\n        'param_grid': {'n_estimators': [10]}\n    },\n    'LightGBM': {\n        'model': LGBMRegressor(random_state=42),\n        'param_grid': {'n_estimators': [10]}\n    }\n}\n\n# Model Training and Evaluation\nresults = {}\nbest_score = -float('inf')\nbest_model = None\n\nprint(\"Model Training Results:\")\nprint(\"-\" * 50)\n\n# Save models\nmodel_folder = Path.home() / \".baicai\" / \"tmp\" / \"models\" / \"garment\"\nif not model_folder.exists():\n    model_folder.mkdir(parents=True, exist_ok=True)\n\ncv = TimeSeriesSplit(n_splits=5)\n\nfor model_name, config in models.items():\n    model = config['model']\n    model.fit(X_train_copy, y_train)\n    \n    y_pred_train = model.predict(X_train_copy)\n    y_pred_test = model.predict(X_test_copy)\n    \n    train_r2 = r2_score(y_train, y_pred_train)\n    test_r2 = r2_score(y_test, y_pred_test)\n    \n    results[model_name] = {'train_r2': train_r2, 'test_r2': test_r2}\n    \n    print(f\"{model_name} Model:\")\n    print(f\"Training R²: {train_r2:.4f}\")\n    print(f\"Testing R²: {test_r2:.4f}\")\n    \n    if test_r2 > best_score:\n        best_score = test_r2\n        best_model = model_name\n    \n    model_path = model_folder / f\"{model_name}_{timestamp}.pkl\"\n    dump(model, model_path)\n    print(f\"Model saved to {model_path}\")\n\nprint(\"=\" * 50)\nprint(f\"Best Model: {best_model}\")\nprint(f\"Best Testing R²: {best_score:.4f}\")\nprint(\"=\" * 50)\n",
        "success": True,
        "result": "Model Training Results:\n--------------------------------------------------\nLinearModel Model:\nTraining R²: 1.0000\nTesting R²: 1.0000\nModel saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\garment\\LinearModel_20250419-204326.pkl\n,RandomForest Model:\nTraining R²: 0.9973\nTesting R²: 0.9839\nModel saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\garment\\RandomForest_20250419-204326.pkl\n,[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000294 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1784\n[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 27\n[LightGBM] [Info] Start training from score 0.543987\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nLightGBM Model:\nTraining R²: 0.9937\nTesting R²: 0.9757\nModel saved to C:\\Users\\gengyabc\\.baicai\\tmp\\models\\garment\\LightGBM_20250419-204326.pkl\n==================================================\nBest Model: LinearModel\nBest Testing R²: 1.0000\n==================================================\n",
        "error": "",
    }
]

ACTION_FEEDBACK = r"""
You got some mistake in your previous code. Please re-complete the code to fix the error. 
Here is the previous version:
```python
'\nimport pandas as pd\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import SimpleImputer\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.inspection import permutation_importance\n\n# Load data\ndf = pd.read_csv(\'d:\\\\onedrive - sziit.edu.cn\\\\programming\\\\baicai\\\\examples\\\\data\\\\house.csv\')\n\n# Select features and target\nX = df.drop([\'id\', \'date\', \'zipcode\', \'price\'], axis=1)\ny = df[\'price\']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create preprocessing steps\nnumerical_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns.tolist()\n\n# Create preprocessing pipelines\ntransformers = []\nif numerical_features:\n    numerical_pipeline = Pipeline([\n        (\'imputer\', SimpleImputer(strategy=\'mean\')),\n    ])\n    transformers.append((\'num\', numerical_pipeline, numerical_features))\n\n# Create preprocessor\npreprocessor = ColumnTransformer(transformers, remainder=\'drop\', verbose_feature_names_out=False)\n\n# Fit preprocessor and get feature names\npreprocessor.fit(X_train)\nfeature_names = preprocessor.get_feature_names_out()\n\n# Transform data\nX_train_preprocessed = pd.DataFrame(\n    preprocessor.transform(X_train),\n    columns=feature_names\n)\nX_test_preprocessed = pd.DataFrame(\n    preprocessor.transform(X_test),\n    columns=feature_names\n)\n\n# Fit regressor\nmodel = RandomForestRegressor(n_estimators=10, random_state=42)\nmodel.fit(X_train_preprocessed, y_train)\n\n# Print basic metrics\ntrain_score = r2_score(y_train, model.predict(X_train_preprocessed))\ntest_score = r2_score(y_test, model.predict(X_test_preprocessed))\n\nprint(f"Training R2 score: {train_score:.4f}")\nprint(f"Testing R2 score: {test_score:.4f}")\n\n# Calculate feature importance\nresult = permutation_importance(\n    model,\n    X_train_preprocessed,\n    y_train,\n    n_repeats=10,\n    random_state=42\n)\n\n# Print feature importance\nimportance_df = pd.DataFrame({\n    \'feature\': X.columns,\n    \'importance\': result.importances_mean\n}).sort_values(\'importance\', ascending=False)\n\nprint("\\nFeature Importance:")\nfor idx, row in importance_df.iterrows():\n    print(f"{row[\'feature\']}: {row[\'importance\']:.4f}")\n\n# Save processed data\ndata = {\n    \'X_train\': X_train_preprocessed,\n    \'y_train\': y_train,\n    \'X_test\': X_test_preprocessed,\n    \'y_test\': y_test\n}\npd.to_pickle(data, \'d:\\\\onedrive - sziit.edu.cn\\\\programming\\\\baicai\\\\examples\\\\data\\\\clean_house.pkl\')\nprint("The pickled data is saved to d:\\\\onedrive - sziit.edu.cn\\\\programming\\\\baicai\\\\examples\\\\data\\\\clean_house.pkl")\n'
```

When we run the above code, it raises this error:

---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 7\n      5 from sklearn.ensemble import RandomForestRegressor\n      6 from sklearn.model_selection import train_test_split\n----> 7 from sklearn.preprocessing import SimpleImputer\n      8 from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n      9 from sklearn.inspection import permutation_importance\n\nImportError: cannot import name 'SimpleImputer' from 'sklearn.preprocessing' (c:\\Users\\gengyabc\\miniconda3\\envs\\llm\\Lib\\site-packages\\sklearn\\preprocessing\\__init__.py)"


"""
