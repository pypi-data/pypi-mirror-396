\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\geometry{margin=1in}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\title{Election Forecast Diffusion Model:\\Mathematical Foundations and Derivations}
\author{Implementation: forecast\_diffusion.py}
\date{November 2025}

\begin{document}
\maketitle

\begin{abstract}
We present a state-space diffusion model for forecasting U.S. presidential elections using time-series polling data. The model treats the latent two-party vote margin in each state as a continuous-time diffusion process with drift, observed through noisy polls from multiple pollsters. We derive the Expectation-Maximization (EM) algorithm for parameter estimation using Kalman filtering and Rauch-Tung-Striebel (RTS) smoothing, and describe forward simulation for probabilistic forecasting.
\end{abstract}

\section{Model Specification}

\subsection{State Variables and Notation}

For each state $s$, let:
\begin{itemize}
    \item $X_t^{(s)}$ = latent two-party margin (Democrat - Republican) at continuous time $t$
    \item $t \in [t_0, T_{elec}]$ where $T_{elec}$ is election day
    \item $y_i$ = observed poll margin from poll $i$ at time $t_i$
    \item $p(i)$ = pollster conducting poll $i$
    \item $n_i$ = sample size of poll $i$
\end{itemize}

\subsection{Latent Process Model}

The latent margin follows a \textbf{Brownian motion with drift}:
\begin{equation}
dX_t = \mu \, dt + \sigma \, dW_t
\end{equation}
where $W_t$ is standard Brownian motion, $\mu$ is the drift parameter, and $\sigma^2$ is the diffusion coefficient.

In discrete time with $\Delta t = 1$ day:
\begin{equation}
X_{t+1} = X_t + \mu + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \sigma^2)
\end{equation}

\textbf{Interpretation:}
\begin{itemize}
    \item $\mu > 0$: systematic trend toward Democrats
    \item $\mu < 0$: systematic trend toward Republicans
    \item $\sigma^2$: day-to-day volatility in voter preferences
\end{itemize}

\subsection{Observation Model}

Poll $i$ at time $t_i$ observes:
\begin{equation}
y_i = X_{t_i} + b_{p(i)} + \eta_i
\end{equation}
where:
\begin{itemize}
    \item $b_p$ = systematic bias of pollster $p$ (house effect)
    \item $\eta_i \sim \mathcal{N}(0, \tau_i^2)$ = poll-specific measurement error
\end{itemize}

\subsection{Observation Variance}

The observation variance combines sampling error and systematic error:
\begin{equation}
\tau_i^2 = \underbrace{\frac{p_i(1-p_i)}{n_i}}_{\text{sampling variance}} + \tau_{extra}^2
\end{equation}
where:
\begin{itemize}
    \item $p_i = \frac{\text{dem}_i}{\text{dem}_i + \text{rep}_i}$ = observed Dem two-party share in poll $i$
    \item $n_i$ = sample size
    \item $\tau_{extra}^2$ = additional variance capturing methodology differences, non-response, etc.
\end{itemize}

\textbf{Derivation of sampling variance:}
For a binomial proportion $\hat{p}$ with $n$ samples:
\[
\text{Var}(\hat{p}) = \frac{p(1-p)}{n}
\]
The two-party margin is $m = 2p - 1$, so:
\[
\text{Var}(m) = \text{Var}(2p) = 4 \cdot \frac{p(1-p)}{n}
\]
For computational stability, we use the approximation $\frac{p(1-p)}{n}$ in the code.

\section{Parameter Estimation via EM Algorithm}

\subsection{Unknown Parameters}

For each state, we estimate:
\begin{align}
\theta_s &= \{\mu_s, \sigma_s^2, \{b_p\}_{p \in \mathcal{P}_s}, \tau_{extra}^2\}
\end{align}
where $\mathcal{P}_s$ is the set of pollsters polling state $s$.

\subsection{Complete-Data Likelihood}

If we observed the latent path $\{X_t\}_{t=t_0}^{T}$ exactly, the log-likelihood would be:
\begin{align}
\log p(\{X_t\}, \{y_i\} | \theta) = &\sum_{t=1}^{T} \log p(X_t | X_{t-1}, \mu, \sigma^2) \\
&+ \sum_{i=1}^{N} \log p(y_i | X_{t_i}, b_{p(i)}, \tau_i^2)
\end{align}

Explicitly:
\begin{align}
\log p(\{X_t\}, \{y_i\} | \theta) = &-\frac{1}{2}\sum_{t=1}^{T} \frac{(X_t - X_{t-1} - \mu \Delta t)^2}{\sigma^2 \Delta t} \\
&-\frac{1}{2}\sum_{i=1}^{N} \frac{(y_i - X_{t_i} - b_{p(i)})^2}{\tau_i^2} + \text{const}
\end{align}

\subsection{EM Algorithm Structure}

\textbf{E-step:} Compute posterior distribution $p(\{X_t\} | \{y_i\}, \theta^{(k)})$ using current parameters $\theta^{(k)}$.

\textbf{M-step:} Update parameters by maximizing expected complete-data log-likelihood:
\begin{equation}
\theta^{(k+1)} = \arg\max_\theta \mathbb{E}_{\{X_t\} | \{y_i\}, \theta^{(k)}} [\log p(\{X_t\}, \{y_i\} | \theta)]
\end{equation}

\section{E-Step: Kalman Filter and RTS Smoother}

\subsection{State-Space Representation}

Define discrete observation times $t_1 < t_2 < \cdots < t_T$. At each time, we may have multiple polls or no polls.

\textbf{State equation:}
\begin{equation}
X_t = X_{t-1} + \mu \Delta t_t + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \sigma^2 \Delta t_t)
\end{equation}
where $\Delta t_t = t_t - t_{t-1}$.

\textbf{Observation equation (if poll available at time $t$):}
\begin{equation}
y_t = X_t + \eta_t, \quad \eta_t \sim \mathcal{N}(0, \tau_t^2)
\end{equation}
If multiple polls at time $t$, we pre-aggregate using inverse-variance weighting:
\begin{equation}
y_t^{agg} = \frac{\sum_i w_i y_i}{\sum_i w_i}, \quad w_i = \frac{1}{\tau_i^2}
\end{equation}
with aggregated variance:
\begin{equation}
(\tau_t^{agg})^2 = \frac{1}{\sum_i w_i}
\end{equation}

\subsection{Kalman Filter (Forward Pass)}

Initialize at $t=0$:
\begin{align}
X_{0|0} &= 0 \quad \text{(prior mean)} \\
P_{0|0} &= 100 \quad \text{(prior variance, diffuse)}
\end{align}

For $t = 1, \ldots, T$:

\textbf{Prediction step:}
\begin{align}
X_{t|t-1} &= X_{t-1|t-1} + \mu \Delta t_t \\
P_{t|t-1} &= P_{t-1|t-1} + \sigma^2 \Delta t_t
\end{align}

\textbf{Update step (if observation $y_t$ available):}
\begin{align}
K_t &= \frac{P_{t|t-1}}{P_{t|t-1} + \tau_t^2} \quad \text{(Kalman gain)} \\
X_{t|t} &= X_{t|t-1} + K_t (y_t - X_{t|t-1}) \\
P_{t|t} &= (1 - K_t) P_{t|t-1}
\end{align}

\textbf{If no observation at time $t$:}
\begin{align}
X_{t|t} &= X_{t|t-1} \\
P_{t|t} &= P_{t|t-1}
\end{align}

\subsection{RTS Smoother (Backward Pass)}

Initialize at $t=T$:
\begin{align}
X_{T|T}^s &= X_{T|T} \\
P_{T|T}^s &= P_{T|T}
\end{align}

For $t = T-1, T-2, \ldots, 1$:
\begin{align}
J_t &= \frac{P_{t|t}}{P_{t+1|t}} \quad \text{(smoother gain)} \\
X_{t|T}^s &= X_{t|t} + J_t (X_{t+1|T}^s - X_{t+1|t}) \\
P_{t|T}^s &= P_{t|t} + J_t^2 (P_{t+1|T}^s - P_{t+1|t})
\end{align}

The RTS smoother produces the posterior mean $X_{t|T}^s$ and variance $P_{t|T}^s$ given all observations.

\section{M-Step: Parameter Updates}

\subsection{Update Pollster Biases}

For each pollster $p$, collect all polls $i$ where $p(i) = p$ and compute residuals:
\begin{equation}
r_i = y_i - X_{t_i|T}^s
\end{equation}

Update bias:
\begin{equation}
b_p^{new} = \frac{1}{N_p} \sum_{i: p(i)=p} r_i
\end{equation}
where $N_p$ is the number of polls by pollster $p$.

\subsection{Update Drift Parameter}

If state has $\geq 6$ polls (sufficient data), estimate drift from smoothed increments:
\begin{equation}
\mu^{new} = \frac{1}{\sum_{t=1}^{T-1} \Delta t_t} \sum_{t=1}^{T-1} (X_{t+1|T}^s - X_{t|T}^s)
\end{equation}

This is the average daily change in the smoothed latent process.

If state has $< 6$ polls, set $\mu = 0$ (agnostic prior).

\subsection{Update Diffusion Coefficient}

Compute variance of residual increments:
\begin{equation}
\sigma^{2, new} = \frac{1}{\sum_{t=1}^{T-1} \Delta t_t} \sum_{t=1}^{T-1} \frac{(X_{t+1|T}^s - X_{t|T}^s - \mu \Delta t_t)^2}{\Delta t_t}
\end{equation}

This measures how much the smoothed path deviates from a pure linear drift.

\textbf{Numerical stability:} Enforce $\sigma^2 \geq 10^{-6}$ to prevent degeneracy.

\subsection{Update Extra Observation Variance}

The extra variance $\tau_{extra}^2$ can be estimated from the variance of bias-corrected residuals:
\begin{equation}
\tau_{extra}^{2, new} = \frac{1}{N} \sum_{i=1}^{N} (y_i - X_{t_i|T}^s - b_{p(i)})^2 - \frac{1}{N}\sum_{i=1}^{N} \frac{p_i(1-p_i)}{n_i}
\end{equation}

In the implementation, we fix $\tau_{extra}^2 = (0.02)^2 = 0.0004$ (2 percentage points).

\section{Forward Simulation for Forecasting}

\subsection{Simulation Procedure}

Given the latent state $X_{t_{last}}^s$ and variance $P_{t_{last}}^s$ at the last observation time, forecast to election day $T_{elec}$ using Euler-Maruyama:

For $j = 1, \ldots, N_{sim}$ (e.g., $N_{sim} = 2000$):
\begin{enumerate}
    \item Initialize: $X_0^{(j)} \sim \mathcal{N}(X_{t_{last}}^s, P_{t_{last}}^s)$
    \item For $d = 1, \ldots, D$ where $D = T_{elec} - t_{last}$ (days to election):
    \begin{equation}
    X_d^{(j)} = X_{d-1}^{(j)} + \mu + \epsilon_d, \quad \epsilon_d \sim \mathcal{N}(0, \sigma^2)
    \end{equation}
    \item Record final margin: $M^{(j)} = X_D^{(j)}$
\end{enumerate}

\subsection{Win Probability}

The probability that the Democrat wins state $s$ is:
\begin{equation}
P(\text{Dem wins } s) = \frac{1}{N_{sim}} \sum_{j=1}^{N_{sim}} \mathbb{1}\{M^{(j)} > 0\}
\end{equation}

\subsection{Predictive Distribution}

The empirical distribution of $\{M^{(j)}\}_{j=1}^{N_{sim}}$ provides:
\begin{itemize}
    \item Mean prediction: $\bar{M} = \frac{1}{N_{sim}} \sum_j M^{(j)}$
    \item Uncertainty: $s_M = \sqrt{\frac{1}{N_{sim}-1} \sum_j (M^{(j)} - \bar{M})^2}$
    \item Quantiles for credible intervals
\end{itemize}

\subsection{Theoretical Justification}

Under the continuous-time model, the terminal value $X_{T_{elec}}$ given $X_{t_{last}}$ is Gaussian:
\begin{equation}
X_{T_{elec}} | X_{t_{last}} \sim \mathcal{N}(X_{t_{last}} + \mu (T_{elec} - t_{last}), \sigma^2 (T_{elec} - t_{last}))
\end{equation}

The simulation approach samples from this distribution while accounting for posterior uncertainty in $X_{t_{last}}$.

\section{Decision Time (Optional Extension)}

\subsection{First-Passage Time}

Define the \textbf{decision time} as the first time the latent margin crosses threshold $c=0$:
\begin{equation}
\tau_c = \inf\{t \geq t_{last} : |X_t| > c\}
\end{equation}

For each simulation path $j$, record:
\begin{equation}
\tau_c^{(j)} = \min\{d : |X_d^{(j)}| > 0\}
\end{equation}

The distribution of $\{\tau_c^{(j)}\}$ provides:
\begin{itemize}
    \item Median decision day
    \item 90\% credible interval $[t_{5\%}, t_{95\%}]$
\end{itemize}

\subsection{Realized Decision Time}

For evaluation, define the \textit{realized} decision time as the earliest day where the final margin sign becomes stable (going backward from election day). If the race was never stable, treat as right-censored.

\section{Evaluation Metrics}

\subsection{Brier Score}

For each state $s$, let $p_s \in [0,1]$ be the predicted win probability and $o_s \in \{0,1\}$ be the actual outcome:
\begin{equation}
\text{BS} = \frac{1}{S} \sum_{s=1}^{S} (p_s - o_s)^2
\end{equation}

Lower is better. Range: $[0, 1]$. Perfect calibration gives BS close to 0.

\subsection{Log Loss (Cross-Entropy)}

\begin{equation}
\text{LL} = -\frac{1}{S} \sum_{s=1}^{S} [o_s \log(p_s + \epsilon) + (1-o_s) \log(1 - p_s + \epsilon)]
\end{equation}
where $\epsilon = 10^{-10}$ prevents numerical issues. Lower is better.

\subsection{Mean Absolute Error (Margin)}

\begin{equation}
\text{MAE} = \frac{1}{S} \sum_{s=1}^{S} |m_s^{pred} - m_s^{actual}|
\end{equation}
where $m_s$ is the two-party margin.

\section{Implementation Notes}

\subsection{Multiple Temporal Splits}

The model is evaluated at forecast dates:
\begin{itemize}
    \item October 1, 2016 (38 days before election)
    \item October 15, 2016 (24 days before)
    \item November 1, 2016 (7 days before)
    \item November 7, 2016 (1 day before)
\end{itemize}

For each date, we:
\begin{enumerate}
    \item Use only polls up to that date for training
    \item Fit $\mu, \sigma^2, \{b_p\}$ via EM
    \item Simulate forward to November 8
    \item Compute metrics against actual results
\end{enumerate}

\subsection{Convergence}

The EM algorithm is run for 10 iterations. In practice, parameters typically converge within 5-7 iterations.

\subsection{Identifiability}

The model is overparameterized: pollster biases $\{b_p\}$ and the latent level $X_t$ are not separately identifiable. We resolve this by initializing $X_0 = 0$ and centering biases around the observed poll means.

\section{Model Improvements and Empirical Results}

\subsection{Problems with Original Implementation}

The initial implementation suffered from:
\begin{enumerate}
    \item \textbf{Overconfidence}: $\sigma^2 = 0.001$ (minimum) produced too little uncertainty, leading to win probabilities of exactly 0.0 or 1.0
    \item \textbf{Pollster bias overfitting}: Estimating $b_p$ independently for each pollster with sparse data
    \item \textbf{No systematic bias correction}: The 2016 cycle had systematic polling errors that favor Democrats
    \item \textbf{Insufficient forecast uncertainty}: Forward simulations didn't account for growing uncertainty with longer horizons
\end{enumerate}

Empirically, the original model's Brier score \textit{worsened} from 0.072 (Oct 1) to 0.098 (Nov 7), the opposite of expected behavior.

\subsection{Improvements Implemented}

We implement the following fixes in \texttt{forecast\_diffusion\_improved.py}:

\subsubsection{1. Regularized Pollster Biases}

Instead of MLE estimates $\hat{b}_p = \frac{1}{N_p}\sum_i r_i$, we shrink toward zero:
\begin{equation}
b_p^{new} = \lambda \cdot \frac{1}{N_p}\sum_{i: p(i)=p} r_i, \quad \lambda = 0.5
\end{equation}

This prevents overfitting to pollster-specific noise.

\subsubsection{2. Fundamentals-Based Prior}

Incorporate 2012 election results as a prior:
\begin{equation}
X_{t|T}^{s,adj} = (1 - w_{prior}) X_{t|T}^s + w_{prior} \cdot m_{2012}^{(s)}
\end{equation}
where $w_{prior} = 0.1$ and $m_{2012}^{(s)}$ is the 2012 two-party margin in state $s$.

\subsubsection{3. Increased Diffusion Variance}

Raise minimum diffusion to $\sigma^2 \geq 0.0005$ (from $0.0001$), and increase observation variance to $\tau_{extra}^2 = (0.03)^2$ (from $0.02^2$).

\subsubsection{4. Forecast Horizon Uncertainty}

Add time-dependent uncertainty to forward simulations:
\begin{equation}
X_d^{(j)} = X_{d-1}^{(j)} + \mu + \epsilon_d, \quad \epsilon_d \sim \mathcal{N}(0, \sigma^2 + \sigma_{horizon}^2)
\end{equation}
where $\sigma_{horizon}^2 = 0.001 \cdot D$ grows with days $D$ to election.

\subsubsection{5. Probability Clipping}

Enforce $P(\text{Dem wins}) \in [0.01, 0.99]$ to avoid extreme overconfidence.

\subsection{Baseline Model: Poll-of-Polls Average}

For comparison, we implement a simple baseline (\texttt{baseline.py}):

\textbf{Algorithm:}
\begin{enumerate}
    \item Take all polls in last 14 days before forecast date
    \item Compute weighted average margin: $\bar{m} = \sum_i w_i y_i / \sum_i w_i$ where $w_i = n_i$ (sample size)
    \item Estimate uncertainty:
    \[
    \sigma_{total}^2 = \max\{\text{Var}(y_i), \overline{\sigma_{sampling}^2}\} + 0.001 \cdot D
    \]
    \item Convert to probability: $P(\text{Dem wins}) = \Phi(\bar{m} / \sigma_{total})$, clipped to $[0.05, 0.95]$
\end{enumerate}

This baseline represents what a journalist would do manually—no state-space modeling, just recent poll aggregation.

\section{Empirical Comparison}

\subsection{Evaluation Setup}

We evaluate three models on 51 states at four forecast dates (Oct 1, Oct 15, Nov 1, Nov 7, 2016):
\begin{itemize}
    \item \textbf{Original Diffusion}: Initial implementation (\texttt{forecast\_diffusion.py})
    \item \textbf{Improved Diffusion}: Enhanced model (\texttt{forecast\_diffusion\_improved.py})
    \item \textbf{Baseline}: Poll-of-polls average (\texttt{baseline.py})
\end{itemize}

\subsection{Results: Average Performance}

\begin{center}
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{Brier Score} & \textbf{Log Loss} & \textbf{MAE} \\
\hline
Baseline (Poll Average) & \textbf{0.0880} & \textbf{0.297} & \textbf{0.0840} \\
Original Diffusion & 0.0921 & 1.582 & 0.0850 \\
Improved Diffusion & 0.1073 & 0.343 & 0.0922 \\
\hline
\end{tabular}
\end{center}

\textbf{Ranking:} Baseline is \#1 on all metrics (average rank = 1.0).

\subsection{Results: Final Forecast (Nov 7)}

One day before election, the improved diffusion model achieves the best Brier score:

\begin{center}
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{Brier Score} & \textbf{Log Loss} & \textbf{MAE} \\
\hline
Improved Diffusion & \textbf{0.0586} & \textbf{0.184} & 0.0676 \\
Baseline & 0.0792 & 0.264 & \textbf{0.0689} \\
Original Diffusion & 0.0981 & 1.896 & 0.0712 \\
CIR Process & 0.2124 & 0.907 & 0.1479 \\
\hline
\end{tabular}
\end{center}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Temporal improvement matters}: The improved diffusion model's Brier score improves from 0.160 (Oct 1) to 0.059 (Nov 7), a 63\% reduction. The original model gets \textit{worse} over time.

    \item \textbf{Simple is robust}: The baseline poll average is most consistent across forecast dates (smallest variance in performance).

    \item \textbf{Sophisticated models need more data}: The diffusion model outperforms on the final forecast but requires careful tuning to avoid overfitting early on.

    \item \textbf{Log Loss reveals overconfidence}: The original model's log loss (1.58) is 5× worse than baseline (0.30), indicating severe overconfidence in wrong predictions.

    \item \textbf{All models struggle with 2016 bias}: Even the best model (Improved, Nov 7) has Brier = 0.059, indicating systematic errors. The 2016 polling bias affected all approaches.
\end{enumerate}

\section{Alternative Model: Cox-Ingersoll-Ross (CIR) Process}

\subsection{Motivation and Specification}

Based on Levene \& Fenner (2020), we also tested a mean-reverting CIR process with gamma-distributed marginals:

\begin{equation}
dX_t = \theta(m - X_t)dt + \sqrt{2\theta X_t / \lambda} \, dW_t
\end{equation}

Key differences from Brownian motion with drift:
\begin{itemize}
    \item \textbf{Mean reversion}: Process reverts to long-term mean $m$ at rate $\theta$
    \item \textbf{State-dependent diffusion}: Variance $\propto X_t$ (ensures positivity)
    \item \textbf{Gamma marginals}: $X_t \sim \text{Gamma}(\alpha, \lambda)$ instead of Gaussian
    \item \textbf{Heavy-tailed autocorrelation}: Sum of 2 CIR processes (fast + slow)
\end{itemize}

\subsection{Implementation}

For Democratic vote proportion $X_t \in (0, 1)$:
\begin{align}
\text{Fast component (85\%):} \quad dX_t^{(1)} &= 0.10(m_1 - X_t^{(1)})dt + \sqrt{0.20 X_t^{(1)} / \lambda} \, dW_t^{(1)} \\
\text{Slow component (15\%):} \quad dX_t^{(2)} &= 0.01(m_2 - X_t^{(2)})dt + \sqrt{0.02 X_t^{(2)} / \lambda} \, dW_t^{(2)} \\
X_t &= X_t^{(1)} + X_t^{(2)}
\end{align}

Parameters $(\alpha, \lambda)$ fitted via method of moments from recent polling data.

\subsection{Results: CIR Performs Poorly}

\begin{center}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Avg Brier} & \textbf{Avg Log Loss} & \textbf{Avg MAE} & \textbf{Rank} \\
\hline
Baseline & 0.0880 & 0.297 & 0.0840 & \textbf{1.0} \\
Original Diffusion & 0.0921 & 1.582 & 0.0850 & 2.7 \\
Improved Diffusion & 0.1073 & 0.343 & 0.0922 & 2.7 \\
\textbf{CIR Process} & \textbf{0.1574} & 0.572 & 0.1152 & \textbf{3.7} \\
\hline
\end{tabular}
\end{center}

\textbf{Critical failure:} CIR Brier score \textit{worsens} over time (0.127 $\rightarrow$ 0.212, +67\%), while Improved Diffusion improves by 63\%.

\subsection{Why CIR Failed}

\begin{enumerate}
    \item \textbf{Mean reversion too strong}: Polls don't necessarily revert to fundamentals—campaign dynamics dominate.
    \item \textbf{Gamma assumption incorrect}: Polling data marginals are better approximated by Normal (after logit transform) than Gamma.
    \item \textbf{Over-parameterization}: Two-component mixture with 4 free parameters overfits limited state-level data.
    \item \textbf{Positivity constraint}: Working with proportions (0-1) loses information about sample sizes and margins.
\end{enumerate}

The CIR model illustrates that \textit{theoretical sophistication does not guarantee empirical performance}. The simpler Brownian motion model better captures polling dynamics.

\section{Limitations and Extensions}

\subsection{Remaining Limitations}

\begin{enumerate}
    \item \textbf{No correlation across states}: Each state is modeled independently. In reality, shocks affect multiple states.
    \item \textbf{Constant parameters}: $\mu, \sigma^2$ are assumed constant over time. Campaign dynamics may change these.
    \item \textbf{Gaussian noise}: Real polling errors may be heavy-tailed or asymmetric.
    \item \textbf{Single-cycle data}: Cannot learn systematic polling biases without multi-cycle training data.
\end{enumerate}

\subsection{Possible Extensions}

\begin{enumerate}
    \item \textbf{Multivariate model}: Use correlated Brownian motions across states:
    \[
    dX_t^{(s)} = \mu^{(s)} dt + \sum_k \sigma_k^{(s)} dW_t^{(k)}
    \]
    where $W_t^{(k)}$ are shared factors (e.g., national, regional).

    \item \textbf{Time-varying drift}: Allow $\mu_t$ to change (e.g., around debates, October surprise).

    \item \textbf{Hierarchical pollster model}: Pool information across pollsters:
    \[
    b_p \sim \mathcal{N}(0, \tau_b^2)
    \]

    \item \textbf{Structural bias correction}: Estimate systematic bias $\delta$ from historical cycles:
    \[
    y_i = X_{t_i} + b_{p(i)} + \delta + \eta_i
    \]
\end{enumerate}

\section{Conclusion}

We have presented a rigorous state-space diffusion model for election forecasting, with complete mathematical derivations of:
\begin{itemize}
    \item The continuous-time stochastic process for vote evolution
    \item The observation model with pollster biases and sampling error
    \item The EM algorithm for parameter estimation via Kalman filtering
    \item The forward simulation procedure for probabilistic forecasting
    \item Evaluation metrics for model performance
\end{itemize}

The model provides a principled Bayesian framework for combining noisy polls into probabilistic forecasts, though practical performance depends critically on the quality and representativeness of polling data.

\end{document}
