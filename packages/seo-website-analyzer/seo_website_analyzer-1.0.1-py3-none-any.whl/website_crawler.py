#!/usr/bin/env python3
"""
ç½‘ç«™çˆ¬è™«æ¨¡å— - è´Ÿè´£çˆ¬å–ç½‘ç«™é¡µé¢
"""

import requests
from bs4 import BeautifulSoup
import time
import re
from urllib.parse import urljoin, urlparse, urlunparse
from typing import List, Set, Dict, Optional
import logging
from collections import deque
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

class WebsiteCrawler:
    """ç½‘ç«™çˆ¬è™«ç±»"""
    
    def __init__(self, config: Dict):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': config['crawler']['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache'
        })
        
        # çˆ¬å–é…ç½®
        self.max_pages = config['crawler']['max_pages']
        self.delay = config['crawler']['delay']
        self.timeout = config['crawler']['timeout']
        self.max_depth = config['crawler']['max_depth']
        
        # æ’é™¤è§„åˆ™
        self.exclude_extensions = ['.pdf', '.jpg', '.png', '.gif', '.css', '.js', '.xml', '.txt', '.zip', '.doc', '.docx']
        self.exclude_paths = ['/admin', '/login', '/register', '/api', '/static', '/assets', '/wp-admin', '/wp-content']
        
        # URLå»é‡è§„åˆ™ - ç»Ÿä¸€ç§»é™¤æ‰€æœ‰æŸ¥è¯¢å‚æ•°
        
        # çŠ¶æ€è·Ÿè¸ª
        self.visited_urls: Set[str] = set()
        self.normalized_urls: Set[str] = set()  # ç”¨äºæ ‡å‡†åŒ–URLå»é‡
        self.url_queue: deque = deque()
        self.lock = threading.Lock()
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
    
    def crawl_website_generator(self, base_url: str):
        """é€ä¸ªçˆ¬å–ç½‘ç«™é¡µé¢ - ç”Ÿæˆå™¨æ¨¡å¼"""
        print(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–ç½‘ç«™: {base_url}")
        
        # æ¸…ç†URL
        base_url = self.clean_url(base_url)
        if not base_url:
            print("âŒ æ— æ•ˆçš„URL")
            return
        
        # åˆå§‹åŒ–
        self.visited_urls.clear()
        self.normalized_urls.clear()
        self.url_queue.clear()
        self.url_queue.append((base_url, 0))  # (url, depth)
        
        while self.url_queue and len(self.visited_urls) < self.max_pages:
            current_url, depth = self.url_queue.popleft()
            
            # æ£€æŸ¥æ·±åº¦é™åˆ¶
            if depth > self.max_depth:
                continue
            
            # æ ‡å‡†åŒ–URLç”¨äºå»é‡
            normalized_url = self.normalize_url(current_url)
            
            # æ£€æŸ¥æ˜¯å¦å·²è®¿é—®ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–URLï¼‰
            if normalized_url in self.normalized_urls:
                continue
            
            # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
            if not self.is_valid_url(current_url, base_url):
                continue
            
            try:
                # è·å–é¡µé¢å†…å®¹
                response = self.get_page_content(current_url)
                if not response:
                    continue
                
                # æ ‡è®°ä¸ºå·²è®¿é—®
                self.visited_urls.add(current_url)
                self.normalized_urls.add(normalized_url)
                
                print(f"âœ… å·²çˆ¬å–: {current_url} (æ·±åº¦: {depth})")
                
                # æå–æ–°é“¾æ¥
                soup = BeautifulSoup(response.text, 'html.parser')
                new_urls = self.extract_links(soup, current_url, base_url)
                
                # æ·»åŠ æ–°é“¾æ¥åˆ°é˜Ÿåˆ—ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–URLæ£€æŸ¥ï¼‰
                for new_url in new_urls:
                    normalized_new_url = self.normalize_url(new_url)
                    if normalized_new_url not in self.normalized_urls:
                        self.url_queue.append((new_url, depth + 1))
                
                # å»¶è¿Ÿ
                if self.delay > 0:
                    time.sleep(self.delay)
                
                # è¿”å›å½“å‰URLä¾›å¤„ç†
                yield current_url
                
            except Exception as e:
                self.logger.error(f"çˆ¬å–é¡µé¢å¤±è´¥ {current_url}: {e}")
                continue
        
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±å‘ç° {len(self.visited_urls)} ä¸ªé¡µé¢")
    
    def crawl_website(self, base_url: str) -> List[str]:
        """çˆ¬å–ç½‘ç«™æ‰€æœ‰é¡µé¢"""
        print(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–ç½‘ç«™: {base_url}")
        
        # æ¸…ç†URL
        base_url = self.clean_url(base_url)
        if not base_url:
            print("âŒ æ— æ•ˆçš„URL")
            return []
        
        # åˆå§‹åŒ–
        self.visited_urls.clear()
        self.url_queue.clear()
        self.url_queue.append((base_url, 0))  # (url, depth)
        
        all_urls = []
        
        while self.url_queue and len(self.visited_urls) < self.max_pages:
            current_url, depth = self.url_queue.popleft()
            
            # æ£€æŸ¥æ·±åº¦é™åˆ¶
            if depth > self.max_depth:
                continue
            
            # æ ‡å‡†åŒ–URLç”¨äºå»é‡
            normalized_url = self.normalize_url(current_url)
            
            # æ£€æŸ¥æ˜¯å¦å·²è®¿é—®ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–URLï¼‰
            if normalized_url in self.normalized_urls:
                continue
            
            # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
            if not self.is_valid_url(current_url, base_url):
                continue
            
            try:
                # è·å–é¡µé¢å†…å®¹
                response = self.get_page_content(current_url)
                if not response:
                    continue
                
                # æ ‡è®°ä¸ºå·²è®¿é—®
                self.visited_urls.add(current_url)
                self.normalized_urls.add(normalized_url)
                all_urls.append(current_url)
                
                print(f"âœ… å·²çˆ¬å–: {current_url} (æ·±åº¦: {depth})")
                
                # è§£æé¡µé¢ï¼Œæå–é“¾æ¥
                soup = BeautifulSoup(response.text, 'html.parser')
                new_urls = self.extract_links(soup, current_url, base_url)
                
                # æ·»åŠ æ–°é“¾æ¥åˆ°é˜Ÿåˆ—ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–URLæ£€æŸ¥ï¼‰
                for new_url in new_urls:
                    normalized_new_url = self.normalize_url(new_url)
                    if normalized_new_url not in self.normalized_urls:
                        self.url_queue.append((new_url, depth + 1))
                
                # å»¶è¿Ÿ
                time.sleep(self.delay)
                
            except Exception as e:
                self.logger.error(f"çˆ¬å–é¡µé¢å¤±è´¥ {current_url}: {e}")
                continue
        
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±å‘ç° {len(all_urls)} ä¸ªé¡µé¢")
        return all_urls
    
    def get_page_content(self, url: str) -> Optional[requests.Response]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = self.session.get(
                url, 
                timeout=self.timeout,
                allow_redirects=True
            )
            # ä¸è°ƒç”¨raise_for_status()ï¼Œè®©è°ƒç”¨è€…å¤„ç†çŠ¶æ€ç 
            return response
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    def clean_url(self, url: str) -> str:
        """æ¸…ç†URL"""
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # ç§»é™¤fragment
        parsed = urlparse(url)
        cleaned = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, parsed.query, ''))
        return cleaned
    
    def normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URLï¼Œç”¨äºå»é‡ - ç»Ÿä¸€ç§»é™¤æ‰€æœ‰æŸ¥è¯¢å‚æ•°"""
        try:
            parsed = urlparse(url)
            
            # ç»Ÿä¸€ç§»é™¤æ‰€æœ‰æŸ¥è¯¢å‚æ•°å’Œfragment
            normalized = urlunparse((
                parsed.scheme,
                parsed.netloc,
                parsed.path,
                parsed.params,
                '',  # ç§»é™¤æ‰€æœ‰æŸ¥è¯¢å‚æ•°
                ''   # ç§»é™¤fragment
            ))
            
            return normalized
        except Exception:
            return url
    
    def is_valid_url(self, url: str, base_url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            base_parsed = urlparse(base_url)
            
            # æ£€æŸ¥åŸŸå
            if parsed.netloc != base_parsed.netloc:
                return False
            
            # æ£€æŸ¥åè®®
            if parsed.scheme not in ['http', 'https']:
                return False
            
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            path = parsed.path.lower()
            for ext in self.exclude_extensions:
                if path.endswith(ext):
                    return False
            
            # æ£€æŸ¥è·¯å¾„
            for exclude_path in self.exclude_paths:
                if exclude_path in path:
                    return False
            
            # æ£€æŸ¥æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
            if '#' in url or '?' in url:
                # å¯¹äºSEOæ£€æŸ¥ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦åŒ…å«è¿™äº›URL
                pass
            
            return True
            
        except Exception:
            return False
    
    def extract_links(self, soup: BeautifulSoup, current_url: str, base_url: str) -> List[str]:
        """ä»é¡µé¢ä¸­æå–é“¾æ¥"""
        links = []
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(current_url, href)
            cleaned_url = self.clean_url(full_url)
            
            if cleaned_url and self.is_valid_url(cleaned_url, base_url):
                links.append(cleaned_url)
        
        return links
    
    def crawl_with_threading(self, base_url: str, max_workers: int = 5) -> List[str]:
        """ä½¿ç”¨å¤šçº¿ç¨‹çˆ¬å–ç½‘ç«™"""
        print(f"ğŸ•·ï¸ å¼€å§‹å¤šçº¿ç¨‹çˆ¬å–ç½‘ç«™: {base_url}")
        
        base_url = self.clean_url(base_url)
        if not base_url:
            return []
        
        self.visited_urls.clear()
        self.normalized_urls.clear()
        self.url_queue.clear()
        self.url_queue.append((base_url, 0))
        
        all_urls = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            while self.url_queue and len(self.visited_urls) < self.max_pages:
                # è·å–ä¸€æ‰¹URLè¿›è¡Œå¤„ç†
                batch_urls = []
                while self.url_queue and len(batch_urls) < max_workers * 2:
                    if self.url_queue:
                        batch_urls.append(self.url_queue.popleft())
                
                if not batch_urls:
                    break
                
                # æäº¤ä»»åŠ¡
                future_to_url = {
                    executor.submit(self.process_url, url, depth, base_url): url 
                    for url, depth in batch_urls
                }
                
                # å¤„ç†ç»“æœ
                for future in as_completed(future_to_url):
                    url, depth = future_to_url[future]
                    try:
                        result = future.result()
                        if result:
                            new_urls, page_url = result
                            normalized_url = self.normalize_url(page_url)
                            
                            with self.lock:
                                if normalized_url not in self.normalized_urls:
                                    self.visited_urls.add(page_url)
                                    self.normalized_urls.add(normalized_url)
                                    all_urls.append(page_url)
                                    
                                    # æ·»åŠ æ–°é“¾æ¥åˆ°é˜Ÿåˆ—ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–URLæ£€æŸ¥ï¼‰
                                    for new_url in new_urls:
                                        normalized_new_url = self.normalize_url(new_url)
                                        if normalized_new_url not in self.normalized_urls:
                                            self.url_queue.append((new_url, depth + 1))
                                
                                print(f"âœ… å·²çˆ¬å–: {page_url}")
                    except Exception as e:
                        self.logger.error(f"å¤„ç†URLå¤±è´¥ {url}: {e}")
        
        print(f"ğŸ‰ å¤šçº¿ç¨‹çˆ¬å–å®Œæˆï¼å…±å‘ç° {len(all_urls)} ä¸ªé¡µé¢")
        return all_urls
    
    def process_url(self, url: str, depth: int, base_url: str) -> Optional[tuple]:
        """å¤„ç†å•ä¸ªURL"""
        try:
            # æ£€æŸ¥æ·±åº¦é™åˆ¶
            if depth > self.max_depth:
                return None
            
            # æ£€æŸ¥æ˜¯å¦å·²è®¿é—®ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–URLï¼‰
            normalized_url = self.normalize_url(url)
            if normalized_url in self.normalized_urls:
                return None
            
            # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
            if not self.is_valid_url(url, base_url):
                return None
            
            # è·å–é¡µé¢å†…å®¹
            response = self.get_page_content(url)
            if not response:
                return None
            
            # è§£æé¡µé¢ï¼Œæå–é“¾æ¥
            soup = BeautifulSoup(response.text, 'html.parser')
            new_urls = self.extract_links(soup, url, base_url)
            
            # å»¶è¿Ÿ
            time.sleep(self.delay)
            
            return new_urls, url
            
        except Exception as e:
            self.logger.error(f"å¤„ç†URLå¤±è´¥ {url}: {e}")
            return None
    
    def get_page_info(self, url: str) -> Optional[Dict]:
        """è·å–é¡µé¢åŸºæœ¬ä¿¡æ¯"""
        try:
            response = self.get_page_content(url)
            if not response:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–åŸºæœ¬ä¿¡æ¯
            title = soup.find('title')
            title_text = title.get_text().strip() if title else ""
            
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', '').strip() if meta_desc else ""
            
            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
            keywords = meta_keywords.get('content', '').strip() if meta_keywords else ""
            
            h1_tags = [h1.get_text().strip() for h1 in soup.find_all('h1')]
            h2_tags = [h2.get_text().strip() for h2 in soup.find_all('h2')]
            
            # è®¡ç®—å†…å®¹é•¿åº¦
            content_length = len(soup.get_text())
            word_count = len(soup.get_text().split())
            
            return {
                'url': url,
                'title': title_text,
                'description': description,
                'keywords': keywords,
                'h1_tags': h1_tags,
                'h2_tags': h2_tags,
                'content_length': content_length,
                'word_count': word_count,
                'load_time': response.elapsed.total_seconds(),
                'status_code': response.status_code,
                'content_type': response.headers.get('content-type', ''),
                'last_modified': response.headers.get('last-modified', ''),
                'content_encoding': response.headers.get('content-encoding', '')
            }
            
        except Exception as e:
            self.logger.error(f"è·å–é¡µé¢ä¿¡æ¯å¤±è´¥ {url}: {e}")
            return None
