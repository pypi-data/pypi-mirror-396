#!/usr/bin/env python3
"""
ç½‘ç«™SEOä¼˜åŒ–æ£€æŸ¥å™¨ - ä¸»ç¨‹åº
æ ¹æ®seo.mdæ–‡æ¡£è¿›è¡Œå…¨é¢SEOæ£€æŸ¥
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import json
import argparse
import sys
import os
from urllib.parse import urljoin, urlparse, urlunparse
from typing import List, Dict, Set, Tuple
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
from datetime import datetime
import logging
from dataclasses import dataclass, asdict
from collections import defaultdict
import hashlib

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from website_crawler import WebsiteCrawler
from seo_analyzer import SEOAnalyzer
from report_generator import ReportGenerator
from seo_api_client import SEOAPIClient, create_seo_issue_data

@dataclass
class SEOIssue:
    """SEOé—®é¢˜æ•°æ®ç±»"""
    page_url: str
    issue_type: str
    severity: str  # critical, warning, info
    message: str
    suggestion: str
    element: str = ""
    line_number: int = 0

@dataclass
class PageSEOData:
    """é¡µé¢SEOæ•°æ®ç±»"""
    url: str
    title: str
    meta_description: str
    meta_keywords: str
    h1_tags: List[str]
    h2_tags: List[str]
    h3_tags: List[str]
    images: List[Dict]
    internal_links: List[str]
    external_links: List[str]
    content_length: int
    word_count: int
    load_time: float
    issues: List[SEOIssue]
    html: str = ""  # æ·»åŠ HTMLå†…å®¹å±æ€§
    score: float = 0.0
    response_time: int = None
    page_size: int = None
    status_code: int = None

class SEOChecker:
    """SEOæ£€æŸ¥å™¨ä¸»ç±»"""
    
    def __init__(self, config_file: str = "seo_config.json"):
        """åˆå§‹åŒ–SEOæ£€æŸ¥å™¨"""
        self.config = self.load_config(config_file)
        self.crawler = WebsiteCrawler(self.config)
        self.analyzer = SEOAnalyzer(self.config)
        self.report_generator = ReportGenerator(self.config)
        
        # æ•°æ®å­˜å‚¨
        self.visited_urls: Set[str] = set()
        self.pages_data: List[PageSEOData] = []
        self.issues_summary: Dict[str, int] = defaultdict(int)
        self.overall_score: float = 0.0
        self.api_client = None
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯ï¼ˆå¦‚æœå¯ç”¨æ•°æ®åº“å­˜å‚¨ï¼‰
        if self.config.get('database', {}).get('enabled', False):
            api_url = self.config['database'].get('api_url', 'http://localhost:3000')
            api_key = self.config['database'].get('api_key', '')
            if api_key:
                self.api_client = SEOAPIClient(api_url, api_key)
                print("âœ… æ•°æ®åº“å­˜å‚¨å·²å¯ç”¨")
            else:
                print("âš ï¸ æ•°æ®åº“å­˜å‚¨å·²å¯ç”¨ä½†æœªé…ç½®APIå¯†é’¥")
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
    def load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.get_default_config()
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "crawler": {
                "max_pages": 100,
                "delay": 1.0,
                "timeout": 30,
                "max_depth": 5,
                "user_agent": "SEO-Checker/1.0"
            },
            "seo_rules": {
                "title_min_length": 30,
                "title_max_length": 60,
                "description_min_length": 120,
                "description_max_length": 160,
                "h1_required": True,
                "max_h1_count": 1,
                "min_content_length": 300,
                "keyword_density_min": 0.5,
                "keyword_density_max": 3.0
            },
            "output": {
                "generate_html": True,
                "generate_excel": False,
                "generate_json": False,
                "include_screenshots": False
            },
            "database": {
                "enabled": False,
                "api_url": "http://localhost:3000",
                "api_key": ""
            }
        }
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'{log_dir}/seo_checker_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def check_website(self, base_url: str) -> Dict:
        """æ£€æŸ¥å•ä¸ªç½‘ç«™ - ä¸€æ¡ä¸€æ¡çˆ¬å–ï¼Œä¸€æ¡ä¸€æ¡åˆ†æï¼Œä¸€æ¡ä¸€æ¡å…¥åº“"""
        print(f"ğŸ” å¼€å§‹æ£€æŸ¥ç½‘ç«™: {base_url}")
        self.logger.info(f"å¼€å§‹æ£€æŸ¥ç½‘ç«™: {base_url}")
        
        # åˆå§‹åŒ–æ•°æ®å­˜å‚¨
        self.pages_data = []
        page_count = 0
        
        # 1. é€ä¸ªçˆ¬å–ã€åˆ†æã€ä¿å­˜ï¼ˆä¸€æ¡é¾™å¤„ç†ï¼‰
        print("ğŸ”„ å¼€å§‹ä¸€æ¡é¾™å¤„ç†ï¼šçˆ¬å– â†’ åˆ†æ â†’ å…¥åº“")
        
        for page_url in self.crawler.crawl_website_generator(base_url):
            page_count += 1
            try:
                print(f"ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page_count} ä¸ªé¡µé¢: {page_url}")
                
                # åˆ†æå½“å‰é¡µé¢
                page_data = self.analyze_page(page_url, None)  # ä¸ä¼ é€’æ€»é¡µé¢æ•°ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯é€ä¸ªå¤„ç†
                
                if page_data:
                    self.pages_data.append(page_data)
                    print(f"âœ… åˆ†æå®Œæˆ: {page_url}")
                    
                    # ç«‹å³ä¿å­˜å½“å‰é¡µé¢çš„é—®é¢˜åˆ°æ•°æ®åº“
                    if self.api_client and page_data.issues:
                        self.save_single_page_issues(base_url, page_data)
                    elif self.api_client:
                        print(f"â„¹ï¸ é¡µé¢ {page_url} æ²¡æœ‰å‘ç°SEOé—®é¢˜")
                else:
                    print(f"âŒ åˆ†æå¤±è´¥: {page_url}")
                    
            except Exception as e:
                self.logger.error(f"å¤„ç†é¡µé¢å¤±è´¥ {page_url}: {e}")
                print(f"âŒ å¤„ç†å¤±è´¥: {page_url}")
        
        # 2. è®¡ç®—æ€»ä½“è¯„åˆ†
        self.calculate_overall_score()
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        print("ğŸ“Š æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
        report_data = self.generate_report_data()
        
        # 4. ä¿å­˜æŠ¥å‘Š
        self.save_reports(report_data, base_url)
        
        print(f"ğŸ‰ SEOæ£€æŸ¥å®Œæˆï¼å…±å¤„ç† {page_count} ä¸ªé¡µé¢ï¼Œæ€»ä½“è¯„åˆ†: {self.overall_score:.1f}/100")
        return report_data
    
    def save_single_page_issues(self, base_url: str, page_data):
        """ä¿å­˜å•ä¸ªé¡µé¢çš„SEOé—®é¢˜åˆ°æ•°æ®åº“"""
        if not self.api_client:
            return
        
        try:
            from urllib.parse import urlparse
            domain = urlparse(base_url).netloc
            check_batch_id = f"batch_{int(datetime.now().timestamp())}"
            
            issues = []
            for issue in page_data.issues:
                # ç”Ÿæˆé—®é¢˜æ ‡è¯†ç¬¦
                issue_identifier = self.generate_issue_identifier(issue)
                
                # åˆ›å»ºé—®é¢˜æ•°æ®
                issue_data = create_seo_issue_data(
                    domain=domain,
                    page_url=page_data.url,
                    page_title=page_data.title,
                    issue_type=issue.issue_type,
                    issue_identifier=issue_identifier,
                    issue_name=issue.message,
                    issue_severity=issue.severity,
                    issue_description=issue.message,
                    issue_suggestion=issue.suggestion,
                    issue_value=issue.element,
                    check_batch_id=check_batch_id,
                    response_time=page_data.response_time,
                    page_size=page_data.page_size,
                    status_code=page_data.status_code
                )
                issues.append(issue_data)
            
            if issues:
                result = self.api_client.submit_seo_issues(issues)
                if result.get('success'):
                    print(f"ğŸ’¾ å·²ä¿å­˜ {len(issues)} ä¸ªé—®é¢˜åˆ°æ•°æ®åº“")
                else:
                    print(f"âŒ ä¿å­˜é—®é¢˜åˆ°æ•°æ®åº“å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜é—®é¢˜åˆ°æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            self.logger.error(f"ä¿å­˜é—®é¢˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
    
    def save_issues_to_database(self, base_url: str):
        """ä¿å­˜SEOé—®é¢˜åˆ°æ•°æ®åº“"""
        if not self.api_client:
            return
        
        try:
            from urllib.parse import urlparse
            domain = urlparse(base_url).netloc
            check_batch_id = f"batch_{int(datetime.now().timestamp())}"
            
            issues = []
            for page_data in self.pages_data:
                for issue in page_data.issues:
                    # ç”Ÿæˆé—®é¢˜æ ‡è¯†ç¬¦
                    issue_identifier = self.generate_issue_identifier(issue)
                    
                    # åˆ›å»ºé—®é¢˜æ•°æ®
                    issue_data = create_seo_issue_data(
                        domain=domain,
                        page_url=page_data.url,
                        page_title=page_data.title,
                        issue_type=issue.issue_type,
                        issue_identifier=issue_identifier,
                        issue_name=issue.message,
                        issue_severity=issue.severity,
                        issue_description=issue.message,
                        issue_suggestion=issue.suggestion,
                        issue_value=issue.element,
                        check_batch_id=check_batch_id,
                        response_time=page_data.response_time,
                        page_size=page_data.page_size,
                        status_code=page_data.status_code
                    )
                    issues.append(issue_data)
            
            if issues:
                result = self.api_client.submit_seo_issues(issues)
                if result.get('success'):
                    print(f"âœ… æˆåŠŸä¿å­˜ {len(issues)} ä¸ªé—®é¢˜åˆ°æ•°æ®åº“")
                else:
                    print(f"âŒ ä¿å­˜é—®é¢˜åˆ°æ•°æ®åº“å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            else:
                print("â„¹ï¸ æ²¡æœ‰å‘ç°SEOé—®é¢˜")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜é—®é¢˜åˆ°æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            self.logger.error(f"ä¿å­˜é—®é¢˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
    
    def generate_issue_identifier(self, issue) -> str:
        """ç”Ÿæˆé—®é¢˜æ ‡è¯†ç¬¦"""
        # åŸºäºé—®é¢˜ç±»å‹å’Œå†…å®¹ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
        identifier_parts = [
            issue.issue_type,
            issue.severity,
            issue.message[:50] if issue.message else "",
            issue.element[:20] if issue.element else ""
        ]
        return "_".join(identifier_parts).replace(" ", "_").replace(":", "").replace(";", "")
    
    def _create_404_page_data(self, page_url: str) -> PageSEOData:
        """åˆ›å»º404é¡µé¢çš„SEOæ•°æ®"""
        # åˆ›å»º404é”™è¯¯çš„é—®é¢˜
        issue = SEOIssue(
            page_url=page_url,
            issue_type="page_not_found",
            severity="critical",
            message="é¡µé¢è¿”å›404é”™è¯¯",
            suggestion="æ£€æŸ¥é¡µé¢URLæ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€ƒè™‘è®¾ç½®301é‡å®šå‘",
            element="http_status"
        )
        
        # åˆ›å»ºé¡µé¢æ•°æ®å¯¹è±¡
        page_data = PageSEOData(
            url=page_url,
            title="404 Not Found",
            meta_description="",
            meta_keywords="",
            h1_tags=[],
            h2_tags=[],
            h3_tags=[],
            images=[],
            internal_links=[],
            external_links=[],
            content_length=0,
            word_count=0,
            load_time=0.0,
            issues=[issue],
            html="",
            status_code=404
        )
        
        return page_data
    
    def analyze_page(self, page_url: str, total_pages: int = None) -> PageSEOData:
        """åˆ†æå•ä¸ªé¡µé¢çš„SEO"""
        try:
            # è·å–é¡µé¢å†…å®¹
            response = self.crawler.get_page_content(page_url)
            if not response:
                # åˆ›å»º404é”™è¯¯çš„SEOé—®é¢˜
                return self._create_404_page_data(page_url)
            
            # æ£€æŸ¥HTTPçŠ¶æ€ç 
            if response.status_code == 404:
                # åˆ›å»º404é”™è¯¯çš„SEOé—®é¢˜
                return self._create_404_page_data(page_url)
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–åŸºç¡€ä¿¡æ¯
            title = self.extract_title(soup)
            meta_description = self.extract_meta_description(soup)
            meta_keywords = self.extract_meta_keywords(soup)
            h1_tags = self.extract_h_tags(soup, 'h1')
            h2_tags = self.extract_h_tags(soup, 'h2')
            h3_tags = self.extract_h_tags(soup, 'h3')
            images = self.extract_images(soup)
            internal_links, external_links = self.extract_links(soup, page_url)
            content_length = len(soup.get_text())
            word_count = len(soup.get_text().split())
            
            # è®¡ç®—é¡µé¢åŠ è½½æ—¶é—´
            load_time = response.elapsed.total_seconds()
            
            # åˆ›å»ºé¡µé¢æ•°æ®å¯¹è±¡
            page_data = PageSEOData(
                url=page_url,
                title=title,
                meta_description=meta_description,
                meta_keywords=meta_keywords,
                h1_tags=h1_tags,
                h2_tags=h2_tags,
                h3_tags=h3_tags,
                images=images,
                internal_links=internal_links,
                external_links=external_links,
                content_length=content_length,
                word_count=word_count,
                load_time=load_time,
                issues=[],
                html=response.text  # æ·»åŠ HTMLå†…å®¹
            )
            
            # è¿›è¡ŒSEOæ£€æŸ¥
            issues = self.analyzer.check_page_seo(page_data, soup, total_pages)
            page_data.issues = issues
            
            # è®¡ç®—é¡µé¢è¯„åˆ†
            page_data.score = self.calculate_page_score(page_data, issues)
            
            return page_data
            
        except Exception as e:
            self.logger.error(f"åˆ†æé¡µé¢å¤±è´¥ {page_url}: {e}")
            return None
    
    def extract_title(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        title_tag = soup.find('title')
        return title_tag.get_text().strip() if title_tag else ""
    
    def extract_meta_description(self, soup: BeautifulSoup) -> str:
        """æå–Metaæè¿°"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        return meta_desc.get('content', '').strip() if meta_desc else ""
    
    def extract_meta_keywords(self, soup: BeautifulSoup) -> str:
        """æå–Metaå…³é”®è¯"""
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        return meta_keywords.get('content', '').strip() if meta_keywords else ""
    
    def extract_h_tags(self, soup: BeautifulSoup, tag_name: str) -> List[str]:
        """æå–Hæ ‡ç­¾"""
        tags = soup.find_all(tag_name)
        return [tag.get_text().strip() for tag in tags]
    
    def extract_images(self, soup: BeautifulSoup) -> List[Dict]:
        """æå–å›¾ç‰‡ä¿¡æ¯"""
        images = []
        for img in soup.find_all('img'):
            images.append({
                'src': img.get('src', ''),
                'alt': img.get('alt', ''),
                'title': img.get('title', ''),
                'width': img.get('width', ''),
                'height': img.get('height', '')
            })
        return images
    
    def extract_links(self, soup: BeautifulSoup, base_url: str) -> Tuple[List[str], List[str]]:
        """æå–å†…éƒ¨å’Œå¤–éƒ¨é“¾æ¥"""
        internal_links = []
        external_links = []
        base_domain = urlparse(base_url).netloc
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(base_url, href)
            link_domain = urlparse(full_url).netloc
            
            if link_domain == base_domain:
                internal_links.append(full_url)
            else:
                external_links.append(full_url)
        
        return internal_links, external_links
    
    def calculate_page_score(self, page_data: PageSEOData, issues: List[SEOIssue]) -> float:
        """è®¡ç®—é¡µé¢SEOè¯„åˆ†"""
        base_score = 100.0
        
        # æ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        for issue in issues:
            if issue.severity == 'critical':
                base_score -= 10
            elif issue.severity == 'warning':
                base_score -= 5
            elif issue.severity == 'info':
                base_score -= 2
        
        # ç¡®ä¿è¯„åˆ†ä¸ä½äº0
        return max(0.0, base_score)
    
    def calculate_overall_score(self):
        """è®¡ç®—æ€»ä½“è¯„åˆ†"""
        if not self.pages_data:
            self.overall_score = 0.0
            return
        
        total_score = sum(page.score for page in self.pages_data)
        self.overall_score = total_score / len(self.pages_data)
        
        # ç»Ÿè®¡é—®é¢˜ç±»å‹
        for page in self.pages_data:
            for issue in page.issues:
                self.issues_summary[issue.issue_type] += 1
    
    def generate_report_data(self) -> Dict:
        """ç”ŸæˆæŠ¥å‘Šæ•°æ®"""
        return {
            'overall_score': self.overall_score,
            'total_pages': len(self.pages_data),
            'issues_summary': dict(self.issues_summary),
            'pages_data': [asdict(page) for page in self.pages_data],
            'check_time': datetime.now().isoformat(),
            'config': self.config
        }
    
    def save_reports(self, report_data: Dict, base_url: str):
        """ä¿å­˜æŠ¥å‘Š"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "reports"
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        domain = urlparse(base_url).netloc
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"{domain}_{timestamp}"
        
        # ä¿å­˜JSONæŠ¥å‘Š
        if self.config['output']['generate_json']:
            json_file = f"{output_dir}/{base_filename}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ JSONæŠ¥å‘Šå·²ä¿å­˜: {json_file}")
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        if self.config['output']['generate_html']:
            html_file = f"{output_dir}/{base_filename}.html"
            self.report_generator.generate_html_report(report_data, html_file)
            print(f"ğŸŒ HTMLæŠ¥å‘Šå·²ä¿å­˜: {html_file}")
        
        # ç”ŸæˆExcelæŠ¥å‘Š
        if self.config['output']['generate_excel']:
            excel_file = f"{output_dir}/{base_filename}.xlsx"
            self.report_generator.generate_excel_report(report_data, excel_file)
            print(f"ğŸ“Š ExcelæŠ¥å‘Šå·²ä¿å­˜: {excel_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç½‘ç«™SEOä¼˜åŒ–æ£€æŸ¥å™¨')
    parser.add_argument('urls', nargs='+', help='è¦æ£€æŸ¥çš„ç½‘ç«™URL')
    parser.add_argument('--config', default='seo_config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š')
    parser.add_argument('--excel', action='store_true', help='ç”ŸæˆExcelæŠ¥å‘Š')
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡æ£€æŸ¥æ¨¡å¼')
    parser.add_argument('--input', help='æ‰¹é‡æ£€æŸ¥çš„è¾“å…¥æ–‡ä»¶')
    parser.add_argument('--rules', help='è‡ªå®šä¹‰æ£€æŸ¥è§„åˆ™æ–‡ä»¶')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--max-pages', type=int, help='æœ€å¤§é‡‡é›†é¡µé¢æ•° (è¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºSEOæ£€æŸ¥å™¨
    checker = SEOChecker(args.config)
    
    # å¦‚æœæŒ‡å®šäº†max-pageså‚æ•°ï¼Œè¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®
    if args.max_pages:
        checker.config['crawler']['max_pages'] = args.max_pages
        print(f"ğŸ“Š è®¾ç½®æœ€å¤§é‡‡é›†é¡µé¢æ•°: {args.max_pages}")
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        if args.batch and args.input:
            # æ‰¹é‡æ£€æŸ¥æ¨¡å¼
            with open(args.input, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]
            
            for url in urls:
                print(f"\n{'='*60}")
                print(f"æ£€æŸ¥ç½‘ç«™: {url}")
                print(f"{'='*60}")
                checker.check_website(url)
        else:
            # å•ä¸ªç½‘ç«™æ£€æŸ¥
            for url in args.urls:
                print(f"\n{'='*60}")
                print(f"æ£€æŸ¥ç½‘ç«™: {url}")
                print(f"{'='*60}")
                checker.check_website(url)
                
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ æ£€æŸ¥è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()
