From 26fdd2026cc3d1ed7da894907ae244a155a16566 Mon Sep 17 00:00:00 2001
From: harrisonyhq <harrisonyhq@gmail.com>
Date: Tue, 4 Nov 2025 19:36:36 -0800
Subject: [PATCH 1/3] [Patch0] UCM PC adapt patch

---
 .../kv_transfer/kv_connector/v1/multi_connector.py    |  7 ++++++-
 vllm/v1/core/sched/scheduler.py                       | 11 +++++++++++
 vllm/v1/outputs.py                                    |  1 +
 vllm/v1/request.py                                    |  2 ++
 vllm/v1/worker/gpu_model_runner.py                    |  7 ++++---
 5 files changed, 24 insertions(+), 4 deletions(-)

diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py b/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
index be3c23399..5f92d69bd 100644
--- a/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
@@ -99,8 +99,13 @@ class MultiConnector(KVConnectorBase_V1):
             c.save_kv_layer(layer_name, kv_layer, attn_metadata, **kwargs)
 
     def wait_for_save(self):
+        success_dumped_blocks = None
         for c in self._connectors:
-            c.wait_for_save()
+            uc_dump_blocks = c.wait_for_save()
+            if uc_dump_blocks:
+                success_dumped_blocks = uc_dump_blocks
+
+        return success_dumped_blocks if success_dumped_blocks else None
 
     def get_finished(
         self, finished_req_ids: set[str]
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index fe552db74..cd80f92a1 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -34,6 +34,7 @@ from vllm.v1.outputs import ModelRunnerOutput
 from vllm.v1.request import Request, RequestStatus
 from vllm.v1.spec_decode.metrics import SpecDecodingStats
 from vllm.v1.structured_output import StructuredOutputManager
+from vllm.distributed.kv_transfer.kv_connector.v1.multi_connector import MultiConnector
 
 logger = init_logger(__name__)
 
@@ -791,6 +792,16 @@ class Scheduler(SchedulerInterface):
             new_logprobs = None
             new_token_ids = generated_token_ids
             kv_transfer_params = None
+            if model_runner_output.finished_dumping is not None:
+                request.succeed_dumped_blocks.extend(model_runner_output.finished_dumping.get(req_id, []))
+                is_prefill = request.num_output_tokens == 0
+                if is_prefill:
+                    if isinstance(self.connector, MultiConnector):
+                        for c in self.connector._connectors:
+                            if hasattr(c, 'connector') and hasattr(c.connector, 'commit'):
+                                c.connector.commit(model_runner_output.finished_dumping.get(req_id, []), True)
+                    else:
+                        self.connector.connector.commit(model_runner_output.finished_dumping.get(req_id, []), True)
 
             # Append generated tokens and check for stop. Note that if
             # a request is still being prefilled, we expect the model runner
diff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py
index f78623f57..c8388baed 100644
--- a/vllm/v1/outputs.py
+++ b/vllm/v1/outputs.py
@@ -107,6 +107,7 @@ class ModelRunnerOutput:
     # [req_ids]
     finished_sending: Optional[set[str]] = None
     finished_recving: Optional[set[str]] = None
+    finished_dumping: Optional[dict[str, list[str]]] = None
 
     # req_id -> num_nans_in_logits
     num_nans_in_logits: Optional[dict[str, int]] = None
diff --git a/vllm/v1/request.py b/vllm/v1/request.py
index 9b96f4599..e70d1695b 100644
--- a/vllm/v1/request.py
+++ b/vllm/v1/request.py
@@ -103,6 +103,8 @@ class Request:
         # The number of tokens with prefix cache hits.
         self.num_cached_tokens = -1
 
+        self.succeed_dumped_blocks: list[str] = []
+
         # The number of NaNs in logits. A value greater than 0
         # indicates that the output is corrupted
         self.num_nans_in_logits = 0
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 5a26e88db..53ee8cfcd 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -1378,7 +1378,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 inputs_embeds=inputs_embeds,
             )
 
-            self.maybe_wait_for_kv_save()
+            finished_dumping = self.maybe_wait_for_kv_save()
             finished_sending, finished_recving = (
                 self.get_finished_kv_transfers(scheduler_output))
 
@@ -1562,6 +1562,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             pooler_output=[],
             finished_sending=finished_sending,
             finished_recving=finished_recving,
+            finished_dumping=finished_dumping,
             num_nans_in_logits=num_nans_in_logits,
         )
 
@@ -1719,9 +1720,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             kv_connector.start_load_kv(get_forward_context())
 
     @staticmethod
-    def maybe_wait_for_kv_save() -> None:
+    def maybe_wait_for_kv_save():
         if has_kv_transfer_group():
-            get_kv_transfer_group().wait_for_save()
+            return get_kv_transfer_group().wait_for_save()
 
     @staticmethod
     def get_finished_kv_transfers(
-- 
2.34.1

