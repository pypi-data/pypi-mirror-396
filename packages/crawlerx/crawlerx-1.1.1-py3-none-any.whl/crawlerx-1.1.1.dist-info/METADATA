Metadata-Version: 2.4
Name: crawlerx
Version: 1.1.1
Summary: CrawlerX - The Ultimate Web Crawler
Home-page: https://github.com/IMApurbo/crawlerx
Author: AKM Korishee Apurbo
Author-email: bandinvisible8@gmail.com
License: MIT
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests==2.32.3
Requires-Dist: beautifulsoup4==4.12.3
Requires-Dist: urllib3==2.2.3
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# CrawlerX â€“ Advanced Web Reconnaissance Crawler

**CrawlerX** is an advanced, multi-threaded **web reconnaissance crawler** built for **security researchers, bug bounty hunters, and penetration testers**.
It focuses on **deep endpoint discovery**, **GET/POST parameter extraction**, **API detection**, **resource categorization**, and **intelligent URL validation**, with optional fuzzing and common path probing.

> âœ¨ Developed by **[@IMApurbo](https://github.com/IMApurbo)**
> ğŸ›¡ï¸ Use only on systems you own or have explicit permission to test.

---

## ğŸš€ Core Capabilities

### ğŸ” Intelligent Crawling

* Discovers **HTML**, **JavaScript**, **CSS**, **JSON**, and **XML** endpoints
* Extracts URLs from:

  * HTML attributes
  * Inline JavaScript
  * Event handlers
  * CSS files
  * JSON / API responses
* Strong filtering to eliminate **JavaScript noise & false positives**

---

### ğŸ”— Endpoint Discovery

* **All endpoints**
* **Parameterized URLs**
* **Non-parameterized URLs**
* Automatic **domain + subdomain validation**

Saved under:

```
endpoints/
â”œâ”€â”€ all_endpoints.txt
â”œâ”€â”€ all_endpoints.json
â”œâ”€â”€ parameterized.txt
â”œâ”€â”€ parameterized.json
â”œâ”€â”€ non_parameterized.txt
```

---

### ğŸ§ª GET & POST Parameter Extraction

* Extracts parameters from **HTML forms**
* Generates **realistic default values**
* Saves:

  * Parsed parameters (JSON)
  * Raw `.req` files (Burp-ready)

```
get/
â”œâ”€â”€ get_urls.txt
â”œâ”€â”€ get_params.json
â”œâ”€â”€ *.req

post/
â”œâ”€â”€ post_urls.txt
â”œâ”€â”€ post_params.json
â”œâ”€â”€ *.req
```

---

### âš™ï¸ API Endpoint Detection

Detects common API patterns:

* `/api/`
* `/v1/`, `/v2/`
* `/rest/`
* `/graphql`
* `.json`, `.xml`

```
api/
â”œâ”€â”€ api_endpoints.txt
â”œâ”€â”€ api_endpoints.json
```

---

### ğŸ“ Resource Categorization

Automatically classifies discovered resources:

* Images
* JavaScript
* Stylesheets
* Fonts
* Media
* Documents
* Other

```
resources/
â”œâ”€â”€ images.txt / images.json
â”œâ”€â”€ scripts.txt / scripts.json
â”œâ”€â”€ stylesheets.txt / stylesheets.json
â”œâ”€â”€ fonts.txt / fonts.json
â”œâ”€â”€ media.txt / media.json
â”œâ”€â”€ documents.txt / documents.json
â”œâ”€â”€ other.txt / other.json
```

---

### ğŸŒ³ Site Structure Mapping

Optional **ASCII tree view** of the entire site:

```
example.com
â”œâ”€â”€ login
â”œâ”€â”€ dashboard
â”‚   â”œâ”€â”€ profile
â”‚   â””â”€â”€ settings
â””â”€â”€ api
    â””â”€â”€ v1
```

Saved to:

```
structure/structure.txt
```

---

### ğŸ§  Smart Features

* **Robots.txt support** (optional)
* **Resume interrupted crawls** using pickle state
* **Incremental auto-save**
* **Graceful Ctrl+C handling**
* **Parameter fuzzing** (numeric params)
* **Common path probing** (admin, api, backup, .env, etc.)
* **Retry logic** with backoff
* **Verbose mode** with interesting parameter highlighting

---

## ğŸ“¦ Installation

```bash
pip install crawlerx
```

---

## ğŸ§‘â€ğŸ’» Usage

```bash
crawlerx -u <url> [options]
```

---

## ğŸ§¾ Command-Line Options

| Flag               | Description                   | Default    |
| ------------------ | ----------------------------- | ---------- |
| `-u, --url`        | Target URL (required)         | â€”          |
| `-o, --output`     | Output directory              | None       |
| `--threads`        | Concurrent threads (1â€“20)     | 5          |
| `--depth`          | Crawl depth                   | 2          |
| `--delay`          | Delay between requests        | 0.1s       |
| `--timeout`        | Request timeout               | 10s        |
| `--ua`             | Custom User-Agent             | Browser UA |
| `-H, --headers`    | Custom headers (`Key:Value;`) | None       |
| `--proxy`          | HTTP/HTTPS proxy              | None       |
| `--exclude`        | Excluded extensions           | None       |
| `--sub`            | Include subdomains            | False      |
| `--structure`      | Generate site structure       | False      |
| `--respect-robots` | Respect robots.txt            | False      |
| `--fuzz-params`    | Fuzz numeric parameters       | False      |
| `--common-paths`   | Probe common paths            | False      |
| `--cont`           | Resume from crawl state       | None       |
| `--verbose`        | Verbose logging               | False      |

---

## ğŸ§ª Examples

### Basic Crawl

```bash
crawlerx -u https://example.com
```

### Save Output

```bash
crawlerx -u https://example.com -o results
```

### Deep Crawl with Threads

```bash
crawlerx -u https://example.com --depth 4 --threads 10
```

### Enable Fuzzing & Common Paths

```bash
crawlerx -u https://example.com --fuzz-params --common-paths
```

### Resume Interrupted Crawl

```bash
crawlerx -u https://example.com --cont results/crawlerx_example.com/crawl_state.pkl
```

### Generate Site Structure

```bash
crawlerx -u https://example.com --structure
```

---

## ğŸ“‚ Output Directory Layout

```
crawlerx_<domain>/
â”œâ”€â”€ endpoints/
â”œâ”€â”€ get/
â”œâ”€â”€ post/
â”œâ”€â”€ api/
â”œâ”€â”€ resources/
â”œâ”€â”€ structure/
â””â”€â”€ crawl_state.pkl
```

---

## âš ï¸ Legal Notice

> ğŸš¨ **Authorized use only**
> This tool is intended for **legal security testing**.
> Unauthorized scanning may violate laws and ethical guidelines.

---

## ğŸ‘¨â€ğŸ’» Author

* **IMApurbo**

---

## ğŸ“œ License

Licensed under the **MIT License**.
See the [LICENSE](LICENSE) file for details.
