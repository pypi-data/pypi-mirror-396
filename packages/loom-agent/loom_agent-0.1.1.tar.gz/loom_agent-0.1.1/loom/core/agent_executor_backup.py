"""
Agent Executor with tt (Tail-Recursive) Control Loop

Core execution engine implementing recursive conversation management,
inspired by Claude Code's tt function design.
"""

from __future__ import annotations

import asyncio
import json
import time
from pathlib import Path
from typing import AsyncGenerator, Dict, List, Optional, Any
from uuid import uuid4

from loom.callbacks.base import BaseCallback
from loom.callbacks.metrics import MetricsCollector
from loom.core.context_assembly import ComponentPriority, ContextAssembler
from loom.core.events import AgentEvent, AgentEventType, ToolResult
from loom.core.execution_context import ExecutionContext
from loom.core.permissions import PermissionManager
from loom.core.recursion_control import RecursionMonitor, RecursionState
from loom.core.steering_control import SteeringControl
from loom.core.tool_orchestrator import ToolOrchestrator
from loom.core.tool_pipeline import ToolExecutionPipeline
from loom.core.turn_state import TurnState
from loom.core.types import Message, ToolCall
from loom.interfaces.compressor import BaseCompressor
from loom.interfaces.llm import BaseLLM
from loom.interfaces.memory import BaseMemory
from loom.interfaces.tool import BaseTool
from loom.utils.token_counter import count_messages_tokens

# RAG support
try:
    from loom.core.context_retriever import ContextRetriever
except ImportError:
    ContextRetriever = None  # type: ignore

# Unified coordination support
try:
    from loom.core.unified_coordination import UnifiedExecutionContext, IntelligentCoordinator
except ImportError:
    UnifiedExecutionContext = None  # type: ignore
    IntelligentCoordinator = None  # type: ignore


class TaskHandler:
    """
    ä»»åŠ¡å¤„ç†å™¨åŸºç±»
    
    å¼€å‘è€…å¯ä»¥ç»§æ‰¿æ­¤ç±»æ¥å®žçŽ°è‡ªå®šä¹‰çš„ä»»åŠ¡å¤„ç†é€»è¾‘
    """
    
    def can_handle(self, task: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†ç»™å®šçš„ä»»åŠ¡
        
        Args:
            task: ä»»åŠ¡æè¿°
            
        Returns:
            bool: æ˜¯å¦èƒ½å¤„ç†æ­¤ä»»åŠ¡
        """
        raise NotImplementedError
    
    def generate_guidance(
        self,
        original_task: str,
        result_analysis: Dict[str, Any],
        recursion_depth: int
    ) -> str:
        """
        ç”Ÿæˆé€’å½’æŒ‡å¯¼æ¶ˆæ¯
        
        Args:
            original_task: åŽŸå§‹ä»»åŠ¡
            result_analysis: å·¥å…·ç»“æžœåˆ†æž
            recursion_depth: é€’å½’æ·±åº¦
            
        Returns:
            str: ç”Ÿæˆçš„æŒ‡å¯¼æ¶ˆæ¯
        """
        raise NotImplementedError


class AgentExecutor:
    """
    Agent Executor with tt Recursive Control Loop.

    Core Design:
    - tt() is the only execution method (tail-recursive)
    - All other methods are thin wrappers around tt()
    - No iteration loops - only recursion
    - Immutable state (TurnState)

    Example:
        ```python
        executor = AgentExecutor(llm=llm, tools=tools)

        # Initialize state
        turn_state = TurnState.initial(max_iterations=10)
        context = ExecutionContext.create()
        messages = [Message(role="user", content="Hello")]

        # Execute with tt recursion
        async for event in executor.tt(messages, turn_state, context):
            print(event)
        ```
    """

    def __init__(
        self,
        llm: BaseLLM,
        tools: Dict[str, BaseTool] | None = None,
        memory: BaseMemory | None = None,
        compressor: BaseCompressor | None = None,
        context_retriever: Optional["ContextRetriever"] = None,
        steering_control: SteeringControl | None = None,
        max_iterations: int = 50,
        max_context_tokens: int = 16000,
        permission_manager: PermissionManager | None = None,
        metrics: MetricsCollector | None = None,
        system_instructions: Optional[str] = None,
        callbacks: Optional[List[BaseCallback]] = None,
        enable_steering: bool = False,
        task_handlers: Optional[List[TaskHandler]] = None,
        unified_context: Optional["UnifiedExecutionContext"] = None,
        enable_unified_coordination: bool = True,
        # Phase 2: Recursion Control
        enable_recursion_control: bool = True,
        recursion_monitor: Optional[RecursionMonitor] = None,
    ) -> None:
        self.llm = llm
        self.tools = tools or {}
        self.memory = memory
        self.compressor = compressor
        self.context_retriever = context_retriever
        self.steering_control = steering_control or SteeringControl()
        self.max_iterations = max_iterations
        self.max_context_tokens = max_context_tokens
        self.metrics = metrics or MetricsCollector()
        self.permission_manager = permission_manager or PermissionManager(
            policy={"default": "allow"}
        )
        self.system_instructions = system_instructions
        self.callbacks = callbacks or []
        self.enable_steering = enable_steering
        self.task_handlers = task_handlers or []

        # Unified coordination
        self.unified_context = unified_context
        self.enable_unified_coordination = enable_unified_coordination

        # Phase 2: Recursion control
        self.enable_recursion_control = enable_recursion_control
        self.recursion_monitor = recursion_monitor or RecursionMonitor(
            max_iterations=max_iterations
        )

        # Initialize unified coordination if enabled
        if self.enable_unified_coordination and UnifiedExecutionContext and IntelligentCoordinator:
            self._setup_unified_coordination()

        # Tool execution (legacy pipeline for backward compatibility)
        self.tool_pipeline = ToolExecutionPipeline(
            self.tools,
            permission_manager=self.permission_manager,
            metrics=self.metrics,
        )

    def _setup_unified_coordination(self):
        """è®¾ç½®ç»Ÿä¸€åè°ƒæœºåˆ¶"""
        if not self.unified_context:
            # åˆ›å»ºé»˜è®¤çš„ç»Ÿä¸€æ‰§è¡Œä¸Šä¸‹æ–‡
            from loom.core.unified_coordination import CoordinationConfig
            self.unified_context = UnifiedExecutionContext(
                execution_id=f"exec_{int(time.time())}",
                config=CoordinationConfig()  # ä½¿ç”¨é»˜è®¤é…ç½®
            )
        
        # é›†æˆå››å¤§æ ¸å¿ƒèƒ½åŠ›
        self._integrate_core_capabilities()
        
        # åˆ›å»ºæ™ºèƒ½åè°ƒå™¨
        self.coordinator = IntelligentCoordinator(self.unified_context)
        
        # è®¾ç½®è·¨ç»„ä»¶å¼•ç”¨
        self._setup_cross_component_references()

    def _integrate_core_capabilities(self):
        """é›†æˆå››å¤§æ ¸å¿ƒèƒ½åŠ›åˆ°ç»Ÿä¸€ä¸Šä¸‹æ–‡"""

        config = self.unified_context.config

        # 1. é›†æˆ ContextAssembler
        if not self.unified_context.context_assembler:
            from loom.core.context_assembly import ContextAssembler, ComponentPriority
            import json

            self.unified_context.context_assembler = ContextAssembler(
                max_tokens=self.max_context_tokens,
                enable_caching=True,
                cache_size=config.context_cache_size
            )

            # ã€å…³é”®ä¿®å¤ã€‘æ·»åŠ  system_instructions ä½œä¸ºåŸºç¡€ç»„ä»¶
            if self.system_instructions:
                self.unified_context.context_assembler.add_component(
                    name="base_instructions",
                    content=self.system_instructions,
                    priority=ComponentPriority.CRITICAL,
                    truncatable=False,
                )

            # æ·»åŠ å·¥å…·å®šä¹‰
            if self.tools:
                tools_spec = self._serialize_tools()
                tools_prompt = f"Available tools:\n{json.dumps(tools_spec, indent=2)}"
                self.unified_context.context_assembler.add_component(
                    name="tool_definitions",
                    content=tools_prompt,
                    priority=ComponentPriority.MEDIUM,
                    truncatable=False,
                )

        # 2. é›†æˆ TaskTool
        if "task" in self.tools and not self.unified_context.task_tool:
            task_tool = self.tools["task"]
            # ä½¿ç”¨é…ç½®æ›´æ–° TaskTool
            task_tool.pool_size = config.subagent_pool_size
            task_tool.enable_pooling = True
            self.unified_context.task_tool = task_tool

        # 3. é›†æˆ EventProcessor
        if not self.unified_context.event_processor:
            from loom.core.events import EventFilter, EventProcessor, AgentEventType

            # åˆ›å»ºæ™ºèƒ½äº‹ä»¶è¿‡æ»¤å™¨ï¼Œä½¿ç”¨é…ç½®å€¼
            llm_filter = EventFilter(
                allowed_types=[
                    AgentEventType.LLM_DELTA,
                    AgentEventType.TOOL_RESULT,
                    AgentEventType.AGENT_FINISH
                ],
                enable_batching=True,
                batch_size=config.event_batch_size,
                batch_timeout=config.event_batch_timeout
            )

            self.unified_context.event_processor = EventProcessor(
                filters=[llm_filter],
                enable_stats=True
            )

        # 4. é›†æˆ TaskHandlers
        if not self.unified_context.task_handlers:
            self.unified_context.task_handlers = self.task_handlers or []

    def _setup_cross_component_references(self):
        """
        è®¾ç½®è·¨ç»„ä»¶å¼•ç”¨ï¼ˆå·²ç®€åŒ–ï¼‰

        ç§»é™¤äº†é­”æ³•å±žæ€§æ³¨å…¥ï¼Œæ”¹ä¸ºé€šè¿‡åè°ƒå™¨å¤„ç†æ‰€æœ‰è·¨ç»„ä»¶é€šä¿¡
        """
        pass  # è·¨ç»„ä»¶é€šä¿¡çŽ°åœ¨é€šè¿‡ IntelligentCoordinator å¤„ç†

        # Tool orchestration (Loom 2.0 - intelligent parallel/sequential execution)
        self.tool_orchestrator = ToolOrchestrator(
            tools=self.tools,
            permission_manager=self.permission_manager,
            max_parallel=5,
        )

    # ==========================================
    # CORE METHOD: tt (Tail-Recursive Control Loop)
    # ==========================================

    async def tt(
        self,
        messages: List[Message],
        turn_state: TurnState,
        context: ExecutionContext,
    ) -> AsyncGenerator[AgentEvent, None]:
        """
        Tail-recursive control loop (inspired by Claude Code).

        This is the ONLY core execution method. It processes one turn of the
        conversation, then recursively calls itself if tools were used.

        Recursion Flow:
            tt(messages, state_0, ctx)
              â†’ LLM generates tool calls
              â†’ Execute tools
              â†’ tt(messages + tool_results, state_1, ctx)  # Recursive call
                  â†’ LLM generates final answer
                  â†’ return (base case)

        Base Cases (recursion terminates):
        1. LLM returns final answer (no tools)
        2. Maximum recursion depth reached
        3. Execution cancelled
        4. Error occurred

        Args:
            messages: New messages for this turn (not full history)
            turn_state: Immutable turn state
            context: Shared execution context

        Yields:
            AgentEvent: Events representing execution progress

        Example:
            ```python
            # Initial turn
            state = TurnState.initial(max_iterations=10)
            context = ExecutionContext.create()
            messages = [Message(role="user", content="Search files")]

            async for event in executor.tt(messages, state, context):
                if event.type == AgentEventType.AGENT_FINISH:
                    print(f"Done: {event.content}")
            ```
        """
        # ==========================================
        # Phase 0: Recursion Control
        # ==========================================
        yield AgentEvent(
            type=AgentEventType.ITERATION_START,
            iteration=turn_state.turn_counter,
            turn_id=turn_state.turn_id,
            metadata={"parent_turn_id": turn_state.parent_turn_id},
        )

        # Phase 2: Advanced recursion control (optional)
        if self.enable_recursion_control:
            # Build recursion state from turn state
            recursion_state = RecursionState(
                iteration=turn_state.turn_counter,
                tool_call_history=turn_state.tool_call_history,
                error_count=turn_state.error_count,
                last_outputs=turn_state.last_outputs
            )

            # Check for termination conditions
            termination_reason = self.recursion_monitor.check_termination(
                recursion_state
            )

            if termination_reason:
                # Emit termination event
                yield AgentEvent(
                    type=AgentEventType.RECURSION_TERMINATED,
                    metadata={
                        "reason": termination_reason.value,
                        "iteration": turn_state.turn_counter,
                        "tool_call_history": turn_state.tool_call_history[-5:],
                        "error_count": turn_state.error_count
                    }
                )

                # Add termination message to prompt LLM to finish
                termination_msg = self.recursion_monitor.build_termination_message(
                    termination_reason
                )

                # Add termination guidance as system message
                messages = messages + [
                    Message(role="system", content=termination_msg)
                ]

                # Note: We continue execution but with termination guidance
                # The LLM will receive the termination message and should wrap up

            # Check for early warnings (not terminating yet, just warning)
            elif warning_msg := self.recursion_monitor.should_add_warning(
                recursion_state,
                warning_threshold=0.8
            ):
                # Add warning as system message
                messages = messages + [
                    Message(role="system", content=warning_msg)
                ]

        # Base case 1: Maximum recursion depth reached
        if turn_state.is_final:
            yield AgentEvent(
                type=AgentEventType.MAX_ITERATIONS_REACHED,
                metadata={
                    "turn_counter": turn_state.turn_counter,
                    "max_iterations": turn_state.max_iterations,
                },
            )
            await self._emit(
                "max_iterations_reached",
                {
                    "turn_counter": turn_state.turn_counter,
                    "max_iterations": turn_state.max_iterations,
                },
            )
            return

        # Base case 2: Execution cancelled
        if context.is_cancelled():
            yield AgentEvent(
                type=AgentEventType.EXECUTION_CANCELLED,
                metadata={"correlation_id": context.correlation_id},
            )
            await self._emit(
                "execution_cancelled",
                {"correlation_id": context.correlation_id},
            )
            return

        # ==========================================
        # Phase 1: Context Assembly
        # ==========================================
        yield AgentEvent.phase_start("context_assembly")

        # Load conversation history from memory
        history = await self._load_history()

        # RAG retrieval (if configured)
        rag_context = None
        if self.context_retriever:
            yield AgentEvent(type=AgentEventType.RETRIEVAL_START)

            try:
                # Extract user query from last message
                user_query = ""
                for msg in reversed(messages):
                    if msg.role == "user":
                        user_query = msg.content
                        break

                if user_query:
                    retrieved_docs = await self.context_retriever.retrieve_for_query(
                        user_query
                    )

                    if retrieved_docs:
                        rag_context = self.context_retriever.format_documents(
                            retrieved_docs
                        )

                        # Emit retrieval progress
                        for doc in retrieved_docs:
                            yield AgentEvent(
                                type=AgentEventType.RETRIEVAL_PROGRESS,
                                metadata={
                                    "doc_title": doc.metadata.get("title", "Unknown"),
                                    "relevance_score": doc.metadata.get("score", 0.0),
                                },
                            )

                    yield AgentEvent(
                        type=AgentEventType.RETRIEVAL_COMPLETE,
                        metadata={"doc_count": len(retrieved_docs)},
                    )
                    self.metrics.metrics.retrievals = (
                        getattr(self.metrics.metrics, "retrievals", 0) + 1
                    )

            except Exception as e:
                yield AgentEvent.error(e, retrieval_failed=True)

        # Add new messages to history
        history.extend(messages)

        # Compression check
        old_len = len(history)
        history_compacted = await self._maybe_compress(history)
        compacted_this_turn = len(history_compacted) < old_len

        if compacted_this_turn:
            history = history_compacted
            yield AgentEvent(
                type=AgentEventType.COMPRESSION_APPLIED,
                metadata={
                    "messages_before": old_len,
                    "messages_after": len(history),
                },
            )

        # ä½¿ç”¨ç»Ÿä¸€åè°ƒçš„æ™ºèƒ½ä¸Šä¸‹æ–‡ç»„è£…
        if self.enable_unified_coordination and hasattr(self, 'coordinator'):
            # ä½¿ç”¨æ™ºèƒ½åè°ƒå™¨è¿›è¡Œä¸Šä¸‹æ–‡ç»„è£…
            execution_plan = self.coordinator.coordinate_tt_recursion(
                messages, turn_state, context
            )
            final_system_prompt = execution_plan.get("context", "")
            # ä½¿ç”¨ç»Ÿä¸€åè°ƒå™¨çš„ assembler
            assembler = self.unified_context.context_assembler
        else:
            # ä¼ ç»Ÿæ–¹å¼ç»„è£…ç³»ç»Ÿæç¤º
            assembler = ContextAssembler(max_tokens=self.max_context_tokens)

            # Add base instructions (critical priority)
            if self.system_instructions:
                assembler.add_component(
                    name="base_instructions",
                    content=self.system_instructions,
                    priority=ComponentPriority.CRITICAL,
                    truncatable=False,
                )

            # Add RAG context (high priority)
            if rag_context:
                assembler.add_component(
                    name="retrieved_context",
                    content=rag_context,
                    priority=ComponentPriority.HIGH,
                    truncatable=True,
                )

            # Add tool definitions (medium priority)
            if self.tools:
                tools_spec = self._serialize_tools()
                tools_prompt = f"Available tools:\n{json.dumps(tools_spec, indent=2)}"
                assembler.add_component(
                    name="tool_definitions",
                    content=tools_prompt,
                    priority=ComponentPriority.MEDIUM,
                    truncatable=False,
                )

            # Assemble final system prompt
            final_system_prompt = assembler.assemble()

        # Inject system prompt into history
        if history and history[0].role == "system":
            history[0] = Message(role="system", content=final_system_prompt)
        else:
            history.insert(0, Message(role="system", content=final_system_prompt))

        # Emit context assembly summary
        summary = assembler.get_summary()
        yield AgentEvent.phase_end(
            "context_assembly",
            tokens_used=summary["total_tokens"],
            metadata={
                "components": len(summary["components"]),
                "utilization": summary["utilization"],
            },
        )

        # ==========================================
        # Phase 2: LLM Call
        # ==========================================
        yield AgentEvent(type=AgentEventType.LLM_START)

        try:
            if self.llm.supports_tools and self.tools:
                # LLM with tool support
                tools_spec = self._serialize_tools()
                # Convert messages to API format, handling tool_calls in metadata
                api_messages = [self._message_to_api_format(m) for m in history]
                response = await self.llm.generate_with_tools(
                    api_messages, tools_spec
                )

                content = response.get("content", "")
                tool_calls = response.get("tool_calls", [])

                # Emit LLM content if available
                if content:
                    yield AgentEvent(type=AgentEventType.LLM_DELTA, content=content)

            else:
                # Simple LLM generation (streaming)
                content_parts = []
                # Convert messages to API format
                api_messages = [self._message_to_api_format(m) for m in history]
                async for delta in self.llm.stream(api_messages):
                    content_parts.append(delta)
                    yield AgentEvent(type=AgentEventType.LLM_DELTA, content=delta)

                content = "".join(content_parts)
                tool_calls = []

            yield AgentEvent(type=AgentEventType.LLM_COMPLETE)

        except Exception as e:
            self.metrics.metrics.total_errors += 1
            yield AgentEvent.error(e, llm_failed=True)
            await self._emit("error", {"stage": "llm_call", "message": str(e)})
            return

        self.metrics.metrics.llm_calls += 1

        # ==========================================
        # Phase 3: Decision Point (Base Case or Recurse)
        # ==========================================

        if not tool_calls:
            # Base case: No tools â†’ Conversation complete
            yield AgentEvent(
                type=AgentEventType.AGENT_FINISH,
                content=content,
                metadata={
                    "turn_counter": turn_state.turn_counter,
                    "total_llm_calls": self.metrics.metrics.llm_calls,
                },
            )

            # Save to memory
            if self.memory and content:
                await self.memory.add_message(
                    Message(role="assistant", content=content)
                )

            await self._emit("agent_finish", {"content": content})
            return

        # ==========================================
        # Phase 4: Tool Execution
        # ==========================================
        yield AgentEvent(
            type=AgentEventType.LLM_TOOL_CALLS,
            metadata={
                "tool_count": len(tool_calls),
                "tool_names": [tc.get("name") for tc in tool_calls],
            },
        )

        # Convert to ToolCall models
        tc_models = [self._to_tool_call(tc) for tc in tool_calls]

        # Execute tools using ToolOrchestrator
        tool_results: List[ToolResult] = []
        
        # Save assistant message with tool_calls to memory first
        # This is critical: assistant message must come before tool messages
        assistant_msg = Message(
            role="assistant",
            content=content or "",
            metadata={"tool_calls": tool_calls}  # Store tool_calls in metadata for API conversion
        )
        if self.memory:
            await self.memory.add_message(assistant_msg)
        
        try:
            async for event in self.tool_orchestrator.execute_batch(tc_models):
                yield event  # Forward all tool events

                if event.type == AgentEventType.TOOL_RESULT:
                    tool_results.append(event.tool_result)

                    # Add to memory
                    tool_msg = Message(
                        role="tool",
                        content=event.tool_result.content,
                        tool_call_id=event.tool_result.tool_call_id,
                    )
                    if self.memory:
                        await self.memory.add_message(tool_msg)

                elif event.type == AgentEventType.TOOL_ERROR:
                    # Collect error results too
                    if event.tool_result:
                        tool_results.append(event.tool_result)

        except Exception as e:
            self.metrics.metrics.total_errors += 1
            yield AgentEvent.error(e, tool_execution_failed=True)
            await self._emit("error", {"stage": "tool_execution", "message": str(e)})
            return

        yield AgentEvent(
            type=AgentEventType.TOOL_CALLS_COMPLETE,
            metadata={"results_count": len(tool_results)},
        )

        self.metrics.metrics.total_iterations += 1

        # ==========================================
        # Phase 5: Recursive Call (Tail Recursion)
        # ==========================================

        # Phase 2: Track tool calls and errors for recursion control
        tool_names_called = [tc.name for tc in tc_models]
        had_tool_errors = any(r.is_error for r in tool_results)

        # Extract output for loop detection (use first tool result or content)
        output_sample = None
        if tool_results:
            output_sample = tool_results[0].content[:200]  # First 200 chars
        elif content:
            output_sample = content[:200]

        # Prepare next turn state with recursion tracking
        next_state = turn_state.next_turn(
            compacted=compacted_this_turn,
            tool_calls=tool_names_called,
            had_error=had_tool_errors,
            output=output_sample
        )

        # Phase 3: Prepare next turn messages with intelligent context guidance
        # This now includes tool results, compression, and recursion hints
        # Pass assistant message and tool_calls for proper message formatting
        next_messages = await self._prepare_recursive_messages(
            messages, tool_results, tool_calls, content, turn_state, context
        )

        # Check if compression was applied and emit event
        if "last_compression" in context.metadata:
            comp_info = context.metadata.pop("last_compression")
            yield AgentEvent(
                type=AgentEventType.COMPRESSION_APPLIED,
                metadata=comp_info
            )

        # Emit recursion event
        yield AgentEvent(
            type=AgentEventType.RECURSION,
            metadata={
                "from_turn": turn_state.turn_id,
                "to_turn": next_state.turn_id,
                "depth": next_state.turn_counter,
                "tools_called": tool_names_called,
                "message_count": len(next_messages),
            },
        )

        # ðŸ”¥ Tail-recursive call
        async for event in self.tt(next_messages, next_state, context):
            yield event

    # ==========================================
    # Intelligent Recursion Methods
    # ==========================================

    async def _prepare_recursive_messages(
        self,
        messages: List[Message],
        tool_results: List[ToolResult],
        tool_calls: List[Dict],
        assistant_content: str,
        turn_state: TurnState,
        context: ExecutionContext,
    ) -> List[Message]:
        """
        Phase 3: æ™ºèƒ½å‡†å¤‡é€’å½’è°ƒç”¨çš„æ¶ˆæ¯

        ç¡®ä¿å·¥å…·ç»“æžœæ­£ç¡®ä¼ é€’åˆ°ä¸‹ä¸€è½®ï¼Œå¹¶è¿›è¡Œå¿…è¦çš„ä¸Šä¸‹æ–‡ä¼˜åŒ–
        å…³é”®ï¼šå¿…é¡»ç¬¦åˆ OpenAI API çš„æ¶ˆæ¯æ ¼å¼è¦æ±‚ï¼š
        - assistant æ¶ˆæ¯ï¼ˆåŒ…å« tool_callsï¼‰å¿…é¡»ç´§è·Ÿåœ¨ä¹‹å‰çš„æ¶ˆæ¯ä¹‹åŽ
        - tool æ¶ˆæ¯å¿…é¡»ç´§è·Ÿåœ¨å¯¹åº”çš„ assistant æ¶ˆæ¯ä¹‹åŽ
        - ä¸èƒ½åœ¨ tool æ¶ˆæ¯å‰æ’å…¥æ–°çš„ user æ¶ˆæ¯

        Args:
            messages: å½“å‰è½®æ¬¡çš„æ¶ˆæ¯
            tool_results: å·¥å…·æ‰§è¡Œç»“æžœ
            tool_calls: å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆç”¨äºŽåˆ›å»º assistant æ¶ˆæ¯ï¼‰
            assistant_content: Assistant æ¶ˆæ¯çš„å†…å®¹
            turn_state: å½“å‰è½®æ¬¡çŠ¶æ€
            context: æ‰§è¡Œä¸Šä¸‹æ–‡

        Returns:
            å‡†å¤‡å¥½çš„ä¸‹ä¸€è½®æ¶ˆæ¯åˆ—è¡¨
        """
        # 1. é¦–å…ˆæ·»åŠ  assistant æ¶ˆæ¯ï¼ˆåŒ…å« tool_callsï¼‰
        # è¿™æ˜¯å…³é”®ï¼šassistant æ¶ˆæ¯å¿…é¡»åœ¨ tool æ¶ˆæ¯ä¹‹å‰
        assistant_msg = Message(
            role="assistant",
            content=assistant_content or "",
            metadata={"tool_calls": tool_calls}  # Store tool_calls in metadata
        )
        next_messages = [assistant_msg]

        # 2. æ·»åŠ å·¥å…·ç»“æžœæ¶ˆæ¯ï¼ˆå¿…é¡»ç´§è·Ÿåœ¨ assistant æ¶ˆæ¯ä¹‹åŽï¼‰
        for result in tool_results:
            next_messages.append(Message(
                role="tool",
                content=result.content,
                tool_call_id=result.tool_call_id,
                metadata=result.metadata or {}
            ))
        
        # 3. å¦‚æžœéœ€è¦æŒ‡å¯¼ä¿¡æ¯ï¼Œåœ¨ tool æ¶ˆæ¯ä¹‹åŽæ·»åŠ ï¼ˆä½œä¸ºç³»ç»Ÿæ¶ˆæ¯ï¼‰
        # è¿™æ ·å¯ä»¥é¿å…è¿å API çš„æ¶ˆæ¯æ ¼å¼è¦æ±‚
        result_analysis = self._analyze_tool_results(tool_results)
        original_task = self._extract_original_task(messages)
        
        guidance_message = self._generate_recursion_guidance(
            original_task, result_analysis, turn_state.turn_counter
        )
        
        # åªæœ‰åœ¨æœ‰æŒ‡å¯¼ä¿¡æ¯æ—¶æ‰æ·»åŠ 
        if guidance_message and guidance_message.strip():
            # å°†æŒ‡å¯¼ä¿¡æ¯ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯æ·»åŠ åˆ° tool æ¶ˆæ¯ä¹‹åŽ
            next_messages.append(Message(role="system", content=guidance_message))

        # 5. Phase 3: æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦
        estimated_tokens = self._estimate_tokens(next_messages)
        compression_applied = False

        if estimated_tokens > self.max_context_tokens:
            # è§¦å‘åŽ‹ç¼©ï¼ˆå¦‚æžœæœ‰ compressorï¼‰
            if self.compressor:
                tokens_before = estimated_tokens
                next_messages = await self._compress_messages(next_messages)
                tokens_after = self._estimate_tokens(next_messages)
                compression_applied = True

                # Store compression info for later emission
                context.metadata["last_compression"] = {
                    "tokens_before": tokens_before,
                    "tokens_after": tokens_after,
                    "trigger": "recursive_message_preparation"
                }

        # 6. Phase 3: æ·»åŠ é€’å½’æ·±åº¦æç¤ºï¼ˆæ·±åº¦é€’å½’æ—¶ï¼‰
        if turn_state.turn_counter > 3:
            hint_content = self._build_recursion_hint(
                turn_state.turn_counter,
                turn_state.max_iterations
            )

            hint = Message(
                role="system",
                content=hint_content
            )
            next_messages.append(hint)

        return next_messages

    def _estimate_tokens(self, messages: List[Message]) -> int:
        """
        ä¼°ç®—æ¶ˆæ¯åˆ—è¡¨çš„ token æ•°é‡

        ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼šå­—ç¬¦æ•° / 4
        ç”Ÿäº§çŽ¯å¢ƒä¸­åº”ä½¿ç”¨å…·ä½“æ¨¡åž‹çš„ tokenizer
        """
        return count_messages_tokens(messages)

    async def _compress_messages(
        self,
        messages: List[Message]
    ) -> List[Message]:
        """
        åŽ‹ç¼©æ¶ˆæ¯åˆ—è¡¨ï¼ˆå¦‚æžœæœ‰ compressorï¼‰

        è¿™ä¸ªæ–¹æ³•ä¼šè°ƒç”¨é…ç½®çš„ compressor æ¥å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
        """
        if not self.compressor:
            return messages

        try:
            compressed, metadata = await self.compressor.compress(messages)

            # Update compression metrics
            self.metrics.metrics.compressions = (
                getattr(self.metrics.metrics, "compressions", 0) + 1
            )

            return compressed
        except Exception as e:
            # If compression fails, return original messages
            self.metrics.metrics.total_errors += 1
            await self._emit(
                "error",
                {"stage": "message_compression", "message": str(e)}
            )
            return messages

    def _build_recursion_hint(self, current_depth: int, max_depth: int) -> str:
        """
        æž„å»ºé€’å½’æ·±åº¦æç¤ºæ¶ˆæ¯

        åœ¨æ·±åº¦é€’å½’æ—¶æé†’ LLM æ³¨æ„è¿›åº¦å’Œé¿å…é‡å¤
        """
        remaining = max_depth - current_depth
        progress = (current_depth / max_depth) * 100

        hint = f"""ðŸ”„ Recursion Status:
- Depth: {current_depth}/{max_depth} ({progress:.0f}% of maximum)
- Remaining iterations: {remaining}

Please review the tool results above and make meaningful progress towards completing the task.
Avoid calling the same tool repeatedly with the same arguments unless necessary.
If you have enough information, please provide your final answer."""

        return hint

    def _analyze_tool_results(self, tool_results: List[ToolResult]) -> Dict[str, Any]:
        """åˆ†æžå·¥å…·ç»“æžœç±»åž‹å’Œè´¨é‡"""
        analysis = {
            "has_data": False,
            "has_errors": False,
            "suggests_completion": False,
            "result_types": [],
            "completeness_score": 0.0
        }
        
        for result in tool_results:
            content = result.content.lower()
            
            # æ£€æŸ¥æ•°æ®ç±»åž‹
            if any(keyword in content for keyword in ["data", "found", "retrieved", "table", "schema", "èŽ·å–åˆ°", "è¡¨ç»“æž„", "ç»“æž„"]):
                analysis["has_data"] = True
                analysis["result_types"].append("data")
                analysis["completeness_score"] += 0.3
            
            # æ£€æŸ¥é”™è¯¯
            if any(keyword in content for keyword in ["error", "failed", "exception", "not found"]):
                analysis["has_errors"] = True
                analysis["result_types"].append("error")
            
            # æ£€æŸ¥å®Œæˆå»ºè®®
            if any(keyword in content for keyword in ["complete", "finished", "done", "ready"]):
                analysis["suggests_completion"] = True
                analysis["result_types"].append("completion")
                analysis["completeness_score"] += 0.5
            
            # æ£€æŸ¥åˆ†æžç»“æžœ
            if any(keyword in content for keyword in ["analysis", "summary", "conclusion", "insights"]):
                analysis["result_types"].append("analysis")
                analysis["completeness_score"] += 0.4
        
        analysis["completeness_score"] = min(analysis["completeness_score"], 1.0)
        return analysis

    def _extract_original_task(self, messages: List[Message]) -> str:
        """ä»Žæ¶ˆæ¯åŽ†å²ä¸­æå–åŽŸå§‹ä»»åŠ¡"""
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯ä½œä¸ºåŽŸå§‹ä»»åŠ¡
        for message in messages:
            if message.role == "user" and message.content:
                # è¿‡æ»¤æŽ‰ç³»ç»Ÿç”Ÿæˆçš„é€’å½’æ¶ˆæ¯
                if not any(keyword in message.content.lower() for keyword in [
                    "å·¥å…·è°ƒç”¨å·²å®Œæˆ", "è¯·åŸºäºŽå·¥å…·è¿”å›žçš„ç»“æžœ", "ä¸è¦ç»§ç»­è°ƒç”¨å·¥å…·"
                ]):
                    return message.content
        return "å¤„ç†ç”¨æˆ·è¯·æ±‚"

    def _generate_recursion_guidance(
        self,
        original_task: str,
        result_analysis: Dict[str, Any],
        recursion_depth: int
    ) -> str:
        """ç”Ÿæˆé€’å½’æŒ‡å¯¼æ¶ˆæ¯"""
        
        # ä½¿ç”¨å¯æ‰©å±•çš„ä»»åŠ¡å¤„ç†å™¨
        if hasattr(self, 'task_handlers') and self.task_handlers:
            for handler in self.task_handlers:
                if handler.can_handle(original_task):
                    return handler.generate_guidance(original_task, result_analysis, recursion_depth)
        
        # é»˜è®¤å¤„ç†
        return self._generate_default_guidance(original_task, result_analysis, recursion_depth)


    def _generate_default_guidance(
        self,
        original_task: str,
        result_analysis: Dict[str, Any],
        recursion_depth: int
    ) -> str:
        """ç”Ÿæˆé»˜è®¤çš„é€’å½’æŒ‡å¯¼"""
        
        if result_analysis["suggests_completion"] or recursion_depth >= 6:
            return f"""å·¥å…·è°ƒç”¨å·²å®Œæˆã€‚è¯·åŸºäºŽè¿”å›žçš„ç»“æžœå®Œæˆä»»åŠ¡ï¼š{original_task}

è¯·æä¾›å®Œæ•´ã€å‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚"""
        
        elif result_analysis["has_errors"]:
            return f"""å·¥å…·æ‰§è¡Œé‡åˆ°é—®é¢˜ã€‚è¯·é‡æ–°å°è¯•å®Œæˆä»»åŠ¡ï¼š{original_task}

å»ºè®®ï¼š
- æ£€æŸ¥å·¥å…·å‚æ•°æ˜¯å¦æ­£ç¡®
- å°è¯•ä½¿ç”¨ä¸åŒçš„å·¥å…·æˆ–æ–¹æ³•
- å¦‚æžœé—®é¢˜æŒç»­ï¼Œè¯·è¯´æ˜Žå…·ä½“é”™è¯¯"""
        
        else:
            return f"""ç»§ç»­å¤„ç†ä»»åŠ¡ï¼š{original_task}

å½“å‰è¿›åº¦ï¼š{result_analysis['completeness_score']:.0%}
å»ºè®®ï¼šä½¿ç”¨æ›´å¤šå·¥å…·æ”¶é›†ä¿¡æ¯æˆ–åˆ†æžå·²èŽ·å¾—çš„ç»“æžœ"""

    # ==========================================
    # Helper Methods
    # ==========================================

    async def _load_history(self) -> List[Message]:
        """Load conversation history from memory."""
        if not self.memory:
            return []
        return await self.memory.get_messages()

    async def _maybe_compress(self, history: List[Message]) -> List[Message]:
        """Check if compression needed and apply if threshold reached."""
        if not self.compressor:
            return history

        tokens_before = count_messages_tokens(history)

        # Check if compression should be triggered (92% threshold)
        if self.compressor.should_compress(tokens_before, self.max_context_tokens):
            try:
                compressed_messages, metadata = await self.compressor.compress(history)

                # Update metrics
                self.metrics.metrics.compressions = (
                    getattr(self.metrics.metrics, "compressions", 0) + 1
                )
                if metadata.key_topics == ["fallback"]:
                    self.metrics.metrics.compression_fallbacks = (
                        getattr(self.metrics.metrics, "compression_fallbacks", 0) + 1
                    )

                # Emit compression event
                await self._emit(
                    "compression_applied",
                    {
                        "before_tokens": metadata.original_tokens,
                        "after_tokens": metadata.compressed_tokens,
                        "compression_ratio": metadata.compression_ratio,
                        "original_message_count": metadata.original_message_count,
                        "compressed_message_count": metadata.compressed_message_count,
                        "key_topics": metadata.key_topics,
                        "fallback_used": metadata.key_topics == ["fallback"],
                    },
                )

                return compressed_messages

            except Exception as e:
                self.metrics.metrics.total_errors += 1
                await self._emit(
                    "error",
                    {"stage": "compression", "message": str(e)},
                )
                return history

        return history

    def _serialize_tools(self) -> List[Dict]:
        """Serialize tools to LLM-compatible format."""
        tools_spec: List[Dict] = []
        for t in self.tools.values():
            schema = {}
            try:
                schema = t.args_schema.model_json_schema()  # type: ignore[attr-defined]
            except Exception:
                schema = {"type": "object", "properties": {}}

            tools_spec.append(
                {
                    "type": "function",
                    "function": {
                        "name": t.name,
                        "description": getattr(t, "description", ""),
                        "parameters": schema,
                    },
                }
            )
        return tools_spec

    def _to_tool_call(self, raw: Dict) -> ToolCall:
        """Convert raw dict to ToolCall model."""
        return ToolCall(
            id=str(raw.get("id", "call_0")),
            name=raw["name"],
            arguments=raw.get("arguments", {}),
        )

    def _message_to_api_format(self, message: Message) -> Dict:
        """
        Convert Message object to OpenAI API format.

        Handles special case: assistant messages with tool_calls in metadata
        must be converted to API format with tool_calls field.

        According to OpenAI API spec:
        - When assistant message has tool_calls, content should be null
        - tool_calls must be at top level, not in metadata

        Args:
            message: Message object to convert

        Returns:
            Dict in OpenAI API message format
        """
        api_msg = {
            "role": message.role,
            "content": message.content or None,
        }

        # Handle tool messages
        if message.role == "tool" and message.tool_call_id:
            api_msg["tool_call_id"] = message.tool_call_id

        # Handle assistant messages with tool_calls
        # Check if metadata contains tool_calls (metadata is always a dict per Message dataclass)
        if message.role == "assistant" and "tool_calls" in message.metadata:
            tool_calls = message.metadata["tool_calls"]
            if isinstance(tool_calls, list) and tool_calls:  # Validate it's a non-empty list
                # According to OpenAI API spec, when tool_calls exist, content should be null
                api_msg["content"] = None

                # Convert tool_calls to OpenAI API format
                api_tool_calls = []
                for tc in tool_calls:
                    if not isinstance(tc, dict):
                        continue  # Skip invalid tool call entries

                    # Handle arguments: validate and serialize properly
                    arguments = tc.get("arguments", {})
                    if isinstance(arguments, str):
                        # Validate it's valid JSON string, use as-is if valid
                        try:
                            json.loads(arguments)  # Validate JSON
                            arguments_str = arguments
                        except (json.JSONDecodeError, ValueError):
                            # Invalid JSON, treat as empty dict
                            arguments_str = "{}"
                    elif isinstance(arguments, dict):
                        # Serialize dict to JSON string
                        try:
                            arguments_str = json.dumps(arguments)
                        except (TypeError, ValueError):
                            # Fallback to empty dict if serialization fails
                            arguments_str = "{}"
                    else:
                        # Fallback: convert to empty dict
                        arguments_str = "{}"

                    # Validate required fields exist
                    tool_id = tc.get("id", "")
                    tool_name = tc.get("name", "")

                    if tool_id and tool_name:  # Only add valid tool calls
                        api_tool_calls.append({
                            "id": tool_id,
                            "type": "function",
                            "function": {
                                "name": tool_name,
                                "arguments": arguments_str
                            }
                        })

                if api_tool_calls:  # Only add if we have valid tool calls
                    api_msg["tool_calls"] = api_tool_calls

        return api_msg

    async def _emit(self, event_type: str, payload: Dict) -> None:
        """Emit event to callbacks."""
        if not self.callbacks:
            return

        enriched = dict(payload)
        enriched.setdefault("ts", time.time())
        enriched.setdefault("type", event_type)

        for cb in self.callbacks:
            try:
                await cb.on_event(event_type, enriched)
            except Exception:
                # Best-effort; don't fail execution on callback errors
                pass

    # ==========================================
    # Backward Compatibility Wrappers
    # ==========================================

    async def execute(
        self,
        user_input: str,
        cancel_token: Optional[asyncio.Event] = None,
        correlation_id: Optional[str] = None,
    ) -> str:
        """
        Execute agent and return final response (backward compatible wrapper).

        This method wraps the new tt() recursive API and extracts the final
        response for backward compatibility with existing code.

        Args:
            user_input: User input text
            cancel_token: Optional cancellation event
            correlation_id: Optional correlation ID for tracing

        Returns:
            str: Final response text

        Example:
            ```python
            executor = AgentExecutor(llm=llm, tools=tools)
            response = await executor.execute("Hello")
            print(response)
            ```
        """
        # Initialize state and context
        turn_state = TurnState.initial(max_iterations=self.max_iterations)
        context = ExecutionContext.create(
            correlation_id=correlation_id,
            cancel_token=cancel_token,
        )
        messages = [Message(role="user", content=user_input)]

        # Execute with tt and collect result
        final_content = ""
        async for event in self.tt(messages, turn_state, context):
            # Accumulate LLM deltas
            if event.type == AgentEventType.LLM_DELTA:
                final_content += event.content or ""

            # Return on finish
            elif event.type == AgentEventType.AGENT_FINISH:
                return event.content or final_content

            # Handle cancellation
            elif event.type == AgentEventType.EXECUTION_CANCELLED:
                return "cancelled"

            # Handle max iterations
            elif event.type == AgentEventType.MAX_ITERATIONS_REACHED:
                return final_content or "Max iterations reached"

            # Raise on error
            elif event.type == AgentEventType.ERROR:
                if event.error:
                    raise event.error

        return final_content
