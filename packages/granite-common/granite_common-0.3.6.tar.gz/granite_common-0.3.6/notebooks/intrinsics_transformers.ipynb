{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a948907",
   "metadata": {},
   "source": [
    "# Using Intrinsics Directly on the Hugging Face Transformers Library\n",
    "\n",
    "This notebook demonstrates how to use the shared input and output processing code for\n",
    "intrinsics when performing model inference directly on top of models from the \n",
    "[Transformers library](https://huggingface.co/docs/transformers/en/index).\n",
    "\n",
    "Note that running inference in this way is significantly slower than using `vLLM` or\n",
    "another OpenAI-compatible scalable inference engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2285bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import itertools\n",
    "import json\n",
    "import granite_common.util\n",
    "import torch\n",
    "from granite_common.base.types import ChatCompletionResponseChoice\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba129e8",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Change the value of the constants `intrinsic_name` and `base_model` in the cell that \n",
    "follows to change which intrinsic will be demonstrated in the remainder of this notebook.\n",
    "\n",
    "Other constants will automatically adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf424c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following two parameters as needed\n",
    "intrinsic_name = \"answerability\"\n",
    "base_model_name = \"granite-3.3-2b-instruct\"\n",
    "intrinsics_repo_name = \"ibm-granite/rag-intrinsics-lib\"\n",
    "use_alora = False\n",
    "\n",
    "use_cuda = False  # Set to False to use default PyTorch device for this machine + model\n",
    "\n",
    "#######################################################################################\n",
    "# The code below adjusts the remaining constants according to the chosen intrinsic.\n",
    "\n",
    "TESTDATA_DIR = \"../tests/granite_common/intrinsics/rag/testdata\"\n",
    "KNOWN_INTRINSICS = [\n",
    "    \"answerability\",\n",
    "    \"answer_relevance_classifier\",\n",
    "    \"answer_relevance_rewriter\",\n",
    "    \"citations\",\n",
    "    \"context_relevance\",\n",
    "    \"hallucination_detection\",\n",
    "    \"query_rewrite\",\n",
    "    \"requirement_check\",\n",
    "    \"uncertainty\",\n",
    "]\n",
    "INTRINSICS_WITH_LOCAL_YAML_FILES = []\n",
    "MODEL_TO_CONSTRAINED_DECODING_PREFIX = {\n",
    "    # Some base models generate their own generation prompts, for example for thinking\n",
    "    # or for OpenAI channel selection. To use constrained decoding with intrinsics\n",
    "    # trained on these models, the code we use in this notebook adds the most common\n",
    "    # generation prompt to the chat template's prompt when applicable.\n",
    "    \"gpt-oss-20b\": (\n",
    "        \"<|channel|>analysis<|message|><|end|>\"\n",
    "        \"<|start|>assistant<|channel|>final<|message|>\"\n",
    "    )\n",
    "}\n",
    "constrained_decoding_prefix = MODEL_TO_CONSTRAINED_DECODING_PREFIX.get(base_model_name)\n",
    "\n",
    "io_yaml_file = None  # None -> load from Hugging Face Hub\n",
    "lora_dir = None  # None --> load from Hugging Face Hub\n",
    "request_json_file = f\"{TESTDATA_DIR}/input_json/{intrinsic_name}.json\"\n",
    "\n",
    "# Include local JSON file with arguments if that file is present.\n",
    "maybe_arg_file = f\"{TESTDATA_DIR}/input_args/{intrinsic_name}.json\"\n",
    "arg_file = maybe_arg_file if os.path.exists(maybe_arg_file) else None\n",
    "\n",
    "# Selectively override defaults\n",
    "if intrinsic_name == \"answerability\":\n",
    "    request_json_file = f\"{TESTDATA_DIR}/input_json/answerable.json\"\n",
    "elif intrinsic_name in INTRINSICS_WITH_LOCAL_YAML_FILES:\n",
    "    # Some io.yaml files not yet delivered to Hugging Face Hub\n",
    "    io_yaml_file = f\"{TESTDATA_DIR}/input_yaml/{intrinsic_name}.yaml\"\n",
    "elif intrinsic_name not in KNOWN_INTRINSICS:\n",
    "    raise ValueError(f\"Unrecognized intrinsic name '{intrinsic_name}'\")\n",
    "\n",
    "# TEMPORARY until we have gpt-oss checkpoints on HF Hub\n",
    "if base_model_name == \"gpt-oss-20b\":\n",
    "    # Use local copy of repo with gpt-oss LoRAs\n",
    "    peft_type = \"alora\" if use_alora else \"lora\"\n",
    "    lora_dir = pathlib.Path(\n",
    "        f\"../../intrinsics-lib/{intrinsic_name}/{peft_type}/{base_model_name}\"\n",
    "    )\n",
    "    io_yaml_file = lora_dir / \"io.yaml\"\n",
    "\n",
    "\n",
    "if io_yaml_file is None:\n",
    "    # Fetch IO configuration file from Hugging Face Hub\n",
    "    io_yaml_file = granite_common.intrinsics.util.obtain_io_yaml(\n",
    "        intrinsic_name, base_model_name, intrinsics_repo_name\n",
    "    )\n",
    "\n",
    "if lora_dir is None:\n",
    "    # Fetch LoRA directory from Hugging Face Hub\n",
    "    lora_dir = granite_common.intrinsics.util.obtain_lora(\n",
    "        intrinsic_name, base_model_name, intrinsics_repo_name, alora=use_alora\n",
    "    )\n",
    "\n",
    "if not os.path.exists(lora_dir):\n",
    "    raise ValueError(f\"LoRA directory {lora_dir} does not exist on this machine.\")\n",
    "\n",
    "# Print the variables we just set\n",
    "print(f\"{lora_dir=}\")\n",
    "print(f\"{io_yaml_file=}\")\n",
    "print(f\"{request_json_file=}\")\n",
    "print(f\"{arg_file=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21faed7",
   "metadata": {},
   "source": [
    "## Instantiate the input and output processing classes\n",
    "\n",
    "The constructors for the classes `IntrinsicsRewriter` and `IntrinsicsResultProcessor`\n",
    "serve as factory methods to produce input and output processors, respectively, for \n",
    "a given intrinsic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Instantiating input and output processing from configuration file:\\n\"\n",
    "    f\"{io_yaml_file}\"\n",
    ")\n",
    "\n",
    "rewriter = granite_common.IntrinsicsRewriter(config_file=io_yaml_file)\n",
    "result_processor = granite_common.IntrinsicsResultProcessor(config_file=io_yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d06c9",
   "metadata": {},
   "source": [
    "## Perform input processing\n",
    "\n",
    "The cells that follow load an example OpenAI-compatible chat completion request from\n",
    "a local file, then show how to apply input processing to the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ea7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read original request from the appropriate file\n",
    "print(f\"Loading request data from {request_json_file}\")\n",
    "with open(request_json_file, encoding=\"utf-8\") as f:\n",
    "    request_json_str = f.read()\n",
    "request_json = json.loads(request_json_str)\n",
    "\n",
    "# Some parameters like model name aren't kept in the JSON files that we use for testing.\n",
    "# Apply appropriate values for those parameters.\n",
    "request_json[\"model\"] = intrinsic_name\n",
    "request_json[\"temperature\"] = 0.0\n",
    "\n",
    "print(\"Original request:\")\n",
    "print(json.dumps(request_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some intrinsics take one or more additional arguments besides the target chat\n",
    "# completion request. Load the additional arguments from a file if that is the case.\n",
    "intrinsic_kwargs = {}\n",
    "if arg_file is not None:\n",
    "    with open(arg_file, encoding=\"utf8\") as file:\n",
    "        intrinsic_kwargs = json.load(file)\n",
    "    print(f\"Using additional arguments:\\n{intrinsic_kwargs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run request through input processing.\n",
    "rewritten_request = rewriter.transform(request_json, **intrinsic_kwargs)\n",
    "\n",
    "print(\"Request after input processing:\")\n",
    "print(rewritten_request.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31e412",
   "metadata": {},
   "source": [
    "## Running inference\n",
    "\n",
    "Passing a request through the input processing `IntrinsicsRewriter.transform()` \n",
    "turns the request into something that can be sent directly to an OpenAI-compatible\n",
    "inference endpoint for the intrinsic.\n",
    "\n",
    "The Transformers library does not have an OpenAI-compatible inference API, so the\n",
    "cells that follow use functions provided by the `granite-common` library to convert\n",
    "the OpenAI-compatible request into the proprietary format of the Transformers\n",
    "library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106457fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model and merge LoRA weights.\n",
    "# Unlike vLLM, the Transformers library does not have a facility for dynamically\n",
    "# switching between many LoRA adapters.\n",
    "model, tokenizer = granite_common.util.load_transformers_lora(lora_dir)\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the chat completion request into a the Transformers library's proprietary\n",
    "# format.\n",
    "generate_input, other_input = (\n",
    "    granite_common.util.chat_completion_request_to_transformers_inputs(\n",
    "        rewritten_request,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        # Note that this last argument is currently only needed for gpt-oss-20b\n",
    "        constrained_decoding_prefix=constrained_decoding_prefix,\n",
    "    )\n",
    ")\n",
    "\n",
    "generate_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14cdf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Transformers library's APIs to generate one or more completions,\n",
    "# then convert those completions into OpenAI-compatible chat completion\n",
    "responses = granite_common.util.generate_with_transformers(\n",
    "    tokenizer, model, generate_input, other_input\n",
    ")\n",
    "print(responses.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022a359",
   "metadata": {},
   "source": [
    "## Post-process inference results\n",
    "\n",
    "The raw output of some intrinsics requires some additional postprocessing to turn it \n",
    "into a form that is easy to consume in an application. This postprocessing occurs in\n",
    "the method `IntrinsicsResultProcessor.transform()`. \n",
    "\n",
    "The cells that follow show how to use this method to transform the raw output of the\n",
    "`chat.completions.create()` API call into the intrinsic's application-level output\n",
    "value.\n",
    "\n",
    "By convention, this application-level output value is returned in the same format as a\n",
    "chat completions request result. Code in the `generate_with_transformers()` function \n",
    "has already converted the results into that format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_responses = result_processor.transform(responses, rewritten_request)\n",
    "print(transformed_responses.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd43cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and pretty-print the JSON from the \"content\" field of the generated\n",
    "# message\n",
    "json.loads(transformed_responses.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc84e86",
   "metadata": {},
   "source": [
    "## Show low-level results\n",
    "\n",
    "Sometimes you may need to see the raw prompts being sent to the Transformers model.\n",
    "\n",
    "The cells that follow contain the code of `generate_with_transformers()` with additional debug printouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGrammar logit processors are single-use, so we need to redo the translation\n",
    "# from a chat completion request.\n",
    "generate_input, other_input = (\n",
    "    granite_common.util.chat_completion_request_to_transformers_inputs(\n",
    "        rewritten_request,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        # Note that this last argument is currently only needed for gpt-oss-20b\n",
    "        constrained_decoding_prefix=constrained_decoding_prefix,\n",
    "    )\n",
    ")\n",
    "generate_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c623de",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc518e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the prompt\n",
    "print(f\"Prompt string:\\n{tokenizer.decode(generate_input['input_tokens'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc518e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tokens must be passed to generate() as a positional argument, not a named\n",
    "# argument.\n",
    "input_tokens = generate_input[\"input_tokens\"]\n",
    "generate_input = generate_input.copy()\n",
    "del generate_input[\"input_tokens\"]\n",
    "\n",
    "\n",
    "generate_result = model.generate(input_tokens, **generate_input)\n",
    "\n",
    "# Result is a a 2D tensor of shape (num responses, prompt + max generated tokens)\n",
    "# containing tokens, plus a tuple of <max generated tokens> tensors of shape\n",
    "# (num beams, vocab size) containing scores.\n",
    "# This is of course not a usable format for downstream processing.\n",
    "# Start by stripping off the prompt, leaving us with a tensor of shape\n",
    "# (num responses, max generated tokens)\n",
    "num_prompt_tokens = input_tokens.shape[1]\n",
    "num_responses = generate_result.sequences.shape[0]\n",
    "generated_tokens = generate_result.sequences[:, num_prompt_tokens:]\n",
    "\n",
    "generated_scores = (\n",
    "    None\n",
    "    if generate_result.scores is None\n",
    "    else (torch.stack(generate_result.scores).swapaxes(0, 1)[:num_responses])\n",
    ")\n",
    "\n",
    "# Iterate over the responses, stripping off EOS tokens\n",
    "choices = []\n",
    "for i in range(num_responses):\n",
    "    response_tokens = generated_tokens[i]\n",
    "\n",
    "    if tokenizer.eos_token_id in response_tokens:\n",
    "        # Strip off everything after the first EOS token.\n",
    "        # Pytorch syntax for finding the first EOS is a bit funky.\n",
    "        eos_ix = (\n",
    "            (response_tokens == tokenizer.eos_token_id).nonzero(as_tuple=True)[0].item()\n",
    "        )\n",
    "        response_tokens = response_tokens[:eos_ix]\n",
    "\n",
    "    response_string = tokenizer.decode(response_tokens)\n",
    "    print(f\"Raw response {i}: {response_string}\")\n",
    "\n",
    "    # The decode() method doesn't return offsets.\n",
    "    # The only supported API to get offsets is to retokenize the string and hope you\n",
    "    # get back the same tokenization.\n",
    "    # This supported API doesn't work reliably, so we fall back on the unsupported\n",
    "    # method of pulling token lengths out of the tokenizer.\n",
    "    ends = list(\n",
    "        itertools.accumulate([len(s) for s in tokenizer.batch_decode(response_tokens)])\n",
    "    )\n",
    "    begins = [0] + ends[:-1]\n",
    "    token_offsets = list(zip(begins, ends, strict=True))\n",
    "\n",
    "    if generated_scores is None:\n",
    "        logprobs_content = None\n",
    "    else:\n",
    "        response_scores = generated_scores[i]\n",
    "\n",
    "        # Scores come back as raw logits. You need to decode them to produce\n",
    "        # logprobs. For consistency with the OpenAI output format, we need to\n",
    "        # decode twice: Once to get the probability of the returned token and a\n",
    "        # second time to get the top k logprobs. As with the OpenAI APIs, the\n",
    "        # returned token may or may not be included in the top k results.\n",
    "        all_logprobs = torch.log_softmax(response_scores.to(torch.float32), 1)\n",
    "        chosen_token_logprobs = [\n",
    "            all_logprobs[token_ix][response_tokens[token_ix]].item()\n",
    "            for token_ix in range(len(response_tokens))\n",
    "        ]\n",
    "        token_strings = [response_string[begin:end] for begin, end in token_offsets]\n",
    "        token_bytes = [list(s.encode(\"utf-8\")) for s in token_strings]\n",
    "\n",
    "        # Transformers has no notion of top-k logprobs, so the parameter that\n",
    "        # triggers that post-processing is passed via other_input.\n",
    "        if \"top_logprobs\" not in other_input:\n",
    "            top_logprobs = [[] for _ in range(len(token_strings))]\n",
    "        else:  # if \"top_logprobs\" in other_input:\n",
    "            top_k_values, top_k_indices = torch.topk(\n",
    "                torch.nan_to_num(all_logprobs, float(\"-inf\")),\n",
    "                other_input[\"top_logprobs\"],\n",
    "            )\n",
    "            top_k_token_strs = [\n",
    "                [tokenizer.decode(t) for t in row_i] for row_i in top_k_indices\n",
    "            ]\n",
    "            top_logprobs = [\n",
    "                [\n",
    "                    {\n",
    "                        \"token\": s,\n",
    "                        \"bytes\": list(s.encode(\"utf8\")),\n",
    "                        \"logprob\": lp.item(),\n",
    "                    }\n",
    "                    for s, lp in zip(strs, lps, strict=True)\n",
    "                ]\n",
    "                for strs, lps in zip(top_k_token_strs, top_k_values, strict=True)\n",
    "            ]\n",
    "\n",
    "        logprobs_content = [\n",
    "            {\n",
    "                \"token\": token_strings[i],\n",
    "                \"bytes\": token_bytes[i],\n",
    "                \"logprob\": chosen_token_logprobs[i],\n",
    "                \"top_logprobs\": top_logprobs[i],\n",
    "            }\n",
    "            for i in range(len(response_tokens))\n",
    "        ]\n",
    "\n",
    "    response_choice_value = {\n",
    "        \"index\": i,\n",
    "        \"message\": {\"content\": response_string, \"role\": \"assistant\"},\n",
    "    }\n",
    "    if logprobs_content is not None:\n",
    "        response_choice_value[\"logprobs\"] = {\"content\": logprobs_content}\n",
    "    response_choice = ChatCompletionResponseChoice.model_validate(response_choice_value)\n",
    "    choices.append(response_choice)\n",
    "\n",
    "# END code from from generate_with_transformers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, choice in enumerate(choices):\n",
    "    print(f\"Choice {i}:\\n{choice.model_dump_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea09a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
