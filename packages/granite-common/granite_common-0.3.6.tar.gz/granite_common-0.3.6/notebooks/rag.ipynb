{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end demo with MTRAG benchmark data\n",
    "\n",
    "This notebook shows several examples of end-to-end RAG use cases that use the retrieval IO processor in conjunction with the IO processors for other Granite-based LoRA adapters. More information about the models used here can be found in our [technical report](https://arxiv.org/html/2504.11704).\n",
    "\n",
    "This notebook requires a hosted Elasticsearch server for retrieval and a hosted vLLM server to perform inference. Change these variables in the `Constants` cell below.\n",
    "\n",
    "```\n",
    "elasticsearch_host = \"https://localhost:32765\"\n",
    "openai_base_url = \"http://localhost:55555/v1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go in this cell\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import granite_common\n",
    "from granite_common.base.types import (\n",
    "    AssistantMessage,\n",
    "    ChatCompletion,\n",
    "    ChatCompletionResponse,\n",
    "    ChatCompletionResponseChoice,\n",
    "    UserMessage,\n",
    "    VLLMExtraBody,\n",
    ")\n",
    "\n",
    "from granite_common.retrievers.util import download_mtrag_embeddings\n",
    "from granite_common.retrievers import (\n",
    "    ElasticsearchRetriever,\n",
    "    InMemoryRetriever,\n",
    "    Retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "CORPUS_NAMES_MAPPINGS = {\n",
    "    \"banking\": \"mt-rag-banking-elser-512-100-20250205\",\n",
    "    \"clapnq\": \"mt-rag-clapnq-elser-512-100-20240503\",\n",
    "    \"fiqa\": \"mt-rag-fiqa-beir-elser-512-100-20240501\",\n",
    "    \"govt\": \"mt-rag-govt-elser-512-100-20240611\",\n",
    "    \"ibmcloud\": \"mt-rag-ibmcloud-elser-512-100-20240502\",\n",
    "    \"scifact\": \"mt-rag-scifact-beir-elser-512-100-20240501\",\n",
    "    \"telco\": \"mt-rag-telco-elser-512-100-20241210\",\n",
    "}\n",
    "\n",
    "DEFAULT_CANNED_RESPONSE = (\n",
    "    \"Sorry, but I am unable to answer this question from the documents retrieved.\"\n",
    ")\n",
    "\n",
    "target_model_name = \"granite-3.3-8b-instruct\"\n",
    "base_model_name = f\"ibm-granite/{target_model_name}\"\n",
    "\n",
    "openai_base_url = \"http://localhost:55555/v1\"\n",
    "openai_api_key = \"rag_intrinsics_1234\"\n",
    "\n",
    "intrinsic_names = [\n",
    "    \"citations\",\n",
    "    \"query_rewrite\",\n",
    "    \"answerability\",\n",
    "    \"hallucination_detection\",\n",
    "    \"uncertainty\",\n",
    "]\n",
    "\n",
    "# retriever_name = \"elasticsearch\"\n",
    "retriever_name = \"embeddings\"\n",
    "corpus_name = \"govt\"\n",
    "\n",
    "if retriever_name == \"elasticsearch\":\n",
    "    # Elasticsearch retriever\n",
    "    elasticsearch_host = \"https://localhost:32765\"\n",
    "elif retriever_name == \"embeddings\":\n",
    "    # Embeddings retriever\n",
    "    temp_data_dir = \"../data/test_retrieval_temp\"\n",
    "    embeddings_data_file = pathlib.Path(temp_data_dir) / f\"{corpus_name}_embeds.parquet\"\n",
    "    embedding_model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "    # Download the indexed corpus if it hasn't already been downloaded.\n",
    "    # This notebook uses a subset of the government corpus from the MTRAG benchmark.\n",
    "    embeddings_location = f\"{temp_data_dir}/{corpus_name}_embeds.parquet\"\n",
    "    if not os.path.exists(embeddings_location):\n",
    "        download_mtrag_embeddings(\n",
    "            embedding_model_name, corpus_name, embeddings_location\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsics\n",
    "# Load config files and create objects\n",
    "\n",
    "intrinsic_rewriters = {}\n",
    "intrinsic_result_processors = {}\n",
    "for intrinsic_name in intrinsic_names:\n",
    "    io_yaml_file = granite_common.intrinsics.util.obtain_io_yaml(\n",
    "        intrinsic_name, base_model_name\n",
    "    )\n",
    "\n",
    "    intrinsic_rewriter = granite_common.IntrinsicsRewriter(config_file=io_yaml_file)\n",
    "    intrinsic_result_processor = granite_common.IntrinsicsResultProcessor(\n",
    "        config_file=io_yaml_file\n",
    "    )\n",
    "\n",
    "    intrinsic_rewriters[intrinsic_name] = intrinsic_rewriter\n",
    "    intrinsic_result_processors[intrinsic_name] = intrinsic_result_processor\n",
    "\n",
    "# Connect to the inference server\n",
    "client = openai.OpenAI(base_url=openai_base_url, api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "\n",
    "if retriever_name == \"elasticsearch\":\n",
    "    # Connect to the Elasticsearch server.\n",
    "    # Due to the setup, we have to open a retriever connection for each corpus.\n",
    "    retrievers = {}\n",
    "    for corpus_name, actual_corpus_name in CORPUS_NAMES_MAPPINGS.items():\n",
    "        retriever = ElasticsearchRetriever(\n",
    "            corpus_name=actual_corpus_name,\n",
    "            host=elasticsearch_host,\n",
    "            verify_certs=False,\n",
    "            ssl_show_warn=False,\n",
    "        )\n",
    "        retrievers[corpus_name] = retriever\n",
    "elif retriever_name == \"embeddings\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "\n",
    "def call_intrinsic(\n",
    "    intrinsic_name: str,\n",
    "    chat_completion_request: dict,\n",
    "    **kwargs,\n",
    ") -> openai.types.chat.ChatCompletion:\n",
    "    \"\"\"\n",
    "    Call an intrinsic with OpenAI Python API objects on input and output.\n",
    "\n",
    "    :param intrinsic_name: Name of intrinsic to invoke\n",
    "    :param chat_completion_request: Chat completion request to make; can be dict or\n",
    "        OpenAI dataclass\n",
    "    :param kwargs: Optional named argument(s) for intrinsic\n",
    "\n",
    "    :returns: OpenAI Python API chat completion containing processed intrinsic outputs\n",
    "    \"\"\"\n",
    "    # Some intrinsics modify the chat object.\n",
    "    _chat_completion_request = chat_completion_request.model_copy(deep=True)\n",
    "\n",
    "    rewriter = intrinsic_rewriters[intrinsic_name]\n",
    "    result_processor = intrinsic_result_processors[intrinsic_name]\n",
    "    rewritten_request = rewriter.transform(_chat_completion_request, **kwargs)\n",
    "\n",
    "    # Set model name manually for now, because vLLM does not maintain any kind of\n",
    "    # metadata that would allow us to determine the right model name.\n",
    "    rewritten_request.model = intrinsic_name\n",
    "\n",
    "    response = client.chat.completions.create(**rewritten_request.model_dump())\n",
    "    # return response\n",
    "    transformed_response = result_processor.transform(response, rewritten_request)\n",
    "\n",
    "    # Convert to same type as OpenAI API\n",
    "    return openai.types.chat.ChatCompletion.model_validate(\n",
    "        transformed_response.model_dump()\n",
    "    )\n",
    "\n",
    "\n",
    "def retrieve_snippets(retriever: Retriever, query: str, top_k: int = 3):\n",
    "    return retriever.retrieve(query, top_k=top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating an example chat completion request. \n",
    "\n",
    "This chat completion request simulates a scenario where the user is chatting with the automated help desk agent of the California State Parks and is asking about internship opportunities. The agent is about to respond to the user's question, \"Cool, how do I sign up?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example chat completion with a user question and two documents.\n",
    "chat_input = ChatCompletion.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the California State Parks help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I'm a student. Do you have internships?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"The California State Parks hires Student Assistants \"\n",
    "                \"to perform a variety of tasks that require limited or no previous \"\n",
    "                \"work experience.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"Cool, how do I sign up?\"},\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "    }\n",
    ")\n",
    "print(chat_input.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by passing the chat completion request directly to the language model, without using retrieval-augmented generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the example through Granite to get an answer.\n",
    "chat_input.model = base_model_name\n",
    "non_rag_completion = client.chat.completions.create(**chat_input.model_dump())\n",
    "\n",
    "display(Markdown(non_rag_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is a hallucination. The actual correct answer can be found [here](\n",
    "    https://www.parks.ca.gov/?page_id=848\n",
    ").\n",
    "\n",
    "We can use the \n",
    "[Uncertainty LoRA](\n",
    "    https://huggingface.co/generative-computing/core-intrinsics-lib/blob/main/uncertainty/README.md)\n",
    "adapter to flag cases such as this one that are not covered by the base model's \n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_intrinsic(\"uncertainty\", chat_input)\n",
    "certainty_score = round(json.loads(response.choices[0].message.content)[\"certainty\"], 2)\n",
    "\n",
    "print(f\"Certainty score is {certainty_score} out of 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low certainty score indicates that the model's training data does not align closely with this question.\n",
    "\n",
    "To answer this question properly, we need to provide the model with domain-specific information. In this case, the relevant information can be found in the Government corpus of the [MTRAG (multi-turn RAG) benchmark](https://github.com/IBM/mt-rag-benchmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retriever_name == \"elasticsearch\":\n",
    "    retriever = retrievers[corpus_name]\n",
    "elif retriever_name == \"embeddings\":\n",
    "    retriever = InMemoryRetriever(embeddings_data_file, embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can send string queries against this vector database to retrieve relevant documents. Here we query the database with the user's last turn from our example conversation, \"Cool, how do I sign up?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The database fetches document snippets that match a given query.\n",
    "# For example, the user's question in the conversation above:\n",
    "query = chat_input.messages[-1].content\n",
    "print(f\"Query is: '{query}'\")\n",
    "print(\"Matching document snippets:\")\n",
    "documents = retrieve_snippets(retriever, query, top_k=3)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We augment the original chat completion request with the document snippets that the retriever fetches when fed the last user turn as a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original chat input and place the documents in the correct place.\n",
    "chat_input_with_docs = chat_input.model_copy(deep=True)\n",
    "chat_input_with_docs.extra_body = VLLMExtraBody(documents=documents)\n",
    "chat_input_with_docs.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the retriever here operates over the last user turn. In this particular conversation, the last user turn is the phrase, \"Cool, how do I sign up?\", which is missing crucial information for retrieving relevant documents -- specifically, what isthe user attempting to sign up for? \n",
    "\n",
    "As a result, the snippets retrieved are not specific to the user's intended question. Instead, they cover the general topic of signing up for things.\n",
    "\n",
    "Let's see what happens if we run our request through the model using these low-quality document snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_completion = client.chat.completions.create(**chat_input_with_docs.model_dump())\n",
    "display(Markdown(rag_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model correctly refuses to answer the question.\n",
    "\n",
    "Unfortunately, the training data for most LLMs is biased against \n",
    "producing this type of result, leading to frequent hallucinations in the presence of faulty retrieved documents. For example, if the last user turn in our example conversation is \"How to I sign up?\", instead of \"*Cool,* how do I sign up?\", the model produces an entirely different response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the last user turn from \"Cool, how do I sign up?\" to \"How to I sign up?\"\n",
    "messages_no_cool = chat_input_with_docs.messages.copy()\n",
    "messages_no_cool[-1].content = \"How do I sign up?\"\n",
    "chat_input_no_cool = chat_input_with_docs.model_copy(\n",
    "    update={\"messages\": messages_no_cool}\n",
    ")\n",
    "rag_result_no_cool = client.chat.completions.create(**chat_input_no_cool.model_dump())\n",
    "display(Markdown(rag_result_no_cool.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [LoRA Adapter for Answerability Classification](\n",
    "    https://huggingface.co/generative-computing/rag-intrinsics-lib/blob/main/answerability/README.md\n",
    ")\n",
    "provides a more robust way to detect this kind of problem. Here's what happens if we run the chat completion request with faulty documents snippets through the answerability model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_intrinsic(\"answerability\", chat_input_with_docs)\n",
    "answerability_likelihood = json.loads(response.choices[0].message.content)[\n",
    "    \"answerability_likelihood\"\n",
    "]\n",
    "answerability_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answerability model detects that the documents we have retrieved cannot be used to answer the user's question. We can wrap this check in a flow that falls back on canned response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if answerability_likelihood >= 0.5:\n",
    "    rag_completion = client.chat.completions.create(**chat_input_with_docs.model_dump())\n",
    "else:\n",
    "    rag_completion = ChatCompletionResponse(\n",
    "        choices=[\n",
    "            ChatCompletionResponseChoice(\n",
    "                index=0, message=AssistantMessage(content=DEFAULT_CANNED_RESPONSE)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "display(Markdown(rag_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we've improved our model output from a hallucinated response to a refusal to answer the question. This result is an improvement, but we can do better if we can retrieve document snippets that are relevant to the user's intent as expressed in the *entire* conversation, not just the last turn.\n",
    "\n",
    "We can use use the [LoRA Adapter for Query Rewrite](\n",
    "    https://huggingface.co/generative-computing/rag-intrinsics-lib/blob/main/query_rewrite/README.md\n",
    ") to rewrite the last user turn into a string that is more useful for retrieving document snippets.\n",
    "Here's what we get if we call this model directly on the original request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_intrinsic(\"query_rewrite\", chat_input_with_docs)\n",
    "rewritten_question = json.loads(response.choices[0].message.content)[\n",
    "    \"rewritten_question\"\n",
    "]\n",
    "rewritten_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the LoRA Adapter for Query Rewrite is a language model, we can ask it to generate multiple different rewrites. We'll use this capability later on to improve end-to-end result quality further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10 rewrites with variations (i.e. increase the temperature).\n",
    "chat_input_10_rewrites = chat_input.model_copy(deep=True)\n",
    "chat_input_10_rewrites.n = 10\n",
    "chat_input_10_rewrites.temperature = 0.8\n",
    "\n",
    "response = call_intrinsic(\"query_rewrite\", chat_input_10_rewrites)\n",
    "for choice in response.choices:\n",
    "    rewritten_question = json.loads(choice.message.content)[\"rewritten_question\"]\n",
    "    print(rewritten_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap the IO processor for this model in a request processor that rewrites\n",
    "the last turn of the chat completion request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_intrinsic(\"query_rewrite\", chat_input)\n",
    "rewritten_question = json.loads(response.choices[0].message.content)[\n",
    "    \"rewritten_question\"\n",
    "]\n",
    "\n",
    "rewritten_chat_input = chat_input.model_copy(deep=True)\n",
    "rewritten_chat_input.messages[-1] = UserMessage(content=rewritten_question)\n",
    "\n",
    "print(\"Messages after rewrite:\")\n",
    "[{\"role\": m.role, \"content\": m.content} for m in rewritten_chat_input.messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch documents with the rewritten query, then use use the answerability LoRA to check that the fetched documents answer the rewritten query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch documents using the rewritten query.\n",
    "documents = retrieve_snippets(\n",
    "    retriever, rewritten_chat_input.messages[-1].content, top_k=3\n",
    ")\n",
    "\n",
    "# Append the documents to the rewritten chat input.\n",
    "rewritten_chat_input_with_docs = rewritten_chat_input.model_copy(deep=True)\n",
    "rewritten_chat_input_with_docs.extra_body = VLLMExtraBody(documents=documents)\n",
    "\n",
    "# Check the answerability of the chat with rewritten query.\n",
    "response = call_intrinsic(\"answerability\", rewritten_chat_input_with_docs)\n",
    "answerability_likelihood = json.loads(response.choices[0].message.content)[\n",
    "    \"answerability_likelihood\"\n",
    "]\n",
    "answerability_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify that the fetched documents answer the *original* query prior to the rewrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input_with_docs_from_rewrite = chat_input_with_docs.model_copy(deep=True)\n",
    "chat_input_with_docs_from_rewrite.extra_body.documents = (\n",
    "    rewritten_chat_input_with_docs.extra_body.documents\n",
    ")\n",
    "\n",
    "response = call_intrinsic(\"answerability\", chat_input_with_docs_from_rewrite)\n",
    "answerability_likelihood = json.loads(response.choices[0].message.content)[\n",
    "    \"answerability_likelihood\"\n",
    "]\n",
    "answerability_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain all of these request processors together with the IO processor for \n",
    "the answerability model to create a single flow that processes requests in multiple\n",
    "steps:\n",
    "1. Rewrite the last user message for retrieval\n",
    "1. Retrieve documents and attach them to the request\n",
    "1. Check for answerability with the retrieved documents\n",
    "1. If the answerability check passes, then send the request to the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rewrite\n",
    "response = call_intrinsic(\"query_rewrite\", chat_input)\n",
    "rewritten_question = json.loads(response.choices[0].message.content)[\n",
    "    \"rewritten_question\"\n",
    "]\n",
    "\n",
    "new_chat_input = chat_input.model_copy(deep=True)\n",
    "new_chat_input.messages[-1] = UserMessage(content=rewritten_question)\n",
    "\n",
    "# 2. Retrieve\n",
    "documents = retrieve_snippets(\n",
    "    retriever,\n",
    "    new_chat_input.messages[-1].content,\n",
    "    top_k=3,\n",
    ")\n",
    "new_chat_input.extra_body = VLLMExtraBody(documents=documents)\n",
    "\n",
    "# 3. Answerability\n",
    "response = call_intrinsic(\"answerability\", new_chat_input)\n",
    "answerability_likelihood = json.loads(response.choices[0].message.content)[\n",
    "    \"answerability_likelihood\"\n",
    "]\n",
    "\n",
    "# 4. Answerable -> base model to generate\n",
    "DEFAULT_CANNED_RESPONSE = (\n",
    "    \"Sorry, but I am unable to answer this question from the documents retrieved.\"\n",
    ")\n",
    "if answerability_likelihood >= 0.5:\n",
    "    rag_completion = client.chat.completions.create(**new_chat_input.model_dump())\n",
    "else:\n",
    "    rag_completion = ChatCompletionResponse(\n",
    "        choices=[\n",
    "            ChatCompletionResponseChoice(\n",
    "                index=0, message=AssistantMessage(content=DEFAULT_CANNED_RESPONSE)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "display(Markdown(rag_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the responses we've seen so far, this response provides information that is both relevant to the user's intended question and grounded in documents retrieved from the  corpus.\n",
    "\n",
    "We can use the [LoRA Adapter for Citation Generation](https://huggingface.co/generative-computing/rag-intrinsics-lib/blob/main/citations/README.md\n",
    ") to explain exactly how this response is grounded in the documents that the rewritten user query retrieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Granite response.\n",
    "chat_input_citations = new_chat_input.model_copy(deep=True)\n",
    "chat_input_citations.model = base_model_name\n",
    "\n",
    "chat_completion = client.chat.completions.create(**chat_input_citations.model_dump())\n",
    "chat_input_citations.messages.append(chat_completion.choices[0].message)\n",
    "\n",
    "response = call_intrinsic(\"citations\", chat_input_citations)\n",
    "citations = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(chat_input_citations.messages[-1].content))\n",
    "print(\"Citations:\")\n",
    "print(json.dumps(citations, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the [LoRA Adapter for Hallucination Detection](\n",
    "    https://huggingface.co/generative-computing/rag-intrinsics-lib/blob/main/hallucination_detection/README.md\n",
    ") to further verify that each sentence of the assistant response is consistent with the information in the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Granite response.\n",
    "chat_input_hallucinations = new_chat_input.model_copy(deep=True)\n",
    "chat_input_hallucinations.model = base_model_name\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    **chat_input_hallucinations.model_dump()\n",
    ")\n",
    "chat_input_hallucinations.messages.append(chat_completion.choices[0].message)\n",
    "\n",
    "response = call_intrinsic(\"hallucination_detection\", chat_input_hallucinations)\n",
    "hallucinations = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(chat_input_hallucinations.messages[-1].content))\n",
    "print(\"Hallucination Checks:\")\n",
    "print(json.dumps(hallucinations, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Composite IO Processor class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite-common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
