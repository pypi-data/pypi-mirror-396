{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to granite-common and the Granite RAG Intrinsics Library\n",
    "\n",
    "This notebook provides a high-level introduction to the `granite-common` library and to the [Granite RAG Intrinsics Library](https://huggingface.co/generative-computing/rag-intrinsics-lib).\n",
    "\n",
    "You will need a hosted vLLM server to perform inference. See the library above for scripts to host the models on your own server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other notebooks in this directory provide a more in-depth treatment of concepts covered\n",
    "in this notebook:\n",
    "\n",
    "* Intro to `granite-common` and simple interface to call each intrinsic: [intrinsics_openai.ipynb](./intrinsics_openai.ipynb) and [intrinsics_transformers.ipynb](./intrinsics_transformers.ipynb) \n",
    "* Advanced end-to-end Retrieval Augmented Generation flows: [rag.ipynb](./rag.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go in this cell\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import granite_common\n",
    "from granite_common.base.types import (\n",
    "    ChatCompletion,\n",
    "    VLLMExtraBody,\n",
    ")\n",
    "\n",
    "from granite_common.retrievers.util import download_mtrag_embeddings\n",
    "from granite_common.retrievers import (\n",
    "    ElasticsearchRetriever,\n",
    "    InMemoryRetriever,\n",
    "    Retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "CORPUS_NAMES_MAPPINGS = {\n",
    "    \"banking\": \"mt-rag-banking-elser-512-100-20250205\",\n",
    "    \"clapnq\": \"mt-rag-clapnq-elser-512-100-20240503\",\n",
    "    \"fiqa\": \"mt-rag-fiqa-beir-elser-512-100-20240501\",\n",
    "    \"govt\": \"mt-rag-govt-elser-512-100-20240611\",\n",
    "    \"ibmcloud\": \"mt-rag-ibmcloud-elser-512-100-20240502\",\n",
    "    \"scifact\": \"mt-rag-scifact-beir-elser-512-100-20240501\",\n",
    "    \"telco\": \"mt-rag-telco-elser-512-100-20241210\",\n",
    "}\n",
    "\n",
    "DEFAULT_CANNED_RESPONSE = (\n",
    "    \"Sorry, but I am unable to answer this question from the documents retrieved.\"\n",
    ")\n",
    "\n",
    "target_model_name = \"granite-3.3-8b-instruct\"\n",
    "base_model_name = f\"ibm-granite/{target_model_name}\"\n",
    "\n",
    "# OpenAI compatible server - e.g. vLLM\n",
    "openai_base_url = \"http://localhost:55555/v1\"\n",
    "openai_api_key = \"rag_intrinsics_1234\"\n",
    "\n",
    "intrinsic_names = [\n",
    "    \"citations\",\n",
    "    \"query_rewrite\",\n",
    "    \"answerability\",\n",
    "    \"hallucination_detection\",\n",
    "    \"uncertainty\",\n",
    "]\n",
    "\n",
    "# retriever_name = \"elasticsearch\"\n",
    "retriever_name = \"embeddings\"\n",
    "corpus_name = \"govt\"\n",
    "\n",
    "if retriever_name == \"elasticsearch\":\n",
    "    # Elasticsearch retriever\n",
    "    elasticsearch_host = \"https://localhost:32765\"\n",
    "elif retriever_name == \"embeddings\":\n",
    "    # Embeddings retriever\n",
    "    temp_data_dir = \"../data/test_retrieval_temp\"\n",
    "    embeddings_data_file = pathlib.Path(temp_data_dir) / f\"{corpus_name}_embeds.parquet\"\n",
    "    embedding_model_name = \"multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "    # Download the indexed corpus if it hasn't already been downloaded.\n",
    "    # This notebook uses a subset of the government corpus from the MTRAG benchmark.\n",
    "    embeddings_location = f\"{temp_data_dir}/{corpus_name}_embeds.parquet\"\n",
    "    if not os.path.exists(embeddings_location):\n",
    "        download_mtrag_embeddings(\n",
    "            embedding_model_name, corpus_name, embeddings_location\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsics\n",
    "# Load config files and create objects\n",
    "\n",
    "intrinsic_rewriters = {}\n",
    "intrinsic_result_processors = {}\n",
    "for intrinsic_name in intrinsic_names:\n",
    "    io_yaml_file = granite_common.intrinsics.util.obtain_io_yaml(\n",
    "        intrinsic_name, target_model_name\n",
    "    )\n",
    "\n",
    "    intrinsic_rewriter = granite_common.IntrinsicsRewriter(config_file=io_yaml_file)\n",
    "    intrinsic_result_processor = granite_common.IntrinsicsResultProcessor(\n",
    "        config_file=io_yaml_file\n",
    "    )\n",
    "\n",
    "    intrinsic_rewriters[intrinsic_name] = intrinsic_rewriter\n",
    "    intrinsic_result_processors[intrinsic_name] = intrinsic_result_processor\n",
    "\n",
    "# Connect to the inference server\n",
    "client = openai.OpenAI(base_url=openai_base_url, api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "\n",
    "if retriever_name == \"elasticsearch\":\n",
    "    # Connect to the Elasticsearch server.\n",
    "    # Due to the setup, we have to open a retriever connection for each corpus.\n",
    "    retrievers = {}\n",
    "    for corpus_name, actual_corpus_name in CORPUS_NAMES_MAPPINGS.items():\n",
    "        retriever = ElasticsearchRetriever(\n",
    "            corpus_name=actual_corpus_name,\n",
    "            host=elasticsearch_host,\n",
    "            verify_certs=False,\n",
    "            ssl_show_warn=False,\n",
    "        )\n",
    "        retrievers[corpus_name] = retriever\n",
    "elif retriever_name == \"embeddings\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "\n",
    "def call_intrinsic(\n",
    "    intrinsic_name: str,\n",
    "    chat_completion_request: dict,\n",
    "    **kwargs,\n",
    ") -> openai.types.chat.ChatCompletion:\n",
    "    \"\"\"\n",
    "    Call an intrinsic with OpenAI Python API objects on input and output.\n",
    "\n",
    "    :param intrinsic_name: Name of intrinsic to invoke\n",
    "    :param chat_completion_request: Chat completion request to make; can be dict or\n",
    "        OpenAI dataclass\n",
    "    :param kwargs: Optional named argument(s) for intrinsic\n",
    "\n",
    "    :returns: OpenAI Python API chat completion containing processed intrinsic outputs\n",
    "    \"\"\"\n",
    "    # Some intrinsics modify the chat object.\n",
    "    _chat_completion_request = chat_completion_request.model_copy(deep=True)\n",
    "\n",
    "    rewriter = intrinsic_rewriters[intrinsic_name]\n",
    "    result_processor = intrinsic_result_processors[intrinsic_name]\n",
    "    rewritten_request = rewriter.transform(_chat_completion_request, **kwargs)\n",
    "\n",
    "    # Set model name manually for now, because vLLM does not maintain any kind of\n",
    "    # metadata that would allow us to determine the right model name.\n",
    "    rewritten_request.model = intrinsic_name\n",
    "\n",
    "    response = client.chat.completions.create(**rewritten_request.model_dump())\n",
    "    # return response\n",
    "    transformed_response = result_processor.transform(response, rewritten_request)\n",
    "\n",
    "    # Convert to same type as OpenAI API\n",
    "    return openai.types.chat.ChatCompletion.model_validate(\n",
    "        transformed_response.model_dump()\n",
    "    )\n",
    "\n",
    "\n",
    "def retrieve_snippets(retriever: Retriever, query: str, top_k: int = 3):\n",
    "    return retriever.retrieve(query, top_k=top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## granite-common\n",
    "\n",
    "TODO: Update description of `granite-common`\n",
    "\n",
    "The `granite-common` library provides input and output processing for large language models.\n",
    "In this context, *input and output processing* refers to the steps that happen \n",
    "immediately before and after low-level model inference. These steps include:\n",
    "\n",
    "* **Input processing:** Translating application data structures such as messages and \n",
    "  documents into a string prompt for a particular model\n",
    "* **Output processing:** Parsing the raw string output of a language model into \n",
    "  structured application data\n",
    "* **Constrained decoding:** Constraining the raw string output of an LLM to ensure that\n",
    "  the model's output will always parse into structured application data\n",
    "* **Inference-time scaling:** Extracting a higher-quality answer from an LLM by \n",
    "  combining the results of multiple inference calls.\n",
    "\n",
    "\n",
    "`granite-common` includes three main types of entry points:\n",
    "* **Backend connectors** connect the `granite-io` library to different model inference \n",
    "  engines and vector databases.\n",
    "  The other components of `granite-io` use these adapters to invoke model inference with\n",
    "  exactly the right low-level parameters for each model and inference layer.\n",
    "* **InputOutputProcessors** provide input and output processing for specific models.\n",
    "  An InputOutputProcessor exposes a \"chat completions\" interface, where the input is the\n",
    "  structured representation of a conversation and the output is the next turn of the\n",
    "  conversation.\n",
    "  For some models, such as [IBM Granite 3.3](https://huggingface.co/collections/ibm-granite/granite-33-language-models-67f65d0cca24bcbd1d3a08e3), we also provide\n",
    "  separate APIs that only perform input processing or output processing.\n",
    "* **RequestProcessors** rewrite chat completion requests in various ways, such as \n",
    "  rewording messages, attaching RAG documents, or filtering documents. You can chain\n",
    "  one or more RequestProcessors with an InputOutputProcessor to implement a custom \n",
    "  inference workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat completions API in `granite-common` runs low-level inference on the target\n",
    "model, passing in raw string prompts and inference paramters and receiving back raw \n",
    "string results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.completions.create(\n",
    "    prompt=\"Complete this sequence: 2, 3, 5, 7, 11, 13, \",\n",
    "    model=base_model_name,\n",
    "    temperature=0.0,\n",
    "    max_tokens=12,\n",
    ")\n",
    "\n",
    "print(completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most users don't interact with the low-level backend API directly. The recommended way\n",
    "to use `granite-common` is via the InputOutputProcessor APIs, which convert high-level \n",
    "request into the specific combination of inference paramters that the model needs,\n",
    "run inference, and then convert the model's raw output into something that an \n",
    "application can use directly.\n",
    "\n",
    "Let's create an example chat completion request so we can show how the high-level \n",
    "InputOutputProcessor API works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input = ChatCompletion.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the City of Dublin, CA help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hi there. Can you answer questions about fences?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Absolutely, I can provide general information about \"\n",
    "                \"fences in Dublin, CA.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Great. I want to add one in my front yard. Do I need a \"\n",
    "                \"permit?\",\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def print_chat(c):\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"\\n\".join([f\"**{m.role.capitalize()}:** {m.content}\\n\" for m in c.messages])\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print_chat(chat_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chat completion request models a scenario where the user is talking to the \n",
    "automated help desk for the City of Dublin, CA and has just asked a question about \n",
    "permitting for installing fences. Running this chat completion request should produce\n",
    "an assistant response to this question.\n",
    "\n",
    "If we pass our chat completion (`chat_input`) to a `granite-common` InputOutputProcessor's \n",
    "`create_chat_completion()` method, the InputOutputProcessor will create a string prompt\n",
    "for the model, set up model-specific generation parameters, invoke model inference, and\n",
    "parse the model's raw output into a structured message.\n",
    "\n",
    "Here we create an InputOutputProcessor for the [IBM Granite 3.3](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-instruct) model and point that InputOutputProcessor at the backend we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the example through Granite to get an answer.\n",
    "chat_input.model = base_model_name\n",
    "non_rag_completion = client.chat.completions.create(**chat_input.model_dump())\n",
    "\n",
    "display(Markdown(non_rag_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's response here is generic and vague, because the model's training data does \n",
    "not cover obscure zoning ordinances of small cities in northern California.\n",
    "\n",
    "We can use the \n",
    "[Uncertainty LoRA](\n",
    "    https://huggingface.co/generative-computing/core-intrinsics-lib/blob/main/uncertainty/README.md)\n",
    "model to flag cases such as this one that are not covered by the base model's \n",
    "training data. \n",
    "\n",
    "This model comes packaged as a LoRA adapter on top of Granite 3.3. To run the model, we\n",
    "create an instance of `CertaintyIOProcessor` -- the `granite-io` InputOutputProcessor\n",
    "for this model -- and point this InputOutputProcessor at a Backend that we have\n",
    "connected to the model's LoRA adapter. Then we can pass the same chat completion request\n",
    "into the model to compute a certainty score from 0 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_intrinsic(\"uncertainty\", chat_input)\n",
    "certainty_score = round(json.loads(response.choices[0].message.content)[\"certainty\"], 2)\n",
    "\n",
    "print(f\"Certainty score is {certainty_score} out of 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low certainty score indicates that the model's training data does not align closely\n",
    "with this question.\n",
    "\n",
    "To answer this question properly, we need to provide the model with domain-specific \n",
    "information. One of the most popular ways to add domain-specific information to an LLM\n",
    "is to use the Retrieval-Augmented Generation (RAG) pattern. RAG involves retrieving\n",
    "snippets of text from a collection of documents and adding those snippets to the model's\n",
    "prompt.\n",
    "\n",
    "\n",
    "In this case, the relevant information can be found in the Government \n",
    "corpus of the [MTRAG multi-turn RAG benchmark](https://github.com/IBM/mt-rag-benchmark).\n",
    "Similar to its connectors for inference backends, `granite-io` has adapters for \n",
    "RAG retrieval backends.\n",
    "\n",
    "Let's spin up a connection in-memory vector database, using embeddings that we've \n",
    "precomputed offline from the MTRAG Government corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retriever_name == \"elasticsearch\":\n",
    "    retriever = retrievers[corpus_name]\n",
    "elif retriever_name == \"embeddings\":\n",
    "    retriever = InMemoryRetriever(embeddings_data_file, embedding_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`granite-io` also includes a RequestProcessor that performs the retrieval phase of\n",
    "RAG. This class, called `RetrievalRequestProcessor`, takes as input a chat completion\n",
    "request. The RequestProcessor uses the text of the last user turn to query a `Retriever`\n",
    "instance and fetch document snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval_request_proc = RetrievalRequestProcessor(retriever, top_k=3)\n",
    "# chat_input_with_docs = retrieval_request_proc.process(chat_input)[0]\n",
    "# chat_input_with_docs.model_dump()\n",
    "\n",
    "# The database fetches document snippets that match a given query.\n",
    "# For example, the user's question in the conversation above:\n",
    "query = chat_input.messages[-1].content\n",
    "print(f\"Query is: '{query}'\")\n",
    "print(\"Matching document snippets:\")\n",
    "documents = retrieve_snippets(retriever, query, top_k=3)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the last user turn in this conversation is:\n",
    "> **User:** Great. I want to add one in my front yard. Do I need a permit?\n",
    "\n",
    "This text is missing key details for retrieving relevant documents: What does the \n",
    "user want to add to their front yard, and what city's municipal code applies to this\n",
    "yard? As a result, the retrieved documents aren't actually relevant to the user's \n",
    "question.\n",
    "\n",
    "The [LoRA Adapter for Answerability Classification](\n",
    "    https://huggingface.co/generative-computing/rag-intrinsics-lib/blob/main/answerability/README.md)\n",
    "provides a robust way to detect this kind of problem. Here's what happens if we \n",
    "run the chat completion request with irrelevant document snippets through the \n",
    "answerability model, using the\n",
    "`granite_common` processor for the model to handle input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval step from before...\n",
    "chat_input_with_docs = chat_input.model_copy(deep=True)\n",
    "chat_input_with_docs.extra_body = VLLMExtraBody(documents=documents)\n",
    "chat_input_with_docs.model_dump()\n",
    "\n",
    "# ...followed by an answerability check\n",
    "response = call_intrinsic(\"answerability\", chat_input_with_docs)\n",
    "answerability_likelihood = json.loads(response.choices[0].message.content)[\n",
    "    \"answerability_likelihood\"\n",
    "]\n",
    "answerability_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use use the [LoRA Adapter for Query Rewrite](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib/blob/main/query_rewrite_lora/README.md) to rewrite\n",
    "the last user turn into a string that is more useful for retrieving document snippets.\n",
    "`granite-io` includes an InputOutputProcessor for running this model.\n",
    "Here's how to use this InputOutputProcessor to apply this model to our example \n",
    "conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = call_intrinsic(\"query_rewrite\", chat_input_with_docs)\n",
    "rewritten_question = json.loads(response.choices[0].message.content)[\n",
    "    \"rewritten_question\"\n",
    "]\n",
    "rewritten_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query rewrite model turns the last user turn in this conversation from:\n",
    "> **User:** Great. I want to add one in my front yard. Do I need a permit?\n",
    "\n",
    "...to a version of the same question that includes vital additional context:\n",
    "> **User:** Do I need a permit to add a fence in my front yard in Dublin, CA?\n",
    "\n",
    "This more specific query should allow the retriever to fetch better document snippets.\n",
    "\n",
    "The following code snippet uses `granite-io` APIs to rewrite the user query, then\n",
    "fetch relevant document snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo initialization so this cell can run independently of previous cells\n",
    "\n",
    "# Rewrite the last user turn into something more suitable for retrieval.\n",
    "response = call_intrinsic(\"query_rewrite\", chat_input)\n",
    "rewritten_question = json.loads(response.choices[0].message.content)[\n",
    "    \"rewritten_question\"\n",
    "]\n",
    "\n",
    "# Retrieve document snippets based on the rewritten turn and attach them to the chat\n",
    "# completion request.\n",
    "query = rewritten_question\n",
    "documents = retrieve_snippets(retriever, query, top_k=3)\n",
    "\n",
    "chat_input_with_docs = chat_input.model_copy(deep=True)\n",
    "chat_input_with_docs.extra_body = VLLMExtraBody(documents=documents)\n",
    "\n",
    "chat_input_with_docs.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attaching relevant information causes the model to respond with a more specific and \n",
    "detailed answer. Here's the result that we get when we pass the rewritten chat \n",
    "completion request to the InputOutputProcessor for Granite 3.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_completion = client.chat.completions.create(**chat_input_with_docs.model_dump())\n",
    "display(Markdown(rag_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer contains specific details about permits for building fences in Dublin, CA.\n",
    "These facts should grounded in documents retrieved from the corpus. We would like\n",
    "to be able to prove that the model used the data from the corpus and did not \n",
    "hallucinate a fictitious building code.\n",
    "\n",
    "We can use the [LoRA Adapter for Citation Generation](\n",
    "    https://huggingface.co/generative-computing/rag-intrinsics-lib/blob/main/citations/README.md\n",
    ") to explain exactly how this response is grounded in the documents that the rewritten\n",
    "user query retrieves. As with the other models we've shown so far, `granite-common` includes\n",
    "an InputOutputProcessor for this model. We can use this InputOutputProcessor to add\n",
    "citations to the assistant response from the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Granite response.\n",
    "chat_input_citations = chat_input_with_docs.model_copy(deep=True)\n",
    "chat_input_citations.messages.append(rag_completion.choices[0].message)\n",
    "\n",
    "response = call_intrinsic(\"citations\", chat_input_citations)\n",
    "citations = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(chat_input_citations.messages[-1].content))\n",
    "print(\"Citations:\")\n",
    "print(json.dumps(citations, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Update CitationsWidget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citations_io_proc = CitationsIOProcessor(citations_lora_backend)\n",
    "\n",
    "# # Add the assistant response to the original chat completion request\n",
    "# input_with_next_message = input.with_next_message(rag_result.results[0].next_message)\n",
    "\n",
    "# # Augment this response with citations to the RAG document snippets\n",
    "# results_with_citations = citations_io_proc.create_chat_completion(\n",
    "#     input_with_next_message\n",
    "# )\n",
    "# CitationsWidget().show(input_with_next_message, results_with_citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the [LoRA Adapter for Hallucination Detection in RAG outputs](\n",
    "    https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib/blob/main/hallucination_detection_lora/README.md\n",
    ") to check whether each sentence of the assistant response is consistent with the\n",
    "information in the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Granite response.\n",
    "chat_input_hallucinations = chat_input_with_docs.model_copy(deep=True)\n",
    "chat_input_hallucinations.messages.append(rag_completion.choices[0].message)\n",
    "\n",
    "response = call_intrinsic(\"hallucination_detection\", chat_input_hallucinations)\n",
    "hallucinations = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(chat_input_hallucinations.messages[-1].content))\n",
    "print(\"Hallucination Checks:\")\n",
    "print(json.dumps(hallucinations, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `granite-common` library also allows developers to create their own custom \n",
    "InputOutputProcessors. For example, here's an InputOutputProcessor that rolls up the rewrite, retrieval, and citations processing steps from this notebook into a single custom `chat_completion()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from granite_common.base.types import (\n",
    "    ChatCompletion,\n",
    "    ChatCompletionResponse,\n",
    "    VLLMExtraBody,\n",
    ")\n",
    "from granite_common.retrievers.elasticsearch import ElasticsearchRetriever\n",
    "\n",
    "\n",
    "class MyRAGIOProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: openai.OpenAI,\n",
    "        retriever: dict[str, ElasticsearchRetriever],\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def call_intrinsic(\n",
    "        self,\n",
    "        intrinsic_name: str,\n",
    "        chat_completion_request: dict,\n",
    "        **kwargs,\n",
    "    ) -> openai.types.chat.ChatCompletion:\n",
    "        \"\"\"\n",
    "        Call an intrinsic with OpenAI Python API objects on input and output.\n",
    "\n",
    "        :param intrinsic_name: Name of intrinsic to invoke\n",
    "        :param chat_completion_request: Chat completion request to make;\n",
    "            can be dict or OpenAI dataclass\n",
    "        :param kwargs: Optional named argument(s) for intrinsic\n",
    "\n",
    "        :returns: OpenAI Python API chat completion containing processed\n",
    "            intrinsic outputs\n",
    "        \"\"\"\n",
    "        # Some intrinsics modify the chat object.\n",
    "        _chat_completion_request = chat_completion_request.model_copy(deep=True)\n",
    "\n",
    "        rewriter = intrinsic_rewriters[intrinsic_name]\n",
    "        result_processor = intrinsic_result_processors[intrinsic_name]\n",
    "        rewritten_request = rewriter.transform(_chat_completion_request, **kwargs)\n",
    "\n",
    "        # Set model name manually for now, because vLLM does not maintain any kind of\n",
    "        # metadata that would allow us to determine the right model name.\n",
    "        rewritten_request.model = intrinsic_name\n",
    "\n",
    "        response = self.client.chat.completions.create(**rewritten_request.model_dump())\n",
    "        # return response\n",
    "        transformed_response = result_processor.transform(response, rewritten_request)\n",
    "\n",
    "        # Convert to same type as OpenAI API\n",
    "        return openai.types.chat.ChatCompletion.model_validate(\n",
    "            transformed_response.model_dump()\n",
    "        )\n",
    "\n",
    "    def retrieve_snippets(\n",
    "        self, retriever: Retriever, query: str, top_k: int = 3\n",
    "    ) -> dict:\n",
    "        return retriever.retrieve(query, top_k=top_k)\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        chat_input: ChatCompletion,\n",
    "    ) -> ChatCompletionResponse:\n",
    "        \"\"\"Placeholder for a proper IO processor base class.\"\"\"\n",
    "\n",
    "        chat_input_with_docs = chat_input.model_copy(deep=True)\n",
    "\n",
    "        # Rewrite the last user turn for retrieval\n",
    "        response = self.call_intrinsic(\"query_rewrite\", chat_input_with_docs)\n",
    "        rewritten_question = json.loads(response.choices[0].message.content)[\n",
    "            \"rewritten_question\"\n",
    "        ]\n",
    "\n",
    "        # Retrieve documents with the rewritten last turn\n",
    "        query = rewritten_question\n",
    "        documents = self.retrieve_snippets(self.retriever, query, top_k=3)\n",
    "        chat_input_with_docs.extra_body = VLLMExtraBody(documents=documents)\n",
    "\n",
    "        # Generate a response\n",
    "        rag_completion = self.client.chat.completions.create(\n",
    "            **chat_input_with_docs.model_dump()\n",
    "        )\n",
    "        chat_response = chat_input_with_docs.model_copy(deep=True)\n",
    "        chat_response.messages.append(rag_completion.choices[0].message)\n",
    "\n",
    "        # Generate citations\n",
    "        chat_input_citations = chat_response.model_copy(deep=True)\n",
    "\n",
    "        response = self.call_intrinsic(\"citations\", chat_input_citations)\n",
    "        citations = json.loads(response.choices[0].message.content)\n",
    "\n",
    "        return chat_response, citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap all of the functionality we've shown so far in a single class that \n",
    "inherits from the `InputOutputProcessor` interface in `granite-common`. Packaging things\n",
    "this way lets applications treat this multi-step flow as if it was a single chat \n",
    "completion request to a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input = ChatCompletion.model_validate(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Welcome to the City of Dublin, CA help desk.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Hi there. Can you answer questions about fences?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Absolutely, I can provide general information about \"\n",
    "                \"fences in Dublin, CA.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Great. I want to add one in my front yard. Do I need a \"\n",
    "                \"permit?\",\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096,\n",
    "        \"model\": base_model_name,\n",
    "    }\n",
    ")\n",
    "\n",
    "rag_io_proc = MyRAGIOProcessor(\n",
    "    client=client,\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "rag_completion, citations = rag_io_proc.chat_completion(chat_input)\n",
    "\n",
    "print(\"Assistant response:\")\n",
    "display(Markdown(rag_completion.messages[-1].content))\n",
    "print(\"Citations:\")\n",
    "print(json.dumps(citations, indent=2))\n",
    "# CitationsWidget().show(input_with_next_message, rag_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "granite-common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
