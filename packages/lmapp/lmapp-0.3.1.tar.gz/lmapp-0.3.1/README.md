# LMAPP

> **Local LLM CLI** â€“ AI everywhere. Easy, simple, and undeniable.  
> Online or offline. The future is yours to command.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![PyPI](https://img.shields.io/pypi/v/lmapp.svg)](https://pypi.org/project/lmapp/) [![CI](https://github.com/nabaznyl/lmapp/actions/workflows/tests.yml/badge.svg)](https://github.com/nabaznyl/lmapp/actions/workflows/tests.yml) [![codecov](https://codecov.io/gh/nabaznyl/lmapp/branch/mother/graph/badge.svg)](https://codecov.io/gh/nabaznyl/lmapp) [![Status](https://img.shields.io/badge/Status-Production%20Ready-blue.svg)]()

**v0.3.0-beta** - Production Ready. Fully Featured. Free.

See [**Demo & Features**](DEMO.md) for examples and use cases.

---

## ðŸš€ Quick Start

Full installation and setup: see [QUICKSTART.md](QUICKSTART.md).
Customize your AI's behavior: see [Roles & Workflows Guide](docs/ROLE_WORKFLOW_QUICKSTART.md).

Everyday commands:
```bash
lmapp chat          # Start chatting locally
lmapp status        # Check backend/model status
lmapp config show   # View current configuration
```

More examples and demos: see [DEMO.md](DEMO.md).

---

## ðŸŽ¯ Features

### ðŸ’¬ Chat
```bash
$ lmapp chat --model mistral
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        Chat with Mistral (Local)           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You: Explain quantum computing in simple terms

AI: Quantum computers use quantum bits (qubits) instead of regular bits.
While regular bits are 0 or 1, qubits can be both at once (superposition).
This lets them solve certain problems exponentially faster...

You: What are the use cases?

AI: Key use cases include:
  â€¢ Drug discovery (molecular simulation)
  â€¢ Finance (portfolio optimization)
  â€¢ Cryptography (breaking encryption)
  â€¢ Machine learning (optimization)
```

### ðŸ” RAG (Semantic Search)
```bash
$ lmapp rag index ~/my_docs
ðŸ“ Indexing documents...
âœ“ Processed: README.md (1,234 tokens)
âœ“ Processed: GUIDE.pdf (5,678 tokens)
âœ“ Processed: NOTES.txt (892 tokens)
âœ“ Index created: 7,804 tokens in 12 documents

$ lmapp rag search "how to optimize python code"
ðŸ“Š Search Results (3 matches):

1. GUIDE.pdf - Line 45 (score: 0.92)
   "Optimization techniques include: list comprehensions,
    caching, and using built-in functions instead of loops"

2. NOTES.txt - Line 12 (score: 0.88)
   "Profile code with cProfile before optimizing"

3. README.md - Line 89 (score: 0.81)
   "Performance tips for production code"

$ lmapp chat --with-context
You: Summarize the best Python optimization tips from my docs

AI: Based on your documents, here are the key optimization tips:
  1. Use list comprehensions instead of loops
  2. Profile with cProfile before optimizing
  3. Leverage built-in functions (map, filter, etc.)
  4. Implement caching for expensive operations
```

### ðŸ“¦ Batch Processing
```bash
$ lmapp batch create inputs.json
Processing 5 queries in batch...
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% (5/5)

Job created: batch_20250211_143022
Estimated time: 45 seconds

$ lmapp batch results batch_20250211_143022 --json
{
  "job_id": "batch_20250211_143022",
  "status": "completed",
  "results": [
    {"input": "Explain AI", "output": "AI is..."},
    {"input": "What is ML?", "output": "Machine learning..."},
    ...
  ],
  "completed_at": "2025-02-11T14:30:47Z"
}
```

### ðŸ”Œ Plugins
```bash
$ lmapp plugin list
Available Plugins:
  âœ“ translator     - Real-time translation (8 languages)
  âœ“ summarizer     - Extract key points from long text
  âœ“ code-reviewer  - Analyze code and suggest improvements
  âœ“ sql-generator  - Write SQL queries from descriptions
  âœ“ regex-helper   - Build and test regex patterns
  âœ“ json-validator - Validate and format JSON
  âœ“ git-helper     - Explain git commands and operations
  âœ“ api-tester     - Test REST APIs interactively

$ lmapp plugin install translator
Installing translator plugin...
âœ“ Downloaded (245 KB)
âœ“ Installed successfully
Ready to use: lmapp translate --help

$ lmapp translate --text "Hello World" --to spanish
Translation (Spanish):
"Â¡Hola Mundo!"
```

### âš™ï¸ Configuration
```bash
$ lmapp config show
Current Configuration:
  Model: mistral (7B)
  Temperature: 0.7
  Max Tokens: 2048
  Context Size: 4096
  System Prompt: You are a helpful AI assistant

$ lmapp config set temperature 0.3
âœ“ Configuration updated

$ lmapp config --set-prompt
Enter your custom system prompt:
> You are a Python expert. Help with code, explain concepts clearly.
âœ“ System prompt saved

$ lmapp status
Status Report:
  âœ“ Backend: Ollama (running)
  âœ“ Model: mistral (7.4B)
  âœ“ Memory: 6.2 GB / 16 GB
  âœ“ Performance: 45 tokens/sec
```

---

## ðŸ’¡ Who Is This For?

### Perfect Fit
- **Developers** - Code explanations, debugging, documentation, CLI workflows
- **Students & Researchers** - Study partner, research assistance, offline-first
- **SysAdmins** - Command lookups, automation scripts, system analysis
- **Professionals** - Writing, analysis, research, note-taking
- **Privacy-Conscious Users** - Want AI without cloud dependencies
- **Anyone** who values control over convenience

---

---

## ðŸ“– Basic Usage

```bash
# Start chat
lmapp chat

# Use specific model
lmapp chat --model mistral

# Check status
lmapp status

# View configuration
lmapp config show
```

**Supported Backends:** Ollama, llamafile (auto-detected). Extensible architecture supports custom backends.

See [QUICKSTART.md](QUICKSTART.md) for complete usage guide.

---

## âœ… Quality & Features

- ðŸ§ª **587 tests** (100% coverage)
- ðŸ”’ **100% private** (no cloud, no tracking)
- âš¡ **Fast & lightweight** (<200ms startup)
- ðŸ”Œ **8 production plugins**
- ðŸ” **RAG system** (semantic search)
- ðŸ“¦ **Batch processing**
- ðŸ’¾ **Session persistence**
- ðŸŒ **Web UI** (optional)

---

## ðŸ” Privacy & Security

- **100% Local** - Everything runs on your device
- **No Cloud** - No internet after setup
- **No Telemetry** - Zero tracking
- **Open Source** - MIT licensed
- **Your Data** - You own it all

---

## ðŸ—ºï¸ Roadmap

**v0.3.0** (Current) - Production ready  
**v0.4.0+** - Mobile/desktop apps, team features, enterprise tier

---

## ðŸ¤ Contributing

Help wanted! See [Contributing Guide](CONTRIBUTING.md) for code contributions, bug reports, or feature ideas.

---

All contributions welcome: bug fixes, features, documentation, tests, and ideas.

---

## ðŸ’¬ Support

- **Found a bug?** Open an [Issue](https://github.com/nabaznyl/lmapp/issues)
- **Questions?** See [Troubleshooting Guide](TROUBLESHOOTING.md)
- **Discussions?** Use [GitHub Discussions](https://github.com/nabaznyl/lmapp/discussions)

---

## âš™ï¸ Troubleshooting

| Issue | Solution |
|-------|----------|
| `command not found` | Add `~/.local/bin` to `$PATH` or use `pipx install lmapp` |
| `ModuleNotFoundError` | Reinstall: `pip install --upgrade lmapp` |
| Debian/Ubuntu issues | Use `pipx install lmapp` instead of `pip` |

See [Troubleshooting Guide](TROUBLESHOOTING.md) for more.

---

## â“ FAQ

**Q: How do I install?**  
`pip install lmapp`

**Q: How do I update?**  
`pip install --upgrade lmapp`

**Q: Can I use commercially?**  
Yes! MIT License allows it. See [LICENSE](LICENSE).

**Q: Does it collect data?**  
No. 100% local, no telemetry.

More questions? See [Troubleshooting Guide](TROUBLESHOOTING.md).

---

## ðŸ“š Documentation

- [Security Policy](./SECURITY.md)
- [License](LICENSE)
- [Changelog](CHANGELOG.md)

---

## ðŸ“„ License

MIT License - [See LICENSE file](LICENSE)

This means:
- âœ… Use commercially
- âœ… Modify and distribute
- âœ… Include in closed-source projects
- âœ… Just include the license

### Third-Party Licenses
- **Ollama**: MIT License
- **llamafile**: Apache 2.0 License
- **Pydantic**: MIT License
- **Pytest**: MIT License
- **AI Models**: Various (see model documentation)

---

## ðŸ™ Built With

- [Ollama](https://ollama.ai/) - LLM management platform
- [llamafile](https://github.com/Mozilla-Ocho/llamafile) - Portable LLM runtime
- [Pydantic](https://docs.pydantic.dev/) - Data validation
- [Pytest](https://pytest.org/) - Testing framework
- Meta, Mistral, and other amazing AI model creators

---

## â­ Show Your Support

If lmapp helps you, please:
- â­ Star this repository
- ðŸ› Report bugs and suggest features
- ðŸ“¢ Share with friends and colleagues
- ðŸ¤ Contribute improvements
- ðŸ“ Share your use cases

---

## ðŸ“ž Get Started Now

```bash
pip install lmapp
lmapp chat
```

---

## ðŸ“– Documentation Map

| Document | Purpose |
|----------|---------|
| **[QUICKSTART.md](QUICKSTART.md)** | 5-minute setup guide â­ Start here |
| **[docs/installation.md](docs/installation.md)** | Installation methods for all platforms |
| **[docs/CONFIGURATION.md](docs/CONFIGURATION.md)** | Configuration, environment, and settings |
| **[docs/development.md](docs/development.md)** | Developer workflow and tips |
| **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** | Solutions for common issues |
| **[SECURITY.md](SECURITY.md)** | Security policy and vulnerability reporting |
| **[CHANGELOG.md](CHANGELOG.md)** | Release history |
| **[CONTRIBUTING.md](CONTRIBUTING.md)** | Contribution guidelines |
| **[CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md)** | Community standards |
| **[LICENSE](LICENSE)** | License terms |
| **[DEMO.md](DEMO.md)** | Live examples and feature tour |
| **[API_REFERENCE.md](API_REFERENCE.md)** | Lightweight CLI + HTTP API reference |

Additional references:
- **[docs/ERROR_DATABASE.md](docs/ERROR_DATABASE.md)** - Known errors and fixes

---

**Welcome to the future of local AI.** ðŸš€
