{
  "description": "Real-world ETL job with multiple reads/writes",
  "priority": "HIGH",
  "tables_read": [
    {"name": "warehouse.dim_users", "pattern": "spark.read.table"},
    {"name": "s3://data-lake/delta/fact_events", "pattern": "spark.read.format.load", "source_type": "delta"},
    {"name": "s3://reference-data/geo/countries.parquet", "pattern": "spark.read.parquet", "source_type": "parquet"},
    {"name": "marketing.campaigns", "pattern": "spark.sql.FROM"}
  ],
  "tables_written": [
    {"name": "warehouse.daily_user_metrics", "pattern": "write.saveAsTable"},
    {"name": "s3://ml-features/user_segments/", "pattern": "write.save", "source_type": "delta"},
    {"name": "reporting.segment_summary", "pattern": "write.insertInto"}
  ],
  "notes": "Env vars detected by Python parser. f-string parquet write is acceptable miss."
}
