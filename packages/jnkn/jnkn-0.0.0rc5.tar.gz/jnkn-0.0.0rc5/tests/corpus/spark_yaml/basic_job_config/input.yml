# Basic Spark job configuration
job_name: daily_user_metrics
schedule: "0 6 * * *"
entry_point: jobs/daily_user_metrics.py

spark_config:
  spark.executor.memory: 8g
  spark.executor.cores: 4
  spark.sql.shuffle.partitions: 200

dependencies:
  - upstream_data_ingestion
  - dim_users_refresh

environment:
  DATABASE_HOST: ${PROD_DATABASE_HOST}
  DATABASE_PORT: "5432"
  OUTPUT_PATH: s3://data-lake/output/

inputs:
  - warehouse.dim_users
  - warehouse.fact_events

outputs:
  - warehouse.daily_user_metrics
  - reporting.summary
