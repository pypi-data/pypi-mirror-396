import ast
from transformers import AutoModelForMaskedLM, AutoModelForCausalLM, AutoTokenizer
from transformers import PreTrainedModel, PreTrainedTokenizerBase
from typing import Tuple, NamedTuple, List, Any
from contextlib import contextmanager
import torch.nn.functional as F
from tqdm import tqdm
import pandas as pd
import itertools
import logging
import torch
import math

from grewtse.utils.validation import load_and_validate_mp_dataset
from grewtse.evaluators.metrics import (
    compute_normalised_surprisal_difference,
    compute_average_surprisal_difference,
    compute_entropy_based_certainty,
    compute_accuracy,
    compute_surprisal,
    compute_mean,
)

EVAL_TEMPLATE = {
    "sentence_id": None,
    "match_id": None,
    "original_text": None,
    "prompt_text": None,
    "form_grammatical": None,
    "p_grammatical": None,
    "I_grammatical": None,
    "form_ungrammatical": None,
    "p_ungrammatical": None,
    "I_ungrammatical": None,
    "certainty": None,
}

# --- Helper function ---
def _init_row_results(row):
    row_results = EVAL_TEMPLATE.copy()
    row_results.update(row._asdict())
    return row_results


class TooManyMasksException(Exception):
    def __init__(self, message: str):
        self.message = message
        super().__init__(f"TMM Exception: {message}")


class Prediction(NamedTuple):
    token: str
    prob: float
    surprisal: float


class GrewTSEvaluator:
    """
    An evaluation class designed specifically for rapid syntactic evaluation of models available on the Hugging Face platform.
    """

    def __init__(self):
        self.evaluator = Evaluator()
        self.evaluation_dataset = None

    def evaluate_model(
        self,
        mp_dataset: pd.DataFrame,
        model_repo: str,
        model_type: str,  # can be 'encoder' or 'decoder'
        entropy_topk: int = 100,
        row_limit: int = None,
    ) -> pd.DataFrame:
        """
        Function for carrying out Targeted Syntactic Evaluation for either encoder or decoder models.

        :param mp_dataset: A DataFrame containing the Minimal-Pair Dataset generated by Grew-TSE.
        :param model_repo: the Hugging Face model repository link.
        :param model_type: choose either 'encoder' or 'decoder'.
        :param entropy_topk: how many probabilities are taken into account for calculating the model uncertainty.
        :param row_limit: place a limit on the number of samples / rows evaluated in the Minimal-Pair Dataset.
        :return: A DataFrame containing the evaluation results for each sample.
        """

        # --- Prepare dataset ---
        mp_dataset_iter = mp_dataset.itertuples()
        if row_limit:
            mp_dataset_iter = itertools.islice(mp_dataset_iter, row_limit)
        n = len(mp_dataset) if not row_limit else row_limit

        # --- Load model & tokenizer ---
        is_encoder = model_type == "encoder"
        model, tokenizer = self.evaluator.setup_parameters(model_repo, is_encoder)
        results = []

        with self.evaluator.load_model(model_repo, is_encoder):
            # --- Evaluate each row ---
            for row in tqdm(mp_dataset_iter, total=n, desc="Evaluating"):
                row_results = _init_row_results(row)

                try:
                    if is_encoder:
                        self._evaluate_encoder_row(row, row_results)
                    else:
                        self._evaluate_decoder_row(row, row_results)

                except TooManyMasksException:
                    logging.error(f"Too many masks in {row.sentence_id}")
                    continue
                except Exception as e:
                    raise RuntimeError(f"Model/tokeniser issue: {e}") from e

                # --- Entropy ---
                entropy, entropy_norm = self.evaluator.get_entropy(entropy_topk, True)
                row_results["entropy"] = entropy
                row_results["entropy_norm"] = entropy_norm

                results.append(row_results)

        results_df = pd.DataFrame(results, columns=EVAL_TEMPLATE.keys())
        self.evaluation_dataset = results_df
        return results_df

    def evaluate_from_filepath(
        self,
        mp_dataset_filepath: str,
        model_repo: str,
        model_type: str,
        entropy_topk: int = 100,
        row_limit: int = None,
    ) -> pd.DataFrame:
        """
        Carries out model evaluation using a Minimal-Pair Dataset generated by Grew-TSE that is provided as a filepath.

        :param mp_dataset_filepath: the filepath pointing to the Minimal-Pair Dataset. Should be a .csv format.
        :param model_repo: the Hugging Face model repository link.
        :param model_type: choose either 'encoder' or 'decoder'.
        :param entropy_topk: how many probabilities are taken into account for calculating the model uncertainty.
        :param row_limit: place a limit on the number of samples / rows evaluated in the Minimal-Pair Dataset.
        :return: A DataFrame containing the evaluation results for each sample.
        """
        mp_dataset = load_and_validate_mp_dataset(mp_dataset_filepath)
        return self.evaluate_model(
            mp_dataset, model_repo, model_type, entropy_topk, row_limit
        )

    def _evaluate_encoder_row(self, row, row_results):
        try:
            prob_gram, prob_ungram = self.evaluator.run_masked_prediction(
                row.masked_text,
                row.form_grammatical,
                row.form_ungrammatical,
            )
        except Exception as e:
            logging.error(f"Failed to evaluate row {row.sentence_id}: {e}")
            prob_gram, prob_ungram = 0.0, 0.0

        row_results["p_grammatical"] = prob_gram
        row_results["p_ungrammatical"] = prob_ungram
        row_results["I_grammatical"] = compute_surprisal(prob_gram)
        row_results["I_ungrammatical"] = compute_surprisal(prob_ungram)

        if "ood_minimal_pairs" in row:
            self._evaluate_ood_pairs(
                row,
                row_results,
                lambda g, u: self.evaluator.run_masked_prediction(
                    row.masked_text, g, u
                ),
            )

    def _evaluate_decoder_row(self, row, row_results):
        try:
            prob_gram, prob_ungram = self.evaluator.run_next_word_prediction(
                row.prompt_text, row.form_grammatical, row.form_ungrammatical
            )
        except Exception as e:
            logging.error(f"Failed to evaluate row {row.sentence_id}: {e}")
            prob_gram, prob_ungram = 0.0, 0.0

        row_results["p_grammatical"] = prob_gram
        row_results["p_ungrammatical"] = prob_ungram
        row_results["I_grammatical"] = compute_surprisal(prob_gram)
        row_results["I_ungrammatical"] = compute_surprisal(prob_ungram)

        if "ood_minimal_pairs" in row:
            self._evaluate_ood_pairs(
                row,
                row_results,
                lambda g, u: self.evaluator.run_next_word_prediction(
                    row.prompt_text, g, u
                ),
            )

    def _evaluate_ood_pairs(self, row, row_results, evaluation_func):
        ood_pairs = ast.literal_eval(row.ood_pairs)
        all_ood_probs_gram = []
        all_ood_probs_ungram = []

        for pair in ood_pairs:
            prob_gram, prob_ungram = evaluation_func(pair[0], pair[1])
            all_ood_probs_gram.append(prob_gram)
            all_ood_probs_ungram.append(prob_ungram)

        avg_ood_prob_gram = compute_mean(all_ood_probs_gram)
        avg_ood_prob_ungram = compute_mean(all_ood_probs_ungram)

        row_results.update(
            {
                "ood_p_grammatical": avg_ood_prob_gram,
                "ood_p_ungrammatical": avg_ood_prob_ungram,
                "ood_I_grammatical": compute_surprisal(avg_ood_prob_gram),
                "ood_I_ungrammatical": compute_surprisal(avg_ood_prob_ungram),
            }
        )

    def get_avg_surprisal_difference(self, is_ood: bool = False) -> float:
        """
        Get the normalised average surprisal difference (ASD).
        A higher score indicates that the model, on average, tends towards being more confident in the grammatical word over the ungrammatical one.
        However, this is not quite fully accurate and as with any average ASD scores may suffer from outliers skewing the result.

        :return: the value of the normalised ASD.
        """
        p_grammatical_col = "p_grammatical" if not is_ood else "ood_p_grammatical"
        p_ungrammatical_col = "p_ungrammatical" if not is_ood else "ood_p_ungrammatical"
        if not self.is_model_evaluated():
            raise KeyError("Please evaluate a model first.")
        return compute_average_surprisal_difference(
            self.evaluation_dataset[p_grammatical_col],
            self.evaluation_dataset[p_ungrammatical_col],
        )

    def get_norm_avg_surprisal_difference(self) -> float:
        """
        Get the normalised average surprisal difference (ASD).
        A higher score indicates that the model, on average, tends towards being more confident in the grammatical word over the ungrammatical one.
        However, this is not quite fully accurate and as with any average ASD scores may suffer from outliers skewing the result.

        This normalised version simply calculates (Average Grammatical Surprisal - Average Ungrammatical Surprisal) / Average Grammatical Surprisal

        :return: the value of the normalised ASD.
        """
        if not self.is_model_evaluated():
            raise KeyError("Please evaluate a model first.")
        return compute_normalised_surprisal_difference(
            self.evaluation_dataset["p_grammatical"],
            self.evaluation_dataset["p_ungrammatical"],
        )

    def get_accuracy(self) -> float:
        """
        Get the proportion of the time that the model predicts the grammatical form over the ungrammatical form.

        :return: a float between 0 and 1, where 1 is 100% accuracy.
        """
        return compute_accuracy(self._get_grammatical_form_probs, self._get_ungrammatical_form_probs)

    @property
    def _get_grammatical_form_probs(self) -> pd.Series:
        if "p_grammatical" in self.evaluation_dataset.columns:
            return self.evaluation_dataset["p_grammatical"]
        else:
            raise KeyError("Please evaluate a model first.")

    @property
    def _get_ungrammatical_form_probs(self) -> pd.Series:
        if "p_ungrammatical" in self.evaluation_dataset.columns:
            return self.evaluation_dataset["p_ungrammatical"]
        else:
            raise KeyError("Please evaluate a model first.")


class Evaluator:
    def __init__(self):
        self.tokeniser: PreTrainedTokenizerBase = None
        self.model: PreTrainedModel = None

        self.mask_token_index: int = -1
        self.mask_probs: torch.Tensor | None = None
        self.logits: torch.Tensor = None
        self.device: str = None

    def setup_parameters(
        self, model_name: str, is_mlm: bool = True, device: str = "cpu"
    ) -> Tuple[PreTrainedTokenizerBase, PreTrainedModel]:
        if is_mlm:
            self.tokeniser = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForMaskedLM.from_pretrained(model_name)
        else:
            self.tokeniser = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(model_name)

        # set to eval mode, disabling things like dropout
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device)
        self.model.eval()

        return self.model, self.tokeniser
    
    @contextmanager
    def load_model(self, model_name: str, is_mlm: bool = True):
        """Context manager for proper model cleanup."""
        try:
            self.setup_parameters(model_name, is_mlm)
            yield self
        finally:
            if self.model is not None:
                del self.model
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            self.model = None
            self.tokeniser = None

    def run_masked_prediction(
        self, sentence: str, grammatical_word: str, ungrammatical_word: str
    ) -> Tuple[float, float]:
        if not self.model or not self.tokeniser:
            raise RuntimeError("Model and tokenizer must be loaded before prediction.")

        mask_token = self.tokeniser.mask_token
        sentence_masked = sentence.replace("[MASK]", mask_token)

        if sentence_masked.count(mask_token) != 1:
            raise TooManyMasksException("Only single-mask sentences are supported.")

        masked_ids = self.tokeniser.encode(sentence_masked, add_special_tokens=False)
        mask_index = masked_ids.index(self.tokeniser.mask_token_id)

        device = next(self.model.parameters()).device
        g_ids = self.tokeniser.encode(grammatical_word, add_special_tokens=False)
        u_ids = self.tokeniser.encode(ungrammatical_word, add_special_tokens=False)

        g_prob = self._compute_masked_joint_probability(
            masked_ids, mask_index, g_ids, device
        )
        u_prob = self._compute_masked_joint_probability(
            masked_ids, mask_index, u_ids, device
        )

        return g_prob, u_prob

    def _compute_masked_joint_probability(
        self, input_ids: List[int], mask_index: int, word_ids: List[int], device
    ) -> float:
        input_ids_tensor = torch.tensor([input_ids], device=device)
        log_prob = 0.0
        index = mask_index

        for i, tid in enumerate(word_ids):
            with torch.no_grad():
                logits = self.model(input_ids_tensor).logits

            probs = F.softmax(logits[:, index, :], dim=-1)
            token_prob = max(probs[0, tid].item(), 1e-12)  # avoid log(0)
            log_prob += math.log(token_prob + 1e-12)

            if i == 0:
                self.mask_probs = probs

            # Replace mask with predicted token
            input_ids_tensor[0, index] = tid

            # Insert new mask if more tokens remain
            if i < len(word_ids) - 1:
                input_ids_tensor = torch.cat(
                    [
                        input_ids_tensor[:, : index + 1],
                        torch.tensor([[self.tokeniser.mask_token_id]], device=device),
                        input_ids_tensor[:, index + 1 :],
                    ],
                    dim=1,
                )

                index += 1

        return math.exp(log_prob)

    def run_next_word_prediction(
        self, context: str, grammatical_word: str, ungrammatical_word: str
    ) -> Tuple[float, float]:
        if not self.model or not self.tokeniser:
            raise RuntimeError("Model and tokenizer must be loaded before prediction.")

        context_ids = self.tokeniser.encode(context, add_special_tokens=False)
        device = next(self.model.parameters()).device

        g_ids = self.tokeniser.encode(grammatical_word, add_special_tokens=False)
        u_ids = self.tokeniser.encode(ungrammatical_word, add_special_tokens=False)

        g_prob = self._compute_next_word_joint_probability(context_ids, g_ids, device)
        u_prob = self._compute_next_word_joint_probability(context_ids, u_ids, device)

        return g_prob, u_prob

    def _compute_next_word_joint_probability(
        self, input_ids: List[int], word_ids: List[int], device
    ) -> float:
        input_ids_tensor = torch.tensor([input_ids], device=device)
        log_prob = 0.0

        for i, tid in enumerate(word_ids):
            with torch.no_grad():
                logits = self.model(input_ids_tensor).logits

            index = input_ids_tensor.shape[1] - 1  # last token position
            probs = F.softmax(logits[:, index, :], dim=-1)
            token_prob = probs[0, tid].item()
            log_prob += math.log(token_prob + 1e-12)

            if i == 0:
                self.mask_probs = probs

            # Append predicted token to context
            input_ids_tensor = torch.cat(
                [input_ids_tensor, torch.tensor([[tid]], device=device)], dim=1
            )

        return math.exp(log_prob)

    def get_entropy_based_certainty(self, k: int = 100, normalise: bool = False) -> float:
        """Compute entropy over the prediction distribution.

        k: Number of top tokens to consider.
        normalise: Whether to normalise entropy.

        Returns:
        :returns: Certainty value based on entropy calculations over token probabiliity distribution.
        """
        if self.mask_probs is None:
            raise ValueError("No output probabilities available. Run evaluation first.")
        return compute_entropy_based_certainty(self.mask_probs, k)

    def _get_mask_index(self, inputs: Any) -> int:
        if "input_ids" not in inputs:
            raise ValueError("Missing 'input_ids' in inputs.")
        elif self.tokeniser.mask_token_id is None:
            raise ValueError("The tokeniser does not have a defined mask_token_id.")

        input_ids = inputs["input_ids"]
        mask_positions = torch.where(input_ids == self.tokeniser.mask_token_id)

        if len(mask_positions[0]) == 0:
            raise ValueError("No mask token found in input_ids.")
        elif len(mask_positions[0]) > 1:
            raise ValueError("Multiple mask tokens found; expected only one.")

        return (
            mask_positions[1].item()
            if len(mask_positions) > 1
            else mask_positions[0].item()
        )

    def _get_mask_probabilities(
        self, mask_token_index: int, logits: Any
    ) -> torch.Tensor:
        mask_logits = logits[0, mask_token_index, :]
        probs = F.softmax(mask_logits, dim=-1)  # shape: (vocab_size, )
        return probs
