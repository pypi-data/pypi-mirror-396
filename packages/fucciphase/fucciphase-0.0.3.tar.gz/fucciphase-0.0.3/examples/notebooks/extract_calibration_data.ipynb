{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Extracting FUCCI calibration data (reference traces)\n",
    "\n",
    "This notebook demonstrates how to **extract calibration time-courses** from raw FUCCI movies\n",
    "(e.g. TrackMate XML + matching image files) to generate reference traces that can be used for\n",
    "sensor construction.\n",
    "\n",
    "**You will learn to:**\n",
    "- Load TrackMate-linked trajectories from `.xml` tracking files\n",
    "- Extract per-cell fluorescence curves for FUCCI channels\n",
    "- Pre-process, normalise and filter calibration tracks\n",
    "- Export reference traces for use in `sensor_calibration.ipynb`\n",
    "\n",
    "**When to use this notebook**\n",
    "Use this notebook *before* building a sensor. If you have raw FUCCI movies and want to\n",
    "extract ground-truth time-courses for G1/S/G2-M marker evolution, this is the first step.\n",
    "\n",
    "**Input expected**\n",
    "\n",
    "\n",
    "|   File type   |                     Example path                           |\n",
    "|---------------|------------------------------------------------------------|\n",
    "| TrackMate XML | `examples/reproducibility/inputs/merged_linked.ome.xml`    |\n",
    "| TIFF/OME-TIFF | `examples/reproducibility/inputs/downscaled_hacat.ome.tif` |\n",
    "\n",
    "\n",
    "**Output**\n",
    "A CSV reference like: `examples/example_data/hacat_fucciphase_reference.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fucciphase import process_trackmate\n",
    "from fucciphase.plot import plot_raw_intensities\n",
    "from fucciphase.sensor import get_fuccisa_default_sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glob.glob(\"*.tif\"))\n",
    "print(glob.glob(\"*.ome.tif\"))\n",
    "print(glob.glob(\"*.xml\"))\n",
    "print(glob.glob(\"inputs/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Read all TrackMate outputs\n",
    "\n",
    "* Multiple videos were processed are saved as XML files\n",
    "* The XML files should follow a template naming scheme so that it can be identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_file = \"*.xml\"\n",
    "track_files = glob.glob(template_file)\n",
    "print(track_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# General information\n",
    "\n",
    "Pass information about the channel names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyan_channel = \"MEAN_INTENSITY_CH2\"\n",
    "magenta_channel = \"MEAN_INTENSITY_CH1\"\n",
    "default_sensor = get_fuccisa_default_sensor()\n",
    "regex = r\"Track_[0-9]+\\.[a-z]+\"\n",
    "timestep = 0.25  # in hours\n",
    "max_n_frames = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Extract full tracks\n",
    "Obtain tracks that go from cell division to cell division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracks followed by another branch\n",
    "dfs_save_tracks = []\n",
    "# tracks that are not followed by another branch\n",
    "dfs_candidate_tracks = []\n",
    "for track_file in track_files:\n",
    "    print(track_file)\n",
    "    df = process_trackmate(\n",
    "        track_file,\n",
    "        channels=[cyan_channel, magenta_channel],\n",
    "        sensor=default_sensor,\n",
    "        thresholds=[0.1, 0.1],\n",
    "        generate_unique_tracks=True,\n",
    "    )\n",
    "    all_names = df[\"name\"].unique()\n",
    "    candidate_tracks = []\n",
    "    track_ids = df[\"UNIQUE_TRACK_ID\"].unique()\n",
    "    for track_id in track_ids:\n",
    "        track = df[df[\"UNIQUE_TRACK_ID\"] == track_id]\n",
    "        name = track[\"name\"].iloc[0]\n",
    "        last_frame = track[\"FRAME\"].max()\n",
    "        # is the track a subtrack\n",
    "        match = re.match(regex, name)\n",
    "        # is there a subtrack\n",
    "        next_match = any(df[\"name\"].str.match(name + \"[a-z]+\").unique())\n",
    "        if match is not None and last_frame < df[\"FRAME\"].max():\n",
    "            print(\"Track ID: \", track_id)\n",
    "            if next_match:\n",
    "                level_last_value = (\n",
    "                    track[magenta_channel].iloc[-1] - track[magenta_channel].min()\n",
    "                ) / (track[magenta_channel].max() - track[magenta_channel].min())\n",
    "                if level_last_value > 0.5:\n",
    "                    continue\n",
    "                dfs_save_tracks.append(track[[\"FRAME\", cyan_channel, magenta_channel]])\n",
    "                title = f\"Save track: {track_id}, {name}\"\n",
    "                max_n_frames = max(\n",
    "                    max_n_frames, track[\"FRAME\"].iloc[-1] - track[\"FRAME\"].iloc[0]\n",
    "                )\n",
    "            else:\n",
    "                if len(track) > 40:\n",
    "                    dfs_candidate_tracks.append(\n",
    "                        track[[\"FRAME\", cyan_channel, magenta_channel]]\n",
    "                    )\n",
    "                    title = f\"Candidate track: {track_id}, {name}\"\n",
    "                    max_n_frames = max(\n",
    "                        max_n_frames, track[\"FRAME\"].iloc[-1] - track[\"FRAME\"].iloc[0]\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            plot_raw_intensities(track, channel1=cyan_channel, channel2=magenta_channel)\n",
    "            plt.title(title)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Accept all candidate tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_save_tracks.extend(dfs_candidate_tracks)\n",
    "print(f\"Selected {len(dfs_save_tracks)} tracks to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Interpolate from time to percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = np.linspace(0, 100, num=101)\n",
    "cyan_interp = []\n",
    "magenta_interp = []\n",
    "average_time = 0\n",
    "counter = 0\n",
    "for track_df in dfs_save_tracks:\n",
    "    track_cyan = track_df[cyan_channel]\n",
    "    track_magenta = track_df[magenta_channel]\n",
    "    time = timestep * (track_df[\"FRAME\"] - track_df[\"FRAME\"].min())\n",
    "    average_time += time.max()\n",
    "    # convert time to percentage\n",
    "    time = 100.0 * time / time.iloc[-1]\n",
    "    time = time.to_numpy()\n",
    "    cyan_interp.append(np.interp(percentages, time, track_cyan))\n",
    "    magenta_interp.append(np.interp(percentages, time, track_magenta))\n",
    "    counter += 1\n",
    "\n",
    "average_time /= counter\n",
    "print(\"Average duration of cell cycle: \", average_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Interpolate all curves\n",
    "\n",
    "Interpolate curves to percentage scale and normalise curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyan_interp = np.array(cyan_interp)\n",
    "magenta_interp = np.array(magenta_interp)\n",
    "cyan_normal = np.zeros(shape=cyan_interp.shape)\n",
    "magenta_normal = np.zeros(shape=magenta_interp.shape)\n",
    "\n",
    "\n",
    "for idx, cyan_curve in enumerate(cyan_interp):\n",
    "    cyan_curve_shifted = cyan_curve - cyan_curve.min()\n",
    "    cyan_normal[idx] = cyan_curve_shifted / cyan_curve_shifted.max()\n",
    "    plt.plot(percentages, cyan_normal[idx])\n",
    "plt.xlabel(\"Percentage\")\n",
    "plt.ylabel(\"Relative intensity\")\n",
    "plt.show()\n",
    "\n",
    "for idx, magenta_curve in enumerate(magenta_interp):\n",
    "    magenta_curve_shifted = magenta_curve - magenta_curve.min()\n",
    "    magenta_normal[idx] = magenta_curve_shifted / magenta_curve_shifted.max()\n",
    "    plt.plot(percentages, magenta_normal[idx])\n",
    "plt.xlabel(\"Percentage\")\n",
    "plt.ylabel(\"Relative intensity\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Plot the average curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = \"tab:cyan\"\n",
    "color2 = \"m\"  # magenta\n",
    "\n",
    "cyan_mean = cyan_normal.mean(axis=0)\n",
    "cyan_std = cyan_normal.std(axis=0)\n",
    "\n",
    "magenta_mean = magenta_normal.mean(axis=0)\n",
    "magenta_std = magenta_normal.std(axis=0)\n",
    "\n",
    "plt.plot(percentages, cyan_mean, color=color)\n",
    "plt.fill_between(\n",
    "    percentages, cyan_mean - cyan_std, cyan_mean + cyan_std, alpha=0.5, color=color\n",
    ")\n",
    "plt.plot(percentages, magenta_mean, color=color2)\n",
    "plt.fill_between(\n",
    "    percentages,\n",
    "    magenta_mean - magenta_std,\n",
    "    magenta_mean + magenta_std,\n",
    "    alpha=0.5,\n",
    "    color=color2,\n",
    ")\n",
    "plt.xlabel(\"Percentage w.r.t. total time\")\n",
    "plt.ylabel(\"Normalised intensity\")\n",
    "plt.savefig(\"average_intensities_hacat.png\")\n",
    "plt.savefig(\"average_intensities_hacat.pdf\")\n",
    "plt.savefig(\"average_intensities_hacat.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = pd.DataFrame(\n",
    "    {\n",
    "        \"percentage\": percentages,\n",
    "        \"time\": percentages * average_time / 100.0,\n",
    "        \"cyan\": cyan_mean,\n",
    "        \"magenta\": magenta_mean,\n",
    "    }\n",
    ")\n",
    "export_df.to_csv(\"hacat_fucciphase_reference.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
