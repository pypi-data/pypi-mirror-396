# -*- coding: utf-8 -*-
"""
Proæ¨¡å¼åµŒå…¥å™¨

ä½¿ç”¨é«˜è´¨é‡ Qwen3-Embedding-0.6B-Q8_0 æ¨¡å‹ï¼Œé€‚åˆéœ€è¦é«˜ç²¾åº¦çš„åœºæ™¯ã€‚
æ¨¡å‹å¤§å°çº¦ 637MBï¼Œç»´åº¦ 1024ã€‚
"""

from typing import List, Optional, Union, cast
import os
from .manager import BaseEmbedder


class ProEmbedder(BaseEmbedder):
    """
    Proæ¨¡å¼åµŒå…¥å™¨

    ä½¿ç”¨ Qwen3-Embedding-0.6B-Q8_0 æ¨¡å‹ï¼Œç‰¹ç‚¹ï¼š
    - é«˜è´¨é‡åµŒå…¥ï¼ˆQ8_0é‡åŒ–ä¿ç•™æ›´å¤šç²¾åº¦ï¼‰
    - ç»´åº¦1024ï¼Œè¯­ä¹‰è¡¨è¾¾èƒ½åŠ›æ›´å¼º
    - æ”¯æŒ32Kä¸Šä¸‹æ–‡
    - é€‚åˆProæ¨¡å¼ä¸‹çš„é«˜ç²¾åº¦è¯­ä¹‰æœç´¢
    """

    # æ¨¡å‹é…ç½® - ä½¿ç”¨Q8_0é‡åŒ–ç‰ˆæœ¬
    MODEL_FILENAME = "Qwen3-Embedding-0.6B-Q8_0.gguf"
    MODEL_URL = "https://hf-mirror.com/Qwen/Qwen3-Embedding-0.6B-GGUF/resolve/main/Qwen3-Embedding-0.6B-Q8_0.gguf?download=true"
    EXPECTED_DIMENSION = 1024

    def __init__(
        self,
        model_path: Optional[str] = None,
        device: Optional[str] = None,
        n_threads: Optional[int] = None,
    ):
        """
        åˆå§‹åŒ–ProåµŒå…¥å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆNoneåˆ™è‡ªåŠ¨ä¸‹è½½ï¼‰
            device: è¿è¡Œè®¾å¤‡ï¼ˆ'cuda', 'cpu', Noneè‡ªåŠ¨æ£€æµ‹ï¼‰
            n_threads: çº¿ç¨‹æ•°ï¼ˆNoneè‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        try:
            from seesea_core import get_file
        except ImportError as e:
            raise ImportError(
                "è¯·å…ˆå®‰è£…ä¾èµ–: pip install llama-cpp-python seesea_core"
            ) from e

        # æ¨¡å‹ç›®å½•
        llm_dir = ".llm"
        models_dir = os.path.join(llm_dir, "models")
        local_model_file = os.path.join(models_dir, self.MODEL_FILENAME)

        # ç¡®å®šæ¨¡å‹è·¯å¾„
        if model_path is None:
            if os.path.exists(local_model_file):
                print(f"ğŸ“ [Pro] ä½¿ç”¨å·²å­˜åœ¨æ¨¡å‹: {local_model_file}")
                model_path = local_model_file
            else:
                print("â¬‡ï¸  [Pro] ä¸‹è½½é«˜è´¨é‡åµŒå…¥æ¨¡å‹ï¼ˆQ8_0é‡åŒ–ï¼‰...")
                os.makedirs(models_dir, exist_ok=True)

                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }

                try:
                    result = get_file(self.MODEL_URL, local_model_file, headers)
                    if result.get("status") != 200:
                        raise RuntimeError(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {result.get('status')}")
                    print(f"âœ… [Pro] æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_model_file}")
                except Exception as e:
                    raise RuntimeError(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}") from e

                model_path = local_model_file

        # GPUé…ç½®
        n_gpu_layers = self._detect_gpu(device)

        # çº¿ç¨‹é…ç½®
        if n_threads is None:
            n_threads = max(1, os.cpu_count() or 4)
        self.n_threads = n_threads

        # åŠ è½½æ¨¡å‹
        print("ğŸ”„ [Pro] åŠ è½½é«˜è´¨é‡åµŒå…¥æ¨¡å‹...")
        self._load_model(model_path, n_gpu_layers, n_threads)

    def _load_model(
        self, model_path: str, n_gpu_layers: int, n_threads: int, retry: bool = True
    ):
        """åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒé‡è¯•"""
        from llama_cpp import Llama

        try:
            self.embedder = Llama(
                model_path=model_path,
                embedding=True,
                n_gpu_layers=n_gpu_layers,
                n_ctx=32768,  # å®Œæ•´32Kä¸Šä¸‹æ–‡
                n_threads=n_threads,
                verbose=False,
                n_output=0,
                logits_all=False,
                use_mmap=True,
                use_mlock=False,
            )

            # æµ‹è¯•è·å–ç»´åº¦
            test_result = self.embedder.create_embedding(input="test")
            self.dimension = len(test_result["data"][0]["embedding"])
            print(f"âœ… [Pro] æ¨¡å‹åŠ è½½å®Œæˆï¼Œç»´åº¦: {self.dimension}")

        except Exception as e:
            if retry and "Failed to load model" in str(e):
                print("âŒ [Pro] æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•é‡æ–°ä¸‹è½½...")
                if os.path.exists(model_path):
                    os.remove(model_path)

                from seesea_core import get_file

                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }
                get_file(self.MODEL_URL, model_path, headers)

                # é‡è¯•åŠ è½½ï¼ˆä¸å†é‡è¯•ï¼‰
                self._load_model(model_path, n_gpu_layers, n_threads, retry=False)
            else:
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}") from e

    def _detect_gpu(self, device: Optional[str]) -> int:
        """æ£€æµ‹GPUé…ç½®"""
        if device == "cuda":
            return -1
        elif device == "cpu":
            return 0
        else:
            # è‡ªåŠ¨æ£€æµ‹
            gpu_env_vars = [
                "CUDA_VISIBLE_DEVICES",
                "NVIDIA_VISIBLE_DEVICES",
                "CUDA_PATH",
            ]
            for var in gpu_env_vars:
                if os.environ.get(var):
                    return -1
            return 0

    def encode(
        self, texts: Union[str, List[str]], batch_size: int = 8
    ) -> Union[List[float], List[List[float]]]:
        """
        ç¼–ç æ–‡æœ¬ä¸ºå‘é‡

        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°

        Returns:
            å•ä¸ªå‘é‡æˆ–å‘é‡åˆ—è¡¨
        """
        single_input = isinstance(texts, str)
        texts_to_process: List[str]
        if single_input:
            texts_to_process = [texts]  # type: ignore[list-item]
        else:
            texts_to_process = texts  # type: ignore[assignment]

        # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼ˆ32K tokens â‰ˆ 8K charsä¿å®ˆä¼°è®¡ï¼‰
        max_chars = 8192
        truncated_texts = [
            text[:max_chars] if len(text) > max_chars else text
            for text in texts_to_process
        ]

        all_embeddings = []
        for text in truncated_texts:
            try:
                result = self.embedder.create_embedding(input=[text])
                if result and "data" in result and result["data"]:
                    embedding = cast(
                        List[float], result["data"][0].get("embedding", [])
                    )
                    if embedding:
                        all_embeddings.append(embedding)
            except Exception:
                pass  # è·³è¿‡å¤±è´¥çš„æ–‡æœ¬

        if single_input and all_embeddings:
            return all_embeddings[0]
        return all_embeddings

    def get_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return self.dimension

    def encode_callback(self, text: str) -> List[float]:
        """
        Rustå›è°ƒæ¥å£

        Args:
            text: è¦ç¼–ç çš„æ–‡æœ¬

        Returns:
            å‘é‡
        """
        result = cast(List[float], self.encode(text))
        return result
