# -*- coding: utf-8 -*-
"""
æ ‡å‡†æ¨¡å¼åµŒå…¥å™¨

ä½¿ç”¨è½»é‡çº§ all-MiniLM-L6-v2-Q4_K_M æ¨¡å‹ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒã€‚
æ¨¡å‹å¤§å°çº¦ 23MBï¼Œç»´åº¦ 384ã€‚
"""

from typing import List, Optional, Union, cast
import os
from .manager import BaseEmbedder


class StandardEmbedder(BaseEmbedder):
    """
    æ ‡å‡†æ¨¡å¼åµŒå…¥å™¨

    ä½¿ç”¨ all-MiniLM-L6-v2-Q4_K_M æ¨¡å‹ï¼Œç‰¹ç‚¹ï¼š
    - æ¨¡å‹å°å·§ï¼ˆ~23MBï¼‰
    - æ¨ç†é€Ÿåº¦å¿«
    - ç»´åº¦384ï¼Œè¶³å¤Ÿç”¨äºç›¸å…³æ€§è®¡ç®—
    - é€‚åˆæ ‡å‡†æ¨¡å¼ä¸‹çš„å¿«é€ŸåµŒå…¥
    """

    # æ¨¡å‹é…ç½®
    MODEL_FILENAME = "all-MiniLM-L6-v2-Q4_K_M.gguf"
    MODEL_URL = "https://hf-mirror.com/second-state/All-MiniLM-L6-v2-Embedding-GGUF/resolve/main/all-MiniLM-L6-v2-Q4_K_M.gguf?download=true"
    EXPECTED_DIMENSION = 384

    def __init__(
        self,
        model_path: Optional[str] = None,
        device: Optional[str] = None,
        n_threads: Optional[int] = None,
    ):
        """
        åˆå§‹åŒ–æ ‡å‡†åµŒå…¥å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆNoneåˆ™è‡ªåŠ¨ä¸‹è½½ï¼‰
            device: è¿è¡Œè®¾å¤‡ï¼ˆ'cuda', 'cpu', Noneè‡ªåŠ¨æ£€æµ‹ï¼‰
            n_threads: çº¿ç¨‹æ•°ï¼ˆNoneè‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        try:
            from llama_cpp import Llama
            from seesea_core import get_file
        except ImportError as e:
            raise ImportError(
                "è¯·å…ˆå®‰è£…ä¾èµ–: pip install llama-cpp-python seesea_core"
            ) from e

        # æ¨¡å‹ç›®å½•
        llm_dir = ".llm"
        models_dir = os.path.join(llm_dir, "models")
        local_model_file = os.path.join(models_dir, self.MODEL_FILENAME)

        # ç¡®å®šæ¨¡å‹è·¯å¾„
        if model_path is None:
            if os.path.exists(local_model_file):
                print(f"ğŸ“ [Standard] ä½¿ç”¨å·²å­˜åœ¨æ¨¡å‹: {local_model_file}")
                model_path = local_model_file
            else:
                print("â¬‡ï¸  [Standard] ä¸‹è½½è½»é‡çº§åµŒå…¥æ¨¡å‹...")
                os.makedirs(models_dir, exist_ok=True)

                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                }

                try:
                    result = get_file(self.MODEL_URL, local_model_file, headers)
                    if result.get("status") != 200:
                        raise RuntimeError(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {result.get('status')}")
                    print(f"âœ… [Standard] æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_model_file}")
                except Exception as e:
                    raise RuntimeError(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}") from e

                model_path = local_model_file

        # GPUé…ç½®
        n_gpu_layers = self._detect_gpu(device)

        # çº¿ç¨‹é…ç½®
        if n_threads is None:
            n_threads = max(1, os.cpu_count() or 4)
        self.n_threads = n_threads

        # åŠ è½½æ¨¡å‹
        print("ğŸ”„ [Standard] åŠ è½½åµŒå…¥æ¨¡å‹...")
        try:
            self.embedder = Llama(
                model_path=model_path,
                embedding=True,
                n_gpu_layers=n_gpu_layers,
                n_ctx=512,  # å°æ¨¡å‹ä½¿ç”¨è¾ƒå°ä¸Šä¸‹æ–‡
                n_threads=n_threads,
                verbose=False,
                use_mmap=True,
                use_mlock=False,
            )

            # æµ‹è¯•è·å–ç»´åº¦
            test_result = self.embedder.create_embedding(input="test")
            self.dimension = len(test_result["data"][0]["embedding"])
            print(f"âœ… [Standard] æ¨¡å‹åŠ è½½å®Œæˆï¼Œç»´åº¦: {self.dimension}")

        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}") from e

    def _detect_gpu(self, device: Optional[str]) -> int:
        """æ£€æµ‹GPUé…ç½®"""
        if device == "cuda":
            return -1
        elif device == "cpu":
            return 0
        else:
            # è‡ªåŠ¨æ£€æµ‹
            gpu_env_vars = [
                "CUDA_VISIBLE_DEVICES",
                "NVIDIA_VISIBLE_DEVICES",
                "CUDA_PATH",
            ]
            for var in gpu_env_vars:
                if os.environ.get(var):
                    return -1
            return 0

    def encode(
        self, texts: Union[str, List[str]], batch_size: int = 8
    ) -> Union[List[float], List[List[float]]]:
        """
        ç¼–ç æ–‡æœ¬ä¸ºå‘é‡

        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆæ ‡å‡†æ¨¡å¼ä½¿ç”¨é€ä¸ªå¤„ç†ï¼‰

        Returns:
            å•ä¸ªå‘é‡æˆ–å‘é‡åˆ—è¡¨
        """
        single_input = isinstance(texts, str)
        texts_to_process: List[str]
        if single_input:
            texts_to_process = [texts]  # type: ignore[list-item]
        else:
            texts_to_process = texts  # type: ignore[assignment]

        # é™åˆ¶æ–‡æœ¬é•¿åº¦
        max_chars = 2048  # æ ‡å‡†æ¨¡å‹ä¸Šä¸‹æ–‡è¾ƒå°
        truncated_texts = [
            text[:max_chars] if len(text) > max_chars else text
            for text in texts_to_process
        ]

        all_embeddings = []
        for text in truncated_texts:
            try:
                result = self.embedder.create_embedding(input=[text])
                if result and "data" in result and result["data"]:
                    embedding = cast(
                        List[float], result["data"][0].get("embedding", [])
                    )
                    if embedding:
                        all_embeddings.append(embedding)
            except Exception:
                pass  # è·³è¿‡å¤±è´¥çš„æ–‡æœ¬

        if single_input and all_embeddings:
            return all_embeddings[0]
        return all_embeddings

    def get_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return self.dimension

    def encode_callback(self, text: str) -> List[float]:
        """
        Rustå›è°ƒæ¥å£

        Args:
            text: è¦ç¼–ç çš„æ–‡æœ¬

        Returns:
            å‘é‡
        """
        result = cast(List[float], self.encode(text))
        return result
