"""
Embeddings module for text vectorization using Qwen3-Embedding model with llama-cpp-python.
æ¨¡å—åç§°: llama_cpp_embeddings
èŒè´£èŒƒå›´: æ–‡æœ¬å‘é‡åŒ–ï¼Œä½¿ç”¨llama-cpp-pythonåŠ è½½å’Œè¿è¡ŒGGUFæ ¼å¼çš„Qwen3åµŒå…¥æ¨¡å‹
æœŸæœ›å®ç°è®¡åˆ’: æä¾›é«˜æ•ˆçš„æ–‡æœ¬åµŒå…¥æœåŠ¡ï¼Œæ”¯æŒæ¨¡å‹è‡ªåŠ¨ä¸‹è½½å’ŒåŠ è½½
å·²å®ç°åŠŸèƒ½: æ–‡æœ¬åµŒå…¥ã€æ‰¹é‡å¤„ç†ã€å›è°ƒå‡½æ•°æ”¯æŒã€æ¨¡å‹è‡ªåŠ¨ä¸‹è½½
ä½¿ç”¨ä¾èµ–: llama-cpp-python, seesea_core
ä¸»è¦æ¥å£: LlamaCppEmbedderç±»ï¼ŒåŒ…å«encodeã€get_dimensionã€encode_callbackæ–¹æ³•
æ³¨æ„äº‹é¡¹: é¦–æ¬¡ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹åˆ°.llm/modelsç›®å½•
"""

from typing import List, Union, Optional
import os


class LlamaCppEmbedder:
    """
    Text embedder using Qwen3-Embedding model with llama-cpp-python backend.

    This class handles the conversion of text to vector embeddings
    that can be used with the Rust vector store, using llama-cpp-python
    to load and run the Qwen3 embedding model in GGUF format.
    """

    def __init__(
        self,
        model_path: Optional[str] = None,
        device: Optional[str] = None,
        n_threads: Optional[int] = None,
    ):
        """
        Initialize the text embedder with llama-cpp-python.

        Args:
            model_path: Path to the GGUF format Qwen3 embedding model
            device: Device to use ('cuda', 'cpu', or None for auto-detect)
            n_threads: Number of threads to use for embedding generation. If None, auto-detects based on CPU cores.
        """
        try:
            # Import required modules
            from llama_cpp import Llama

            # Import seesea_core functions for model download
            from seesea_core import get_file

            # Model settings
            model_filename = "Qwen3-Embedding-0.6B-f16.gguf"
            # Use fixed directory for models
            llm_dir = ".llm"
            models_dir = os.path.join(llm_dir, "models")
            local_model_file = os.path.join(models_dir, model_filename)

            # Set default model path if not provided
            if model_path is None:
                # 1. Check if local model file exists
                if os.path.exists(local_model_file):
                    print(f"ğŸ“ æ£€æµ‹åˆ°å·²å­˜åœ¨æ¨¡å‹æ–‡ä»¶: {local_model_file}")
                    print(f"ğŸ” æ¨¡å‹æ–‡ä»¶å¤§å°: {os.path.getsize(local_model_file)} bytes")
                    model_path = local_model_file
                else:
                    print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½")
                    # 2. Create directory if it doesn't exist
                    os.makedirs(models_dir, exist_ok=True)
                    print(f"ğŸ“ åˆ›å»ºæ¨¡å‹ç›®å½•: {models_dir}")

                    # Download model using seesea_core get_file function with zero-copy
                    model_url = "https://hf-mirror.com/Qwen/Qwen3-Embedding-0.6B-GGUF/resolve/main/Qwen3-Embedding-0.6B-f16.gguf?download=true"

                    # Set custom headers for faster download
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                    }

                    print(f"ğŸ”„ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_filename}")
                    print(f"ğŸ“¥ ä¸‹è½½åœ°å€: {model_url}")
                    print(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {local_model_file}")

                    # Download model using seesea_core get_file (zero-copy optimized for large files)
                    try:
                        download_result = get_file(model_url, local_model_file, headers)

                        # Check if download was successful
                        status = download_result["status"]
                        print(f"ğŸ“Š ä¸‹è½½çŠ¶æ€ç : {status}")
                        print(f"ğŸ“‹ ä¸‹è½½ç»“æœ: {download_result}")

                        if status != 200:
                            raise RuntimeError(f"Failed to download model. Status code: {status}")

                        print("âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸï¼")
                        print(f"ğŸ“ ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶å¤§å°: {os.path.getsize(local_model_file)} bytes")
                    except Exception as download_error:
                        print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {download_error}")
                        raise

                    model_path = local_model_file

            # Configure GPU layers based on device and auto-detection
            n_gpu_layers = 0

            # 1. Check if device is explicitly set
            if device == "cuda":
                n_gpu_layers = -1  # Use all GPU layers
            elif device is not None and "cpu" not in device.lower():
                # Try to use GPU if device is not explicitly CPU
                n_gpu_layers = -1
            else:
                # 2. Auto-detect GPU if device is None
                # Use lightweight detection methods without adding new dependencies
                try:
                    # Check if CUDA is available via environment variables
                    # and other lightweight methods
                    gpu_env_vars = ["CUDA_VISIBLE_DEVICES", "NVIDIA_VISIBLE_DEVICES"]
                    for env_var in gpu_env_vars:
                        if os.environ.get(env_var):
                            n_gpu_layers = -1  # Auto-detect: use all GPU layers
                            break
                    # Additional fallback: check if CUDA_PATH is set (Windows)
                    if n_gpu_layers == 0 and os.environ.get("CUDA_PATH"):
                        n_gpu_layers = -1
                except Exception:
                    # Ignore any errors during auto-detection
                    pass

            # Configure number of threads
            if n_threads is None:
                # Auto-detect based on CPU cores
                n_threads = os.cpu_count() or 4  # Default to 4 if auto-detection fails

            # Ensure n_threads is at least 1
            n_threads = max(1, n_threads)

            # Save n_threads as an instance attribute for VectorStoreWrapper to access
            self.n_threads = n_threads

            # åŠ è½½æ¨¡å‹ï¼Œå¤„ç†æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´çš„æƒ…å†µ
            max_attempts = 2  # æœ€å¤§å°è¯•æ¬¡æ•°
            current_attempt = 1
            success = False

            while current_attempt <= max_attempts and not success:
                try:
                    # Initialize llama-cpp-python with embedding support
                    # Use correct parameter names based on llama-cpp-python API
                    # Add parameters to reduce verbosity and fix embedding issues
                    self.embedder = Llama(
                        model_path=model_path,
                        embedding=True,  # Enable embedding mode
                        n_gpu_layers=n_gpu_layers,
                        n_ctx=32768,  # Full context size for embedding, match training context
                        n_threads=n_threads,  # Number of threads to use
                        verbose=False,  # Reduce verbosity
                        n_output=0,  # No output needed for embedding models
                        output_format="json",  # Ensure proper output format
                        logits_all=False,  # Don't return logits
                        use_mmap=True,  # Use memory mapping for faster loading
                        use_mlock=False,  # Don't lock memory
                    )

                    # Test embedding to get dimension
                    # Call create_embedding with correct parameter name 'input'
                    test_embedding = self.embedder.create_embedding(input="test")
                    # Extract embedding correctly from the response
                    self.dimension = len(test_embedding["data"][0]["embedding"])

                    success = True
                except Exception as e:
                    error_msg = str(e)
                    current_attempt += 1

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä¸”æ˜¯ç¬¬ä¸€æ¬¡å°è¯•
                    if "Failed to load model" in error_msg and current_attempt <= max_attempts:
                        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå¼€å§‹æ¸…ç†å¹¶é‡æ–°ä¸‹è½½")
                        # åˆ é™¤å¯èƒ½æŸåçš„æ¨¡å‹æ–‡ä»¶
                        if os.path.exists(model_path):
                            print(f"ğŸ—‘ï¸ åˆ é™¤æŸåçš„æ¨¡å‹æ–‡ä»¶: {model_path}")
                            os.remove(model_path)

                        # é‡æ–°åˆ›å»ºæ¨¡å‹ç›®å½•
                        print("ğŸ“ é‡æ–°åˆ›å»ºæ¨¡å‹ç›®å½•")
                        os.makedirs(os.path.dirname(model_path), exist_ok=True)

                        # é‡æ–°ä¸‹è½½æ¨¡å‹
                        print("ğŸ”„ å¼€å§‹é‡æ–°ä¸‹è½½æ¨¡å‹")
                        from seesea_core import get_file

                        model_url = "https://hf-mirror.com/Qwen/Qwen3-Embedding-0.6B-GGUF/resolve/main/Qwen3-Embedding-0.6B-f16.gguf?download=true"
                        headers = {
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                        }

                        try:
                            print(f"ğŸ“¥ é‡æ–°ä¸‹è½½åœ°å€: {model_url}")
                            print(f"ğŸ’¾ é‡æ–°ä¿å­˜è·¯å¾„: {model_path}")
                            download_result = get_file(model_url, model_path, headers)
                            print(f"ğŸ“Š é‡æ–°ä¸‹è½½çŠ¶æ€ç : {download_result['status']}")
                            print(f"ğŸ“‹ é‡æ–°ä¸‹è½½ç»“æœ: {download_result}")
                        except Exception as redownload_error:
                            print(f"âŒ é‡æ–°ä¸‹è½½å¤±è´¥: {redownload_error}")
                            raise

                        # æ£€æŸ¥ä¸‹è½½çŠ¶æ€
                        if download_result["status"] != 200:
                            raise RuntimeError(
                                f"é‡æ–°ä¸‹è½½æ¨¡å‹å¤±è´¥ã€‚çŠ¶æ€ç : {download_result['status']}"
                            )
                    else:
                        # è¶…è¿‡æœ€å¤§å°è¯•æ¬¡æ•°æˆ–ä¸æ˜¯æ¨¡å‹åŠ è½½é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸
                        raise RuntimeError(
                            f"Failed to initialize Qwen3 embedding model with llama-cpp-python after {current_attempt-1} attempts: {error_msg}"
                        ) from e

        except ImportError as e:
            raise ImportError(
                "Failed to import required modules. Please install llama-cpp-python and seesea_core first: "
                "'pip install llama-cpp-python seesea_core'"
            ) from e

    def encode(
        self, texts: Union[str, List[str]], batch_size: int = 8
    ) -> Union[List[float], List[List[float]]]:
        """
        Encode text(s) into vector embeddings using llama-cpp-python.
        Implements safe processing by handling documents individually when necessary.

        Args:
            texts: Single text string or list of text strings
            batch_size: Not used for safety reasons, individual processing is safer

        Returns:
            Single embedding (List[float]) if input is a string,
            or list of embeddings (List[List[float]]) if input is a list
        """
        # Handle single text input
        if isinstance(texts, str):
            texts = [texts]
            single_input = True
        else:
            single_input = False

        # Limit input text length to avoid llama_decode errors
        max_chars_per_text = 8192  # ~2048 tokens
        truncated_texts = []
        for text in texts:
            if isinstance(text, str) and len(text) > max_chars_per_text:
                truncated_texts.append(text[:max_chars_per_text])
            else:
                truncated_texts.append(text)

        # Generate embeddings with safe processing
        all_embeddings = []

        # Process each document individually to avoid context overflow
        # This is the safest approach for embedding models
        for text in truncated_texts:
            try:
                # Process a single document at a time to avoid context overflow
                result = self.embedder.create_embedding(input=[text])

                # Extract embedding from the response
                if result and "data" in result and result["data"]:
                    data_items = result["data"]
                    embedding = data_items[0].get("embedding", [])
                    if embedding:
                        all_embeddings.append(embedding)
            except Exception:
                # If we encounter any error, just skip this document
                # Avoid complex logging to prevent syntax errors
                pass

        from typing import cast

        # Return single embedding if single input
        if single_input and all_embeddings:
            return cast(List[float], all_embeddings[0])

        return cast(List[List[float]], all_embeddings)

    def get_dimension(self) -> int:
        """
        Get the dimension of the embeddings.

        Returns:
            Embedding dimension
        """
        return self.dimension

    def encode_callback(self, text: str) -> List[float]:
        """
        Callback function for Rust integration.

        This function is designed to be called from Rust to get embeddings.

        Args:
            text: Text to encode

        Returns:
            List of floats representing the embedding
        """
        return self.encode(text)  # type: ignore[return-value]
