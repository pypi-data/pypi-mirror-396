# Copyright (C) 2025 nostalgiatan
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

"""
SeeSea Pro API Handlers - Pro API å¤„ç†å‡½æ•°

æä¾›é«˜çº§æœç´¢å’Œå¤„ç†åŠŸèƒ½
"""

import asyncio
import json
import signal
import traceback
from typing import Dict, Optional, List
import time

from seesea_core import (
    PyCleaner,
    PyDatePage,
    PyVectorClient,
)  # type: ignore[import-untyped]
from seesea.search import SearchClient
from seesea.Pro.url_to_markdown import UrlToMarkdownConverter

# äº‹ä»¶é©±åŠ¨çš„èµ„æºç®¡ç†


class ProHandlersResources:
    """Pro handlersèµ„æºç®¡ç†å™¨ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç®¡ç†èµ„æº"""

    def __init__(self):
        self.vector_client: Optional[PyVectorClient] = None
        self.vector_client_initialized: bool = False
        self.cleaning_queue = asyncio.Queue[tuple[PyDatePage, dict, str]]()
        self.processing_queue = asyncio.Queue[tuple[PyDatePage, dict, str]]()
        self.worker_tasks: List[asyncio.Task] = []
        self.should_exit: bool = False

    async def __aenter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡ï¼Œåˆå§‹åŒ–èµ„æº"""
        # è®¾ç½®ä¿¡å·å¤„ç†
        loop = asyncio.get_running_loop()
        for sig in [signal.SIGINT, signal.SIGTERM]:
            loop.add_signal_handler(sig, self._handle_exit)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡ï¼Œæ¸…ç†èµ„æº"""
        self._handle_exit()
        await self._cleanup()

    def _handle_exit(self):
        """å¤„ç†é€€å‡ºä¿¡å·"""
        print("ğŸ”„ æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
        self.should_exit = True

    async def _cleanup(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        # å–æ¶ˆå·¥ä½œä»»åŠ¡
        for task in self.worker_tasks:
            task.cancel()

        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        if self.worker_tasks:
            await asyncio.gather(*self.worker_tasks, return_exceptions=True)

        # å…³é—­å‘é‡å®¢æˆ·ç«¯
        if self.vector_client_initialized and self.vector_client:
            try:
                self.vector_client.close()
                print("âœ… å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯å·²å…³é—­")
            except Exception as e:
                print(f"âŒ å…³é—­å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯å¤±è´¥: {e}")
            finally:
                self.vector_client = None
                self.vector_client_initialized = False

        print("âœ… æ‰€æœ‰èµ„æºå·²æ¸…ç†å®Œæˆ")


# å…¨å±€èµ„æºå®ä¾‹
_resources = None


# è·å–å…¨å±€èµ„æºå®ä¾‹
def get_resources() -> ProHandlersResources:
    """è·å–å…¨å±€èµ„æºå®ä¾‹"""
    global _resources
    if _resources is None:
        _resources = ProHandlersResources()
    return _resources


# åˆå§‹åŒ–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
async def initialize_vector_client(config: Optional[Dict] = None):
    """
    åˆå§‹åŒ–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯

    Args:
        config: å‘é‡æ•°æ®åº“é…ç½®

    Raises:
        RuntimeError: å¦‚æœæ— æ³•è¿æ¥åˆ°å‘é‡æ•°æ®åº“
    """
    resources = get_resources()

    if resources.vector_client_initialized:
        return

    # é…ç½®å·²ç»ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå‘é‡å®¢æˆ·ç«¯åˆå§‹åŒ–ä¼šå¤„ç†è¿æ¥
    # æˆ‘ä»¬ä¸å†éœ€è¦ç¡¬ç¼–ç ç«¯å£æ£€æŸ¥ï¼Œå› ä¸ºå‘é‡å®¢æˆ·ç«¯ä¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
    print("ğŸ” è·³è¿‡ç¡¬ç¼–ç ç«¯å£æ£€æŸ¥ï¼Œå‘é‡å®¢æˆ·ç«¯å°†ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®")

    # ç¡®ä¿æ‰€æœ‰åŠŸèƒ½çœŸæ­£è¿è¡Œï¼Œæ²¡æœ‰å›é€€
    print("ğŸ” æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯...")

    # ç›´æ¥åˆå§‹åŒ–å‘é‡å®¢æˆ·ç«¯ï¼Œè®©å®ƒæŠ›å‡ºå®é™…é”™è¯¯
    # æ³¨æ„ï¼šæ ¹æ®é”™è¯¯ä¿¡æ¯ï¼Œå‘é‡å­˜å‚¨é…ç½®éœ€è¦Qdrantçš„é…ç½®
    # è¿™é‡Œæˆ‘ä»¬ç›´æ¥åˆ›å»ºå®¢æˆ·ç«¯ï¼Œè®©å®ƒå°è¯•è¿æ¥åˆ°QdrantæœåŠ¡
    # å¦‚æœå¤±è´¥ï¼Œä¼šæŠ›å‡ºæ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
    try:
        # ç›´æ¥åˆ›å»ºå‘é‡å®¢æˆ·ç«¯ï¼Œé…ç½®ä»é…ç½®æ–‡ä»¶è¯»å–
        resources.vector_client = PyVectorClient.new()
        resources.vector_client_initialized = True
        print("âœ… å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        raise RuntimeError(
            f"æ— æ³•åˆå§‹åŒ–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯ï¼Œè¯·ç¡®ä¿QdrantæœåŠ¡æ­£åœ¨è¿è¡Œä¸”é…ç½®æ­£ç¡®: {e}"
        ) from e


# è·å–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
async def get_vector_client() -> Optional[PyVectorClient]:
    """
    è·å–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯ï¼Œå¦‚æœæœªåˆå§‹åŒ–åˆ™åˆå§‹åŒ–

    Returns:
        Optional[PyVectorClient]: å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
    """
    resources = get_resources()
    if not resources.vector_client_initialized:
        await initialize_vector_client()
    return resources.vector_client


# æœç´¢ç»“æœå¤„ç†ä»»åŠ¡
async def process_search_results(results: Dict, query: str, page: int, page_size: int):
    """
    å¤„ç†æœç´¢ç»“æœ

    Args:
        results: æœç´¢ç»“æœ
        query: æœç´¢æŸ¥è¯¢
        page: é¡µç 
        page_size: æ¯é¡µç»“æœæ•°
    """
    resources = get_resources()
    print(f"ğŸ” å¼€å§‹å¤„ç†æœç´¢ç»“æœ: æŸ¥è¯¢='{query}', é¡µç ={page}, æ¯é¡µç»“æœæ•°={page_size}")

    # 1. ä½¿ç”¨å¼‚æ­¥å¤„ç†æ¯ä¸ªURLï¼Œæé«˜æ•ˆç‡
    async def process_single_url(result):
        url = result.get("url")
        if not url:
            return

        try:
            # 2. ä½¿ç”¨UrlToMarkdownConverterè·å–URLçš„æ­£æ–‡ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            async with UrlToMarkdownConverter() as converter:
                convert_result = await converter.convert(url)

            # æ£€æŸ¥è½¬æ¢æ˜¯å¦æˆåŠŸ
            if not convert_result.get("success", False):
                print(f"âŒ URLè½¬æ¢å¤±è´¥ {url}: {convert_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return

            # æå–markdownå†…å®¹
            md_content = convert_result.get("markdown", "")
            print(f"ğŸ“„ æˆåŠŸè·å–URLå†…å®¹: {url}")

            # 3. åœ¨å‘é‡æ•°æ®åº“ä¸­ç²¾å‡†æœç´¢URL
            vector_client = await get_vector_client()

            if vector_client:
                # å‘é‡æ•°æ®åº“æœç´¢
                vector_results = vector_client.search_by_url(url, limit=1)
                if vector_results and len(vector_results) > 0:
                    print(f"ğŸ” åœ¨å‘é‡æ•°æ®åº“ä¸­æ‰¾åˆ°URL: {url}")

            # 4. åˆ›å»ºDatePageå¯¹è±¡
            current_time = time.time()
            date_page = PyDatePage(
                url=url,
                time=current_time,
                description=result.get("description", ""),
                source_data=md_content,
            )
            print(f"ğŸ“ åˆ›å»ºDatePageå¯¹è±¡: {url}")

            # 5. å°†DatePageæ”¾å…¥æ¸…ç†é˜Ÿåˆ—ï¼Œå¼‚æ­¥å¤„ç†
            await resources.cleaning_queue.put((date_page, result, query))
            print(f"ğŸ“¥ å·²å°† {url} æ”¾å…¥æ¸…ç†é˜Ÿåˆ—")

        except Exception as e:
            print(f"âŒ å¤„ç†URL {url} å¤±è´¥: {e}")
            # ä¸å†è·³è¿‡ï¼Œè€Œæ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œç¡®ä¿æ•´ä¸ªæµç¨‹çœŸæ­£è¿è¡Œ
            raise RuntimeError(f"å¤„ç†URL {url} å¤±è´¥: {e}") from e

    # å¹¶å‘å¤„ç†æ‰€æœ‰URL
    tasks = [process_single_url(result) for result in results.get("results", [])]
    if tasks:
        await asyncio.gather(*tasks, return_exceptions=False)


# æ¸…ç†ä»»åŠ¡å¤„ç†å™¨
async def cleaning_worker():
    """
    æ¸…ç†ä»»åŠ¡å·¥ä½œå™¨ - å¤„ç†æ¸…ç†é˜Ÿåˆ—ä¸­çš„DatePageå¯¹è±¡
    """
    resources = get_resources()
    # åˆå§‹åŒ–æ¸…æ´—å™¨ - æä¾›max_lines_per_blockå‚æ•°
    cleaner = PyCleaner(50)

    while not resources.should_exit:
        try:
            # ä½¿ç”¨è¶…æ—¶è·å–é˜Ÿåˆ—é¡¹ï¼Œå…è®¸å®šæœŸæ£€æŸ¥é€€å‡ºæ ‡å¿—
            date_page, result, query = await asyncio.wait_for(
                resources.cleaning_queue.get(), timeout=1.0
            )
            print(f"ğŸ§¹ å¼€å§‹æ¸…ç†: {date_page.url()}")

            # 6. è°ƒç”¨cleaningå‡½æ•°è¿›è¡Œé¢„å¤„ç†
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨blockingæ–¹å¼è°ƒç”¨ï¼Œå®é™…ç”Ÿäº§ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¼‚æ­¥å¤„ç†
            # ä½†æ ¹æ®è¦æ±‚ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼‚æ­¥æ‰”ä»»åŠ¡åå°ç­‰å®Œæˆï¼Œä¸å¿…åŒæ­¥ç­‰å¾…
            is_unchanged = False
            try:
                # è°ƒç”¨cleaningå‡½æ•°
                date_page.cleaning(cleaner)
                is_unchanged = True  # å‡è®¾æˆåŠŸï¼Œå®é™…éœ€è¦æ ¹æ®è¿”å›å€¼åˆ¤æ–­
            except Exception as e:
                print(f"âŒ æ¸…ç†å¤±è´¥ {date_page.url()}: {e}")
                resources.cleaning_queue.task_done()
                continue

            if is_unchanged:
                # 7. å¦‚æœå“ˆå¸Œä¸€è‡´ï¼Œç›´æ¥å®Œæˆä»»åŠ¡
                print(f"âœ… å“ˆå¸Œä¸€è‡´ï¼Œè·³è¿‡å¤„ç†: {date_page.url()}")
                resources.cleaning_queue.task_done()
                continue

            # 8. å¦åˆ™å°†ä»»åŠ¡æ”¾å…¥å¤„ç†é˜Ÿåˆ—
            await resources.processing_queue.put((date_page, result, query))
            print(f"ğŸ“¤ å·²å°† {date_page.url()} æ”¾å…¥å¤„ç†é˜Ÿåˆ—")

            resources.cleaning_queue.task_done()
        except asyncio.TimeoutError:
            # è¶…æ—¶ï¼Œç»§ç»­å¾ªç¯æ£€æŸ¥é€€å‡ºæ ‡å¿—
            continue
        except Exception as e:
            print(f"âŒ æ¸…ç†å·¥ä½œå™¨é”™è¯¯: {e}")
            try:
                resources.cleaning_queue.task_done()
            except ValueError:
                # é˜Ÿåˆ—å·²ç»ç©ºäº†ï¼Œå¿½ç•¥
                pass
    print("ğŸ§¹ æ¸…ç†å·¥ä½œå™¨å·²é€€å‡º")


# å¤„ç†ä»»åŠ¡å¤„ç†å™¨
async def processing_worker():
    """
    å¤„ç†ä»»åŠ¡å·¥ä½œå™¨ - å¤„ç†å¤„ç†é˜Ÿåˆ—ä¸­çš„DatePageå¯¹è±¡
    """
    resources = get_resources()
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embedder = None
    try:
        from seesea.Pro.llm.embeddings.llama_cpp_embeddings import LlamaCppEmbedder

        embedder = LlamaCppEmbedder()
        print(f"âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œç»´åº¦: {embedder.get_dimension()}")
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

    while not resources.should_exit:
        try:
            # ä½¿ç”¨è¶…æ—¶è·å–é˜Ÿåˆ—é¡¹ï¼Œå…è®¸å®šæœŸæ£€æŸ¥é€€å‡ºæ ‡å¿—
            date_page, result, query = await asyncio.wait_for(
                resources.processing_queue.get(), timeout=1.0
            )
            print(f"ğŸ“Š å¼€å§‹å¤„ç†: {date_page.url()}")

            # 9. ä½¿ç”¨åµŒå…¥æ¨¡å‹æ¨¡å—å¯¹æ•°æ®å—è¿›è¡Œå‘é‡åŒ–
            try:
                # è·å–æ•°æ®å—
                data_blocks = date_page.data_blocks()
                vectors = []

                # ä½¿ç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹è¿›è¡Œå‘é‡åŒ–
                # è·å–æ‰€æœ‰æ•°æ®å—å†…å®¹
                block_contents = [block.content() for block in data_blocks]

                # ä½¿ç”¨åµŒå…¥æ¨¡å‹ç”Ÿæˆå‘é‡
                vectors = embedder.encode(block_contents)
                print(f"ğŸ”¢ ç”Ÿæˆå‘é‡: {len(vectors)}ä¸ªå‘é‡ï¼Œæ¯ä¸ªç»´åº¦: {embedder.get_dimension()}")

                # 10. ä½¿ç”¨cleanerçš„èšç¾¤ç®—æ³•è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†
                # æ³¨æ„ï¼šcleanerå·²ç»åœ¨cleaningè¿‡ç¨‹ä¸­å®Œæˆäº†èšç¾¤ç®—æ³•å¤„ç†

                # 11. å°†æ–°çš„ç»“æœç»“åˆDatePageå¯¹è±¡å†™å…¥å‘é‡æ•°æ®åº“
                vector_client = await get_vector_client()

                # ç¡®ä¿å‘é‡å®¢æˆ·ç«¯å¯ç”¨
                if vector_client is None:
                    raise RuntimeError("âŒ å‘é‡å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•å†™å…¥å‘é‡æ•°æ®åº“")

                # å‡†å¤‡å†™å…¥æ•°æ®
                metadata = {
                    "url": date_page.url(),
                    "title": result.get("title", ""),
                    "description": result.get("description", ""),
                    "hash": str(date_page.hash()),
                    "last_update_time": date_page.last_update_time(),
                    "query": query,
                }

                # å†™å…¥å‘é‡æ•°æ®åº“
                vector_client.upsert_with_metadata(
                    vectors=vectors,
                    metadata=metadata,
                    data_blocks=[block.content() for block in data_blocks],
                )

                # 12. æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
                date_page.update_source_data(date_page.source_data())

                print(f"âœ… å¤„ç†å®Œæˆå¹¶å†™å…¥å‘é‡æ•°æ®åº“: {date_page.url()}")
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥ {date_page.url()}: {e}")

            resources.processing_queue.task_done()
        except asyncio.TimeoutError:
            # è¶…æ—¶ï¼Œç»§ç»­å¾ªç¯æ£€æŸ¥é€€å‡ºæ ‡å¿—
            continue
        except Exception as e:
            print(f"âŒ å¤„ç†å·¥ä½œå™¨é”™è¯¯: {e}")
            try:
                resources.processing_queue.task_done()
            except ValueError:
                # é˜Ÿåˆ—å·²ç»ç©ºäº†ï¼Œå¿½ç•¥
                pass
    print("ğŸ“Š å¤„ç†å·¥ä½œå™¨å·²é€€å‡º")


# å¯åŠ¨å·¥ä½œå™¨
async def start_workers():
    """
    å¯åŠ¨å¼‚æ­¥å·¥ä½œå™¨
    """
    resources = get_resources()
    # å¯åŠ¨æ¸…ç†å·¥ä½œå™¨
    resources.worker_tasks.append(asyncio.create_task(cleaning_worker()))
    # å¯åŠ¨å¤„ç†å·¥ä½œå™¨
    resources.worker_tasks.append(asyncio.create_task(processing_worker()))
    print("âœ… å¼‚æ­¥å·¥ä½œå™¨å¯åŠ¨æˆåŠŸ")


# å¤„ç†Proæœç´¢è¯·æ±‚
async def handle_pro_search(req: Dict) -> Dict:
    """
    å¤„ç†Proæœç´¢è¯·æ±‚

    Args:
        req: è¯·æ±‚ä¸Šä¸‹æ–‡

    Returns:
        Dict: å“åº”
    """
    try:
        # è§£æè¯·æ±‚å‚æ•°
        query_params = req.get("query_params", {})
        query = query_params.get("q", "")
        page = int(query_params.get("page", 1))
        page_size = int(query_params.get("page_size", 10))

        if not query:
            return {"status": 400, "body": json.dumps({"error": "Query is required"})}

        print(f"ğŸ” æ”¶åˆ°Proæœç´¢è¯·æ±‚: query='{query}', page={page}, page_size={page_size}")

        # 1. ä½¿ç”¨æ­£å¸¸æœç´¢å‡½æ•°è¿›è¡Œæœç´¢
        search_client = SearchClient()
        results = search_client.search(query=query, page=page, page_size=page_size)

        # å°†æœç´¢ç»“æœè½¬æ¢ä¸ºå­—å…¸
        results_dict: Dict = {
            "query": results.query,
            "results": [
                {
                    "title": item.title,
                    "url": item.url,
                    "description": item.content,
                    "score": getattr(item, "score", 0),
                }
                for item in results.results
            ],
            "total_count": results.total_count,
            "cached": results.cached,
            "query_time_ms": results.query_time_ms,
            "engines_used": results.engines_used,
        }

        # 2. åŒæ­¥å¤„ç†æœç´¢ç»“æœï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½ç»è¿‡å‘é‡å¤„ç†
        await process_search_results(results_dict, query, page, page_size)

        # 3. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼Œç”ŸæˆæŸ¥è¯¢å‘é‡
        from seesea.Pro.llm.embeddings.llama_cpp_embeddings import LlamaCppEmbedder

        embedder = LlamaCppEmbedder()
        query_vector = embedder.encode([query])[0]

        # 4. ä½¿ç”¨å‘é‡æ•°æ®åº“æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼Œè·å–ç›¸å…³æ€§åˆ†æ•°
        vector_client = await get_vector_client()
        if vector_client:
            vector_results = vector_client.search(query_vector, limit=page_size)

            # 5. æ ¹æ®å‘é‡æœç´¢ç»“æœé‡æ–°æ’åºåŸå§‹æœç´¢ç»“æœ
            # åˆ›å»ºURLåˆ°ç›¸å…³æ€§åˆ†æ•°çš„æ˜ å°„
            url_score_map = {}
            for result in vector_results:
                # è·å–ç»“æœä¸­çš„URLå’Œåˆ†æ•°
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„PyVectorClient.searchè¿”å›ç»“æ„è°ƒæ•´
                result_dict = result.as_dict() if hasattr(result, "as_dict") else {}
                url = result_dict.get("url", "")
                score = result_dict.get("score", 0.0)
                if url:
                    url_score_map[url] = score

            # é‡æ–°æ’åºåŸå§‹æœç´¢ç»“æœ
            sorted_results = sorted(
                results_dict["results"],
                key=lambda x: url_score_map.get(x["url"], 0.0),
                reverse=True,
            )

            # æ›´æ–°ç»“æœå’Œåˆ†æ•°
            results_dict["results"] = sorted_results

        # 6. è¿”å›é‡æ–°æ’åºåçš„ç»“æœ
        return {"status": 200, "body": json.dumps(results_dict, ensure_ascii=False)}
    except Exception as e:
        print(f"âŒ Proæœç´¢è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        return {"status": 500, "body": json.dumps({"error": str(e)})}


# æ·»åŠ Proè·¯ç”±åˆ°APIæœåŠ¡å™¨
def add_pro_routes(server):
    """
    æ·»åŠ Proè·¯ç”±åˆ°APIæœåŠ¡å™¨

    Args:
        server: APIæœåŠ¡å™¨å®ä¾‹
    """
    # æ³¨å†ŒProæœç´¢è·¯ç”±
    server.add_pro_route("/search", handle_pro_search, method="GET")
    server.add_pro_route("/search", handle_pro_search, method="POST")

    print("âœ… Pro APIè·¯ç”±æ³¨å†Œå®Œæˆ")


# æ¨¡å—åˆå§‹åŒ–
async def initialize_pro_handlers():
    """
    åˆå§‹åŒ–Pro handlers
    """
    try:
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
        await initialize_vector_client()
        print("âœ… Pro handlersåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ Pro handlersåˆå§‹åŒ–å¤±è´¥ï¼Œä½†æœåŠ¡å™¨ä»å°†ç»§ç»­è¿è¡Œ: {e}")
        print(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        # å³ä½¿ProåŠŸèƒ½åˆå§‹åŒ–å¤±è´¥ï¼ŒæœåŠ¡å™¨ä»å°†ç»§ç»­è¿è¡Œ
        # åªè®°å½•é”™è¯¯ï¼Œä¸æŠ›å‡ºå¼‚å¸¸

    # å¯åŠ¨å¼‚æ­¥å·¥ä½œå™¨
    try:
        await start_workers()
    except Exception as e:
        print(f"âš ï¸ å¼‚æ­¥å·¥ä½œå™¨å¯åŠ¨å¤±è´¥ï¼Œä½†æœåŠ¡å™¨ä»å°†ç»§ç»­è¿è¡Œ: {e}")


# å‘½ä»¤è¡Œæ¨¡å¼ä¸‹çš„æ¸…ç†å‡½æ•°
async def cleanup_command_line():
    """
    å‘½ä»¤è¡Œæ¨¡å¼ä¸‹çš„æ¸…ç†å‡½æ•°
    """
    resources = get_resources()
    await resources._cleanup()
