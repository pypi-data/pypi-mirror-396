\title{Model-free selective inference under covariate shift\\ via weighted conformal p-values} 

\begin{abstract}
  This paper introduces novel weighted conformal p-values and methods for model-free selective inference. The problem is as follows: given test units with covariates $X$ and missing responses $Y$, how do we select units for which the responses $Y$ are larger than user-specified values while controlling the proportion of false positives? Can we achieve this without any modeling assumptions on the data and without any restriction on the model for predicting the responses? Last, methods should be applicable when there is a covariate shift between training and test data, which commonly occurs in practice. 

  We answer these questions by first leveraging any prediction model to produce a class of well-calibrated weighted conformal p-values, which control the type-I error in detecting a large response. These p-values cannot be passed onto classical multiple testing procedures since they may not obey a well-known positive dependence property. Hence, we introduce weighted conformalized selection (WCS), a new procedure which controls false discovery rate (FDR) in finite samples. Besides prediction-assisted candidate selection, WCS (1) allows to infer multiple individual treatment effects, and (2) extends to outlier detection with inlier distributions shifts. We demonstrate performance via simulations and applications to causal inference, drug discovery, and outlier detection datasets.
\end{abstract}

Many scientific discovery and decision making tasks concern the selection 
of promising candidates from a potentially large pool. In drug discovery, 
scientists aim to find
drug candidates---from a huge 
chemical space of molecules or compounds---with sufficiently 
high binding affinity to a target.  
College admission officers or hiring managers seek applicants with a high potential 
of excellence. In healthcare, one would like to allocate resources to individuals 
who would benefit the most. The list of such examples is endless.  
In all these problems, a decision maker would like to identify those units for which   
an unknown outcome  of interest (affinity for a target molecule/job performance/reduction in stress level) takes on a large value. 

Machine learning has shown tremendous potential for accelerating these 
resource- and time-consuming processes whereby 
predictions for the unknown outcomes are used to 
shortlist promising candidates before any further in-depth investigation~\cite{lavecchia2013virtual,carpenter2018machine,sajjadiani2019using}.  
This is a sharp departure from traditional strategies that either 
directly evaluate the outcomes via a physical screening to determine drug binding 
affinities, or ``predict'' the unknown outcomes via human judgement 
in healthcare or job hiring applications. 
In the new paradigm, a prediction model draws evidence from training data
to inform values the unknown outcome may take on given the features of a unit. 

If the intent is thus to use machine learning to screen candidates, 
we would need to understand and control 
the uncertainty in black-box prediction models as a selection tool. 
First, gathering evidence for new discoveries from past instances 
is challenging as they are not necessarily similar; i.e.~they may be drawn from distinct distributions.
Second, since a subsequent investigation applied to the shortlisted candidates 
is often resource intensive, it is meaningful to limit the \emph{error rate on the selected}. It is not so much the accuracy of the prediction that matters, rather the proportion of selected units not passing the test.  Keeping such a proportion at sufficiently low levels is a non-traditional criterion in typical prediction problems. 

This paper studies the reliable selection of promising units/individuals whose unknown outcomes 
are sufficiently large. We develop methods that apply in a model-free fashion---without making any modeling assumptions on the data generating process while controlling the expected proportion of false discoveries among the selected.



\subsection{Reliable selection under distribution shift}


 
Formally, suppose test samples $\{(X_{j},Y_{j})\}_{j\in\cD_\test}$ 
are drawn i.i.d.~from an unknown distribution $Q$, whose outcomes $\{Y_j\}_{j\in\cD_\test}$ 
are unobserved. Our goal is to identify a subset $\cR\subseteq \cD_\test$ 
with outcomes above some known, potentially random, 
thresholds $\{c_{ j}\}_{j\in \cD_\test}$. To draw statistical evidence for large outcomes in $\cD_\test$,  
we assume we hold an independent set of 
i.i.d.~calibration data $\{(X_i,Y_i)\}_{i\in \cD_\calib}$ whose outcomes  
are, this time, observed. 
 
Before we begin our discussion, we note that
a challenge that underlies almost all applications is that samples in $\cD_\calib$ 
are often different from those in $\cD_\test$. 
That is, in many cases, a point $(X_{j},Y_{j})$ in $\cD_\test$ is sampled from $Q$ while a point $(X_{i},Y_{i})$ in $\cD_\calib$ is sampled from a different distribution $P$. 
We elaborate on the distribution shift issue by considering two motivating examples. 

\vspace{-0.3em}


\paragraph{Drug discovery.} %\ejc{The next sentence makes no sense.} 
{Virtual screening, which identifies viable drug candidates 
using predicted affinity scores from supervised machine learning models, 
is increasingly popular in early stages of drug discovery~\cite{huang2007drug,koutsoukas2017deep,vamathevan2019applications,dara2021machine}. 
In this application, $\cD_\calib$ is the set of drugs with known affinities, while $\cD_\test$ 
is the set of new drugs whose binding affinities are of interest.}  
In practice, there usually is a distribution shift between the two batches: for instance, an experimenter may favor a 
specific structure 
when choosing candidates to physically screen their true binding affinities~\cite{polak2015early}. Since physically screened drugs are subsequently added to the calibration dataset $\cD_\calib$, they may not be representative of the new drugs in $\cD_\test$.
% 
Accounting for such distribution shifts when applying virtual screening is crucial for 
the reliability of the whole drug discovery process, and 
has attracted recent attention~\cite{Krstajic2021}.

\vspace{-0.3em}

\paragraph{College admission.}
A college may be interested in prospective students who are likely to 
graduate after four years of undergraduate education (a binary outcome of interest) 
among all applicants in $\cD_\test$ as to maintain a reasonable graduation rate. 
The issue is that $\cD_\calib$
usually consists of students who \emph{were} admitted 
in the past as the outcome is only known for these students. 
Since previous cohorts may have been selected by applying various admission criteria, 
the distribution of the training data is different from that of the new applicants 
even if the distribution of applicants does not much vary over the years. 
Similar concerns apply to job hiring when using prediction models to select suitable applicants~\cite{faliagka2012application,shehu2016adaptive}; the way an institution chooses to record the outcomes of former candidates 
can lead to a 
discrepancy between the documented candidates and those under 
consideration.

\vspace{0.5em}

In this paper, 
we propose to address the distribution shift issue 
by considering a scenario where the shift 
can be entirely attributed to observed covariates; this is 
commonly referred to as   
a covariate shift in the literature~\cite{sugiyama2007covariate,tibshirani2019conformal}. 
% 
Concretely, we assume that 
there is 
some function $w\colon \cX\to \RR^+$ such that 
\#\label{eq:cov_shift}
\frac{\ud Q}{\ud P}(x,y) = w(x),\quad  P\textrm{-almost surely},
\#
where $ \ud Q/\ud P $ denotes the Radon-Nikodym derivative. 
% 
While all kinds of distribution shift could happen in practice, 
covariate shift---widely used to characterize  
distribution shifts  
caused by selection on known features---covers many important cases. 
In drug discovery, if the preference for choosing which samples 
to screen only depends upon the covariates, e.g.~the sequence of amino acids of the compound or the physical properties of the compound, we are dealing with the covariate shift \eqref{eq:cov_shift}.
In college admission or job hiring, 
if a specific type of applicant---characterized by certain features---were favored in 
previous admission/hiring cycles, such a preference would 
yield the shift \eqref{eq:cov_shift} between previous students/employees and current applicants. 

Returning to our decision problem,
we are interested in finding as many units $j\in \cD_\test$ as possible 
with $Y_{j}>c_j$, while controlling the false discovery rate (FDR) 
below some pre-specified level $q\in (0,1)$. 
The FDR is the expected fraction  of false discoveries, where a false discovery is a selected unit for which $Y_j \le c_j$, i.e., the outcome turns out to fall below the threshold. 
Formally, we wish to find $\cR$ obeying 
\#\label{eq:fdr}
\fdr := \EE\Bigg[  \frac{\sum_{j\in\cD_\test} \ind\{j\in \cR,Y_{j}\leq c_{ j}\}}{\max\{1,|\cR|\}}  \Bigg] \leq q, 
\#
where $q$ is the nominal target FDR level. The expectation in \eqref{eq:fdr} is taken   
over both the new test samples and the calibration data/screening procedure. 
The FDR quantifies the tradeoff 
between resources dedicated to the shortlisted candidates 
and the benefits from true positives~\cite{benjamini1995controlling}. 
By controlling the FDR, we ensure that a sufficiently large proportion of 
follow-up resources---such as clinical trials 
in drug discovery, human evaluation of student profiles and interviews---to confirm predictions from machine learning models are 
devoted to interesting candidates. 

The problem of FDR-controlled selection with machine learning predictions 
has been studied in an earlier paper~\cite{jin2022selection} 
under the assumption of no distribution shift. 
This means the new instances 
are exchangeable with the known calibration data, which, as we discussed above, 
is rarely the case in practice. %\ejc{Say this requires radically novel ideas.}
{Therefore, there is a pressing need for novel techniques 
to address the distribution shift issue.}

\vspace{-0.3em}
 
\subsection{Preview of theoretical contributions}
 
{Our strategy consists in (1) constructing a calibrated confidence measure (p-value), which applies to any predictive model, for detecting a large unknown outcome, and (2) in employing multiple testing ideas to build the selection set from test p-values. }  

\vspace{-0.5em}
\paragraph{Weighted conformal p-value: a calibrated confidence measure.}
We introduce random variables $\{p_j\}_{j\in \cD_\test}$, which resemble p-values in the sense that for each $j\in \cD_\test$, they obey 
\#\label{eq:general_pvalue}
\PP(p_j \leq t, ~ Y_{j} \leq c_{j} ) \leq t,\quad \textrm{for all}~t\in [0,1],  
\# 
where the probability is over both the calibration data and the test sample 
$(X_{j},Y_{j},c_{j})$. 
This is similar to the definition of a classical p-value, hence, we call $p_j$ a``p-value''. With this, for any $\alpha\in(0,1)$, selecting $j$ if and only if $p_j\leq \alpha$ 
controls the type-I error below $\alpha$ (the chance that $Y_j$ is below threshold is at most $\alpha$).  
Note however that~\eqref{eq:general_pvalue} is perhaps an unconventional notion of 
type-I error, since it accounts for the randomness in the ``null hypothesis'' 
$H_{j}\colon Y_{j}\leq c_{j}$.

We construct $p_j$ using ideas from conformal prediction~\cite{vovk2005algorithmic,tibshirani2019conformal}. 
Take any monotone 
function $V\colon \cX\times\cY\to \RR$ (see Definition \ref{def:monotone}) 
built upon any prediction model that is independent of $\cD_\calib$ and 
$\cD_\test$. A concrete instance is 
$V(x,y) = y - \hat\mu(x)$, where $\hat\mu\colon \cX\to \cY$ 
is any predictor trained on a held-out dataset. 
With scores $V_i = V(X_i,Y_i)$,  $i\in \cD_\calib$, and 
weight function $w(\cdot)$ given by~\eqref{eq:cov_shift}, we construct $p_j$ to be approximately equal to  $\hat{F}(\hat{V}_j)$, 
where  $\hat{F}(\cdot)$ is the cumulative distribution function (c.d.f.) of the empirical distribution 
\[
\frac{ \sum_{i \in \cD_\calib} \,  w(X_i) \, \delta_{V_i} }{  \sum_{i \in \cD_\calib} w(X_i) }.
\] 
While the precise definition of $p_j$ is in  Section~\ref{sec:pval}, Figure~\ref{fig:pvalue} visualizes the idea. 
 
Intuitively, $p_j$ contrasts the value of $\hat{V}_{j}=V(X_{j},c_{j})$ against the distribution of the unknown `oracle' score $V_j=V(X_j,Y_j)$. The latter is 
approximated by $\hat{F}(\cdot)$, which reweights the training data 
with weights reflecting the covariate shift~\eqref{eq:cov_shift}. As such,  
$\hat{F}(V_j)$ is approximately uniformly distributed in $[0,1]$. 
Thus, a notably small value of $p_j\approx \hat{F}(\hat{V}_j)$ relative to the uniform distribution informs that $\hat{V}_j$ is smaller than what is typical of $V_j$, which further 
suggests evidence for $Y_j\geq c_j$. 
Leveraging conformal prediction techniques, such high-level ideas are made exact to achieve~\eqref{eq:general_pvalue}.
 

\begin{figure}[htbp]
    \centering 
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figs/hist_plot.pdf} 
    \end{subfigure} \hspace{0.5em}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figs/ecdf_plot_edit.pdf} 
    \end{subfigure} 
\caption{Visualization of weighted conformal p-values.
Left: Distribution of scores in the calibration set (gray) and individual values of $w(X_i)$ (orange) for $i\in\cD_\calib$. Right: Weighted empirical c.d.f.~of $\{V_i\}_{i\in \cD_{\calib}}$; the red dashed lines illustrate how $p_j$ is computed from $\hat{V}_j$, in which $j\in \cD_\test$.  
}
\label{fig:pvalue}
\end{figure} 
 
\vspace{-0.5em}
\paragraph{Intricate dependence among weighted conformal p-values.}

Going beyond per-unit type-I error control, 
selecting from multiple test samples 
requires dealing with multiple p-values $\{p_j\}_{j\in \cD_\test}$. 
A natural idea is then to use multiple testing ideas as if 
$\{p_j\}$ were classical p-values. 
% 
However, the situation is complicated here because  $\{p_j\}_{j\in \cD_\test}$ 
all depend on the same set of calibration data, 
and, in sharp contrast to classical multiple testing, 
there is another source of randomness in the unknown outcomes. 

Our second theoretical result shows the mutual dependence among 
multiple p-values is particularly challenging. 
It states that   
the favorable positive dependence structure 
among p-values, which was the key to FDR control 
with other 
p-values derived with conformal prediction ideas~\cite{bates2021testing,jin2022selection}, 
can be violated in the presence of data-dependent weights in \eqref{eq:def_wcpval_rand}. 
As a result, it remains unclear whether applying existing multiple 
testing procedures controls the FDR in finite samples. 

\vspace{-0.5em}
\paragraph{Weighted Conformalized Selection (WCS).} 
Last, we introduce WCS, 
a new procedure 
that achieves finite-sample FDR control under distribution shift. 
The idea is to calibrate the selection threshold for each $j\in \cD_\test$ 
using a set of ``auxiliary'' p-values $\{\bar{p}_\ell^{(j)}\}_{\ell\in \cD_\test}$. 
These p-values  happen to obey 
a special conditional independence property, namely,  
\$
\bar p_j^{(j)}  \indep \{\bar p_\ell^{(j)}\}_{\ell\neq j} \biggiven \cZ_j,
\$
where $\cZ_j$ is the unordered set of the calibration data 
and the $j$-th test sample. 
We show that FDR control is valid regardless 
of the machine learning model used in constructing $V(\cdot,\cdot)$, 
and applies to a wide range of scenarios where the cutoffs $c_{j}$ can themselves be random variables. 

\subsection{Broader scope}

Our methods happen to be applicable to additional statistical inference problems. 
 
\vspace{-0.3em}

\paragraph{Multiple counterfactual inference.}
The individual treatment effect (ITE) is a random variable 
that characterizes the difference between an individualâ€™s outcome 
when receiving a treatment versus not, whose inference relies on comparing the observed outcome 
with the counterfactual~\cite{imbens2015causal,lei2021conformal,jin2023sensitivity}. To infer the counterfactual of a unit under treatment, 
the calibration data are units who do not receive the treatment.  
If the treatment assignment depends on the covariates, 
then there may be, and usually will be, a covariate shift between treated and control units. 
We discuss this problem in Section~\ref{sec:ite}. 

\vspace{-0.3em}
\paragraph{Outlier detection.} Imagine we have access to i.i.d.~inliers $\{Z_i\}_{i\in \cD_\calib}$
from an unknown distribution $P$. Then outlier detection~\cite{bates2021testing} 
seeks to find outliers among new samples $\{Z_j\}_{j\in \cD_\test}$, i.e., those which 
do not follow the inlier distribution $P$. 
A potential application is fraud detection in financial transactions, 
where controlling the FDR ensures reasonable resource allocation  
in follow-up inquiries.  
However,  financial behavior varies with users and it is possible that normal transactions of interest follow different distributions across different populations. 
A related problem is to detect 
concept drifts; that is, allowing a subset of features $X\subseteq Z$ 
to follow a distinct distribution, and only detecting test samples 
whose $Z\given X$ distribution differs from $P_{Z\given X}$. We discuss this problem in Section~\ref{sec:outlier}.  

As in~\eqref{eq:general_pvalue}, 
the FDR~\eqref{eq:fdr} is marginalized over the randomness in the hypotheses. 
To this end, WCS requires  all 
$\{X_{n+j},Y_{n+j}\}_{j=1}^m$ to be i.i.d.~draws from a super-population. 
To relax this, we  generalize our methods to 
control a notion of FDR that is conditional on the random hypotheses. 
This is useful for handling imbalanced data in classification, and  
the aforementioned outlier detection problem. 

\subsection{Data and code}
 
An R package, \texttt{ConfSelect}, that implements Weighted Conformalized Selection (as well as the version without weights) together with code to reproduce all experiments in this paper, can be found in the GitHub repository \href{https://github.com/ying531/conformal-selection}{https://github.com/ying531/conformal-selection}.

\subsection{Related work}

Our methods build upon the conformal inference framework~\cite{vovk2005algorithmic} {to infer unknown outcomes}. There, 
the theoretical guarantee for conformal prediction sets 
is usually marginally valid over the randomness in 
a single test point. 
As discussed in~\cite{jin2022selection},  
one might be interested in multiple test samples
simultaneously; in such situations, standard conformal prediction tools 
are insufficient.   

This work is connected to a recent line of research on 
selective inference issues arising in predictive inference. 
Some works use  
conformal p-values (whose definition differs from ours) 
with `full' observations $\{(X_{j},Y_j)\}_{j\in \cD_\test}$---that is, the response is observed---for outlier detection, i.e., for identifying whether $(X_{j},Y_j)$ 
follows the same distribution as the calibration data 
$\{(X_{i},Y_i)\}_{i\in \cD_\calib}$~\cite{bates2021testing,liang2022integrative,marandon2022machine}. 
Whereas these methods do not consider distribution shift between 
calibration and test inliers, 
Section~\ref{sec:outlier} extends all this. 
Here, the response $Y_{n+j}$---the object of inference---is not observed and, therefore, the problem is completely different. Consequently, methods and techniques to deal with this are new.  
% 
Our focus on FDR control is similar in spirit to other recent papers; for instance, 
rather than marginal coverage of prediction sets,~\cite{fisch2022conformal} 
studies the number of false positives, 
and~\cite{bao2023selective} the 
miscoverage rate on selected units. 

Our methodology draws ideas from the multiple hypothesis testing literature, 
where the FDR is 
a popular type-I error criterion in exploratory analysis for controlling the 
proportion of `false leads' in follow-up studies~\cite{benjamini1995controlling,benjamini2001control}.  
This  paper is distinct from the conventional setting. 
First, testing a random outcome instead of a model parameter 
leads to a random null set and a complicated dependence structure. 
Second, our inference relies on (weighted)
exchangeability of the data while imposing no assumptions on their distributions. In contrast,  
a null hypothesis often 
specifies the distribution of test statistics or p-values. Later on, we shall also draw connections to a recent literature dealing with dependence in multiple testing, 
such as that concerned with e-values~\cite{vovk2021values,wang2020false} 
and conditional calibration of the BH procedure~\cite{fithian2020conditional}.  

The application of our method is related 
to a substantial literature that emphasizes 
the importance of uncertainty quantification in drug discovery~\cite{Norinder2014,svensson2017improving,Ahlberg2017current,svensson2018conformal,cortes2019concepts,wang2022improving}. 
Although existing works aim to employ conformal inference 
to assign confidence to machine prediction, 
this is often done  
in heuristic ways, which 
potentially invalidates the guarantees of conformal prediction due to selection issues. 
Instead, 
we provide here a principled approach to 
prioritizing drug candidates 
with clear and interpretable error control (i.e., guaranteeing a 
sufficiently high `hit rate'~\cite{wang2022improving}), 
and offer a potential solution to  
the widely recognized distribution shift problem~\cite{Krstajic2021,fannjiang2023novelty}.

The application to individual treatment effects in Section~\ref{sec:ite}
is connected to~\cite{caughey2021randomization};
the distinction is that they focus on a summary statistic such as 
a population quantile of the ITEs, 
while our method {detects individuals with positive ITEs.} 
Also related is~\cite{duan2021interactive},  
which tests individuals for positive treatment effects
with FDR control; however, in their setting a positive ITE 
means the whole distribution  of 
the control outcome is dominated by that of the treated outcome, 
whereas we
directly compare two random variables. 
Accordingly, our techniques are very different from these two references. 

\subsection{Construction of weighted conformal p-values}
 
Our weighted conformal p-values build upon 
any \emph{monotone} score function, defined as follows. 

\begin{definition}[Monotonicity]
    \label{def:monotone}
A score function $V(\cdot,\cdot)\colon \cX\times\cY\to \RR$ is monotone if 
$V(x,y)\leq V(x,y')$ holds for any $x\in \cX$ and any $y,y'\in \cY$ obeying $y\leq y'$. 
\end{definition}


Intuitively, the score function (often referred to as nonconformity score 
in conformal prediction~\cite{vovk2005algorithmic}) 
describes how well a hypothesized value $y\in \cY$ 
\emph{conforms} to the machine prediction. 
A popular and monotone nonconformity score 
is $V(x,y) = y-\hat\mu(x)$, where $\hat\mu\colon\cX\to \RR$ is 
a point prediction; other choices include those based on 
quantile regression~\cite{romano2019conformalized} or estimates of the conditional 
c.d.f.~\cite{chernozhukov2021distributional}. 

{From now on, write $\cD_\calib = \{1,\dots,n\}$ and $\cD_\test=\{n+1,\dots,n+m\}$. 
Compute $V_i=V(X_i,Y_i)$ for $i=1,\dots,n$  and 
$\hat{V}_{n+j}=V(X_{n+j},c_{n+j})$ for $j=1,\dots,m$. 
Our weighted conformal p-values are defined as  
\#\label{eq:def_wcpval_rand}
p_j = \frac{\sum_{i=1}^n w(X_i)\ind {\{V_i <\hat{V}_{n+j} \}}+  ( w(X_{n+j}) + \sum_{i=1}^n w(X_i)\ind {\{V_i = \hat{V}_{n+j} \}})\cdot U_j}{\sum_{i=1}^n w(X_i) + w(X_{n+j})},
\#
where $w(\cdot)$ is the covariate shift function in~\eqref{eq:cov_shift}, 
and  $U_j\iid \textrm{Unif}([0,1])$ are tie-breaking random variables. 
When $w(\cdot)\equiv 1$, our p-values reduce to the conformal p-values in~\cite{jin2022selection}, see Appendix~\ref{app:recap_unweighted}. 
We also refer readers to Appendix~\ref{app:connection_conformal} for 
the connection between 
our p-values and conformal prediction intervals.}



{Roughly speaking, with a monotone score, 
the p-value~\eqref{eq:def_wcpval_rand} 
measures how small $c_{n+j}$ is compared with 
typical values of $Y_{n+j}$, and provides calibrated 
evidence for a large outcome as expressed in~\eqref{eq:general_pvalue}. 
This can be derived using conformal inference theory~\cite{tibshirani2019conformal} and the monotocity of $V$. We include a formal result here for completeness, whose proof is 
in Appendix~\ref{app:lem_general_pval}.} 

\begin{lemma}\label{lem:general_pval}
The tail inequality~\eqref{eq:general_pvalue} holds 
if the covariate shift~\eqref{eq:cov_shift} holds and the score function is monotone.
\end{lemma}
Consequently, selecting a unit 
if $p_j \le \alpha$ controls the type-I error at level $\alpha$ for each $j\in \cD_\test$.  
We shall however see that dealing with multiple test samples 
is far more challenging.   
 

\subsection{Weighted conformal p-values are not PRDS}
\label{subsec:no_PRDS}

Applying multiple testing procedures 
to our p-values to obtain a selection set naturally comes to mind. Previous works 
applying the  Benjamini-Hochberg (BH) procedure~\cite{benjamini1995controlling} to p-values obtained via conformal inference ideas indeed control the FDR~\cite{bates2021testing,jin2022selection,marandon2022machine}. This is because the p-values obey a favorable dependence structure called 
\emph{positive dependence on a subset} (PRDS).
 

\begin{definition}[PRDS]
\label{def:prds}
A random vector $X=(X_1,\dots,X_m)$ is PRDS 
on a subset $\cI$ if for any $i\in \cI$ 
and any increasing set $D$, 
the probability $\PP(X\in D\given X_i=x)$ is increasing in $x$. 
\end{definition}
Above, a set $D\subseteq \RR^m$ is 
\emph{increasing} if $a\in D$ and $b\succeq a$ implies $b\in D$ ($b \succeq a$ means that all the components of $b$ are larger than or equal to those of $a$). It is well-known that the BH procedure 
controls the FDR when applied to PRDS p-values~\cite{benjamini2001control}.  
% 
The PRDS property among (unweighted) conformal p-values was first studied in
\cite{bates2021testing},\footnote{They also show the non-randomized unweighted conformal 
p-values are PRDS when the scores are almost surely distinct. 
We do not elaborate on this distinction as it is not the focus here.} 
and generalized in  
\cite{jin2022selection} 
to plug-in values $c_{n+j}$ in lieu of $Y_{n+j}$, thereby  
forming the basis for FDR control. 
% 
If the PRDS property were to hold for 
the weighted conformal p-values, then applying the BH procedure 
would also control the FDR. This is however not always the case; 
a constructive proof of Proposition~\ref{prop:counter_PRDS} is 
in Appendix~\ref{app:subsec_prds}. 

\begin{prop}\label{prop:counter_PRDS}
There exist a sample size $n$, a weight function $w\colon \cX\to \RR^+$, 
a nonconformity score $V\colon \cX\times\cY\to \RR$, 
and  distributions $P$ and $Q$ obeying~\eqref{eq:cov_shift}, 
such that 
the weighted conformal p-values~\eqref{eq:def_wcpval_rand} are not PRDS for 
$\{X_i,Y_i\}_{i=1}^n\iid P$ and $\{X_{n+j},Y_{n+j}\}_{j=1}^m\iid Q$. 
\end{prop} 

A little more concretely, we find that the PRDS property 
is likely to fail when 
the nonconformity scores are negatively 
correlated with the 
weights.  
Note that the dependence among conformal p-values 
arises from sharing the calibration data. 
Under exchangeability (i.e.~taking equal weightes $w(x)\equiv 1$), a smaller conformal p-value 
is (only) associated with larger calibration scores, 
and hence smaller p-values for other test points. 
However, with negatively associated  weights and scores, 
a smaller p-value may also be due to 
smaller calibration weights---instead of larger calibration scores---which 
implies that other p-values may take on larger values. 
 




\subsection{BH procedure controls FDR asymptotically}
 

Before introducing our new selection methods, 
we nevertheless show {\em asymptotic} FDR control using the BH procedure.   

\begin{theorem}\label{thm:fdr_asymp}
Suppose $w(\cdot)$ is uniformly bounded by a fixed constant, the covariate shift~\eqref{eq:cov_shift} holds, 
and $\{c_{n+j}\}_{j=1}^m$ are i.i.d.~random variables. 
Fix any $q\in(0,1)$, 
and let $\cR$ be the output of the BH procedure applied to 
$\{p_j\}_{j=1}^m$ in~\eqref{eq:def_wcpval_rand}. 
Then the following  results hold. 
\begin{enumerate}[(i)]
\item For any fixed $m$, 
it holds   that $\limsup_{n\to \infty} \fdr \leq q$.
\item Suppose $m,n\to \infty$. Under a mild technical condition (see Appendix~\ref{app:fdr_asymp}), $\limsup_{m,n\to\infty} \fdr \leq q$. Furthermore, the asymptotic FDR and power of BH  can be exactly characterized in this case.  
\end{enumerate} 
\end{theorem}
 

We provide the complete technical version of Theorem~\ref{thm:fdr_asymp} 
in Theorem~\ref{thm:fdr_asymp_full}, whose 
proof is in Appendix~\ref{app:thm_fdr_asymp}.
In fact, we prove that as $n\to \infty$, the weighted conformal p-values 
converge to i.i.d.~random variables that are dominated by the uniform distribution, 
which ensures asymptotic FDR control (see Appendix~\ref{app:thm_fdr_asymp} for details). 
The technical condition  we impose resembles that in~\cite{storey2004strong}, 
which ensures the existence of a limit point of the rejection threshold and enables the asymptotic analysis. 

We also  remark that 
the PRDS property is a sufficient, but not necessary, condition for 
the BH procedure to control the FDR.  
In fact, we will see  that 
the BH procedure with weighted conformal p-values  
empirically controls 
the FDR in all of our numerical experiments, hence it 
remains a reasonable option in practice.  

We now introduce a new 
multiple testing procedure, WCS, that 
controls the FDR in finite samples with weighted conformal p-values. 
Our method builds on the following p-values:  
\#\label{eq:weighted_pval}
{p}_j = \frac{\sum_{i=1}^n w(X_i)\ind{\{V_i < \hat{V}_{n+j} \}}+  w(X_{n+j}) }{\sum_{i=1}^n w(X_i) + w(X_{n+j})}.
\#
It is a slight modification of~\eqref{eq:def_wcpval_rand} up to 
random tie-breaking. The asymptotic results in 
Theorem~\ref{thm:fdr_asymp} also hold with these  
p-values as long as the distributions of the $V_i$'s and 
$\hat{V}_{n+j}$'s do not have point masses. 

\subsection{Weighted Conformalized Selection}
 
As before, we compute $V_i=V(X_i,Y_i)$ for $i=1,\dots,n$ 
and $\hat{V}_{n+j} = V(X_{n+j},c_{n+j})$ for $j=1,\dots,m$. 
For each $j$, we compute a set of auxiliary p-values 
\#\label{eq:mod_pval}
 {p}_{\ell}^{(j)} := \frac{\sum_{i=1}^n w(X_i) \ind {\{V_i < \hat{V}_{n+\ell}\}} + w(X_{n+j}) \ind {\{ \hat{V}_{n+j}<  \hat{V}_{n+\ell}\}} }{\sum_{i=1}^n w(X_i) + w(X_{n+j})}, \quad \ell\neq j.
\# 
Then, we let $\hat{\cR}_{j\to 0}$ be the rejection set 
of BH applied to 
$\{  p_1^{(j)},\dots,  p_{j-1}^{(j)}, 0,   p_{j+1}^{(j)},\dots,  p_{m}^{(j)}\}$ at the nominal level $q$. Note that $j\in \hat{\cR}_{j\to 0}$ by default, and  
set $s_j = \frac{q|\hat{\cR}_{j\to 0}  | }{m}$.  
We then compute the first-step rejection set $\cR^{(1)}:=\{j \colon   p_j \leq s_j\}$. 
Finally, we prune $\cR^{(1)}$ 
to obtain the final selection set $\cR$ 
using any of the following three methods. 
\begin{enumerate}[(a)]
\item \emph{Heterogeneous pruning}: 
generate i.i.d.~random variables $\xi_j \sim \textrm{Unif}[0,1]$, 
and set  
\#\label{eq:R_hete}
\cR := \cR_{\hete} = \Big\{ j\colon  p_j \leq s_j, ~ \xi_j |\hat\cR_{j\to 0} | \leq  r_{\hete}^*  \Big\},
\#
where 
$
r_\hete^* := \max\big\{ r\colon  \sum_{j=1}^m \ind {\{  p_j\leq s_j, \, \xi_j   |\hat\cR_{j\to 0}  | \leq r \}}   \geq r \big\}.
$
\item \emph{Homogeneous pruning}:  
generate an independent $\xi\sim \textrm{Unif}[0,1]$, 
and set 
\#\label{eq:R_homo}
\cR :=  \cR_{\homo} = \Big\{ j\colon  p_j \leq s_j, ~ \xi  |\hat\cR_{j\to 0} | \leq  r_\homo^*  \Big\},
\#
where  
$
r_{\homo}^* := \max\big\{ r\colon  \sum_{j=1}^m \ind {\{ p_j\leq s_j, \, \xi   |\hat\cR_{j\to 0}  | \leq r \}}   \geq r \big\}.
$
\item \emph{Deterministic pruning}: define the rejection set 
\#\label{eq:R_dete}
\cR := \cR_{\dtm} = \Big\{ j\colon p_j \leq s_j, ~  |\hat\cR_{j\to 0} | \leq  r_\dtm^*  \Big\},
\#
where 
$
r_{\dtm}^* := \max\big\{ r\colon  \sum_{j=1}^m \ind {\{ p_j\leq s_j, \, |\hat\cR_{j\to 0}  | \leq r \}}   \geq r \big\}.
$
In all options, we use $p_j$ defined in~\eqref{eq:weighted_pval}.  
\end{enumerate}

It is straightforward to see that $\cR_{\dtm}\subseteq \cR_{\hete}$ and 
$\cR_{\dtm}\subseteq \cR_{\homo}$, i.e., 
random pruning leads to larger rejection sets. 
The selection procedure is summarized in Algorithm~\ref{alg:bh}. 

\begin{algorithm}[h]
  \caption{Weighted Conformalized Selection}\label{alg:bh}
  \begin{algorithmic}[1]
  \REQUIRE Calibration data $\{(X_i,Y_i)\}_{i=1}^n$, 
  test data  $\{X_{n+j}\}_{j=1}^m$, 
  thresholds $\{c_{n+j}\}_{j=1}^m$, 
  weight $w(\cdot)$,
  FDR target $q\in(0,1)$, monotone nonconformity score $V\colon \cX\times\cY\to \RR$, 
  pruning method $\in\{\texttt{hete}, \texttt{homo}, \texttt{dtm}\}$.
  \vspace{0.05in} 
  \STATE Compute $V_i = V(X_i,Y_i)$ for $i=1,\dots,n$,  
  and $\hat{V}_{n+j}= V(X_{n+j},c_{n+j})$ for $j=1,\dots,m$.
  \STATE Construct weighted conformal p-values $\{ p_j\}_{j=1}^m$ as in~\eqref{eq:weighted_pval}. 

  \vspace{0.3em}
  \noindent \texttt{- First-step selection -}
  \FOR{$j=1,\dots,m$}
  \STATE Compute p-values $\{ {p}_\ell^{(j)}\}$ as in~\eqref{eq:mod_pval}.
  \STATE (BH procedure) Compute $k^*_j = \max\big\{k\colon 1 +\sum_{\ell\neq j} \ind\{{p}_\ell^{(j)}\leq qk/m\}\geq k\big\}$. 
  \STATE Compute $\hat{\cR}_{j\to 0} = \{j\}\cup\{\ell \neq j\colon  {p}_\ell^{(j)}\leq q k^*_j /m\}$.
  \ENDFOR
  \STATE Compute the first-step selection set $\cR^{(1)} = \{j\colon  {p}_j \leq q|\hat\cR_{j\to 0}|/m\}$.
  
  \vspace{0.3em}
  \noindent \texttt{- Second-step pruning -}
  \STATE Compute $\cR = \cR_{\textrm{hete}}$ as in~\eqref{eq:R_hete}
  or $\cR = \cR_{\textrm{homo}}$ as in~\eqref{eq:R_homo}
  or $\cR = \cR_{\textrm{dtm}}$ as in~\eqref{eq:R_dete}. 
  \vspace{0.05in}
  \ENSURE Selection set $\cR$.
  \end{algorithmic}
\end{algorithm}









\subsection{Theoretical guarantee}


The following theorem, whose proof is in Appendix~\ref{app:thm_calib_ite}, shows exact finite-sample FDR control. 


\begin{theorem}\label{thm:calib_ite}
  Write $Z_i=(X_i,Y_i)$ for $i=1,\dots,m+n$, 
  and $\tilde{Z}_{n+j}=(X_{n+j},c_{n+j})$ for $j=1,\dots,m$.  Suppose $\{Z_i\}_{i=1}^n\iid P$ 
and $\{Z_{n+j}\}_{j=1}^m\iid Q$, and~\eqref{eq:cov_shift} holds 
for 
the input weight function $w(\cdot)$ of 
Algorithm~\ref{alg:bh}. 
Assume that for each $j=1,\dots,m$, the samples in  $\{Z_1,\dots,Z_n,Z_{n+j}\}\cup\{\tilde{Z}_{n+\ell}\}_{\ell\neq j}$ are mutually independent.  
Then with either 
$\cR \in \{\cR_{\hete}, \cR_{\homo}, \cR_{\dtm}\}$, it holds that  
\$
\EE\Bigg[  \frac{\sum_{j=1}^m \ind {\{j \in \cR, Y_{n+j}\leq c_{n+j}\}} }{1\vee |\cR|} \Bigg]   \leq q 
\$
(the expectation is taken over both calibration and test data).  
\end{theorem}
 

As we allow for random thresholds $c_{n+j}$, the independence assumption 
rules out the cases where the thresholds $c_{n+j}$ are adversarially chosen; the reason is that they would break certain exchangeability properties between scores $\{\hat{V}_{n+\ell}\}_{\ell=1}^m$ 
and calibration scores $\{V_{i}\}_{i=1}^n$. Independence holds 
if $c_{n+j}$ is pre-determined, or more generally, if $c_{n+j}$ is a random variable associated 
with independent test samples (such as when
$\{X_{n+j},c_{n+j},Y_{n+j}\}_{j=1}^m$ are i.i.d.~tuples that 
are independent of the calibration data).  Examples 
include 
individual treatment effects studied in Section~\ref{sec:ite}, 
and the drug-target interaction prediction task studied in Section~\ref{subsec:dti}.  
Other examples with random thresholds related to healthcare are in~\cite[Section 2.4]{jin2022selection}.
 
While the $p_j$'s are still marginally stochastically larger than uniforms, their complicated dependence requires additional care, and this is why 
we use the auxiliary p-values $\{p_\ell^{(j)}\}$ 
as `calibrators' to determine a potentially different rejection rule 
than naively applying the BH procedure. The proof of Theorem~\ref{thm:calib_ite} 
relies on comparing our p-values~\eqref{eq:weighted_pval}  to 
oracle p-values  
\#\label{eq:orc_w_pval_nr}
\bar p_j  = \frac{\sum_{i=1}^n w(X_i)\ind {\{V_i < {V}_{n+j} \}}+  w(X_{n+j})  }{\sum_{i=1}^n w(X_i) + w(X_{n+j})}.
\#
As we shall see, 
replacing our conformal p-values with 
their oracle counterparts does not change the first-step 
rejection set $\hat\cR_{j\to 0}$. The advantage of working with the oracle p-values is a friendly dependence structure, which ultimately ensures FDR control. 
The main ideas of the argument are: 
 
\begin{itemize} 
  \item {\em Randomization reduction:} for each $j$,
  $\hat\cR_{j\to 0}$ can be expressed as $\hat\cR_{j\to 0} = f(p_j,\bm{p}_{-j}^{(j)} )$,
  where $\bm{p}_{-j}^{(j)}:=\{p_\ell^{(j)}\}_{\ell\neq j}$. For all three pruning options, it holds that $$\fdr\leq \sum_{j=1}^m \EE\bigg[\frac{ \ind\{  p_j \leq q|f( p_j,\bm{p}_{-j}^{(j)})|/m\}}{ |f( p_j, {\bm{p}}_{-j}^{(j)})| }\bigg].$$ 
  \item {\em Leave-one-out:} $\hat\cR_{j\to 0}$ remains the same if we replace all $\{p_\ell\}$ with $\{\bar{p}_\ell\}$ defined above, i.e., $f( p_j,\bm{p}_{-j}^{(j)})= f(\bar p_j,\bar{\bm{p}}_{-j}^{(j)})$. 
  Since $\hat\cR_{j\to 0}$ is obtained by BH, it follows that $f(\bar p_j,\bar{\bm{p}}_{-j}^{(j)})=f(0,\bar{\bm{p}}_{-j}^{(j)})$. 
  \item {\em Conditional independence:} 
under the covariate shift~\eqref{eq:cov_shift}, the oracle p-values possess a nice conditional independence structure, namely, $\bar{p}_j \indep \bar{\bm{p}}_{-j}^{(j)} \given \cZ_j$, where $\cZ_j = [Z_1,\dots,Z_n,Z_{n+j}]$ is an unordered set of $Z_i=(X_i,Y_i)$, $i=1,\dots,n,n+j$. This, together with the first two steps and the fact that 
  $\bar{p}_j$ is stochastically larger than a uniform random variable conditional on $\cZ_j$, gives $\fdr\leq q$. 
\end{itemize}

Along the way, we shall explore interesting connections between Algorithm~\ref{alg:bh} 
and existing ideas in the multiple testing literature. 

\begin{remark}\normalfont
Algorithm~\ref{alg:bh} is related to the 
conditional calibration method~\cite{fithian2020conditional}, 
which utilizes sufficient statistics to calibrate a 
rejection threshold  for each individual hypothesis 
to achieve finite-sample FDR control. 
Indeed, for each $j$, 
$s_j:= {q|\hat{\cR}_{j\to 0}  | }/{m}$ 
can be viewed as the `calibrated threshold' for p-values in their framework; 
the unordered set $\cZ_j$ (after a careful leave-one-out analysis
in our proof)
plays a similar role as a `sufficient statistic'. 
Our heterogeneous pruning is similar to their random pruning step, 
while the other two options generalize their approach.
In addition, the procedure and its theoretical analysis  
are specific to our problem, and they are significantly different.
\end{remark}


\begin{remark}\normalfont
Algorithm~\ref{alg:bh} is also connected to 
the eBH procedure~\cite{wang2020false}, a generalization of 
the BH procedure to {\em e-values}. 
In the conventional setting with 
a set of (deterministic) hypotheses $\{H_j\}_{j=1}^m$, 
e-values are nonnegative random variables $\{e_j\}_{j=1}^m$  
such that $\EE[e_j]\leq 1$ 
if $H_j$ is null. 
For a target level $q\in(0,1)$, 
the eBH procedure outputs 
$\cR_{\ebh}:=\{j\colon e_j \geq m/(q\hat{k})\}$, where 
$\hat{k} =\max\big\{k\colon \sum_{j=1}^m \ind\{e_j\geq m/(qk)\}\geq k\big\}$. 
One can check that $\cR_{\dtm}$ is 
equivalent to $\cR_{\ebh}$ applied to 
\#\label{eq:eval}
e_j := \frac{\ind\{p_j\leq q|\hat\cR_{j\to 0}|/m\}}{q|\hat\cR_{j\to 0}|/m},
\quad j=1,\dots,m.
\#
Similar to~\eqref{eq:general_pvalue}, 
the $e_j$'s above obey a generalized notion of ``null'' e-values, in the sense that 
\$
\EE\big[e_j\ind\{j\in \cH_0\}\big]\leq 1,\quad \textrm{for all}~j=1,\dots,m, 
\$
see Appendix~\ref{app:thm_calib_ite} for details. 
Furthermore, using the generalized e-values~\eqref{eq:eval},  
$\cR_\hete$ and $\cR_\homo$ are equivalent to running eBH 
using $\{e_j/\xi_j\}$ and $\{e_j/\xi\}$, respectively. 
Note that $\{e_j/\xi_j\}$ and $\{e_j/\xi_j\}$ are no longer 
e-values, yet our procedures still control the FDR while achieving higher power. 
\end{remark}


Our next result shows that the 
extra randomness in the second step 
does not incur too much additional variation for $\cR_\hete$ and $\cR_\homo$.  
The proof of Proposition~\ref{prop:asymp_equiv} is in Appendix~\ref{app:subsec_asymp_equiv}.



\begin{prop}
\label{prop:asymp_equiv}
Suppose $\|w\|_\infty \leq M$ for some constant $M>0$. 
Suppose the distributions of $\{V(X_i,Y_i)\}_{i=1}^n$ 
and $\{V(X_{n+j},c_{n+j} )\}_{j=1}^m$ have no point mass.  
Let $\cR_{\bh}$ be the rejection set 
of BH with weighted conformal $p$-values~\eqref{eq:def_wcpval_rand}, 
and $\cR$ be any of the three selections from Algorithm~\ref{alg:bh}. Let $\cR^{(1)}=\{j\colon  p_j\leq q|\hat\cR_{j\to 0}|/m\}$ 
be the first-step selection set. 
Then the following holds: 
\begin{enumerate}[(i)]
\item If $m$ is fixed, then $\lim_{n\to \infty}\PP(\cR_{\bh}= \cR = \cR^{(1)})=1$ for each $\cR \in \{\cR_{\homo},\cR_{\hete}, \cR_{\dtm}\}$. 
\item If $m,n\to \infty$ and the regularity conditions in case (ii) of Theorem~\ref{thm:fdr_asymp}  hold, 
then  
 $\frac{|\cR^{(1)}\Delta \cR_{\bh}|}{|\cR^{(1)}|} ~\asto~0$, 
 $\frac{|\cR^{(1)}\Delta \cR_{\bh}|}{|\cR_\bh|} ~\asto~0$, 
$ \frac{|\cR^{(1)}\Delta \cR_{\homo}|}{|\cR^{(1)}|} ~\pto ~0$, and 
$ \frac{|\cR_\bh\Delta \cR_{\homo}|}{|\cR_\bh|} ~\pto ~0$.
\end{enumerate}
\end{prop}
 
Proposition~\ref{prop:asymp_equiv} 
shows that when the size of the calibration set  
is sufficiently large,  
our procedure is  close to the BH procedure applied to 
the weighted conformal p-values, the latter being a deterministic function of the data. 
In particular, our first-step rejection set is asymptotically equivalent to BH 
under mild conditions, and the same applies to 
$\cR_{\homo}$. 
The asymptotic analysis of $\cR_{\hete}$ and $\cR_{\dtm}$ is 
challenging. 
In our numerical experiments, we find that $\cR_{\hete}$ 
is often very close to $\cR_{\homo}$, 
while $\cR_{\dtm}$ usually makes too few rejections. 



\subsection{FDR bounds with estimated weights}


In practice, the covariate shift $w(\cdot)$ may be unknown. 
When the conformal p-values 
are computed with some fitted weights $\hat{w}(X_i)$, 
Theorem~\ref{thm:est_w} establishes upper bounds on the FDR.  
The proof is in Appendix~\ref{app:thm_est_w}. 

\begin{theorem}\label{thm:est_w}
Take $w(\cdot):=\hat{w}(\cdot)$ 
as the input weight function in Algorithm~\ref{alg:bh} and assume $\hat{w}(\cdot)$ 
is estimated in a training process with data independent from 
$\{(X_i,Y_i)\}_{i=1}^{n}\cup\{(X_{n+j},c_{n+j})\}_{j=1}^m$. 
Under the conditions of Theorem~\ref{thm:calib_ite}, for each $\cR \in \{\cR_{\homo},\cR_{\hete}, \cR_{\dtm}\}$, we have 
\$
\fdr \leq  q\cdot \EE\bigg[ \frac{\hat\gamma^2}{1+ q(\hat\gamma^2-1)/m} \bigg],
\$
where $\hat\gamma := \sup_{x\in \cX} \max\{  \hat{w}(x)/w(x),\, w(x)/\hat{w}(x) \}$. 
\end{theorem}

Above, $\hat\gamma\geq 1$ measures the estimation error in $\hat{w}(\cdot)$ 
relative to the true weights. 
When both $w(\cdot)$ and $\hat{w}(\cdot)$ 
are bounded away from zero and infinity, 
$\hat\gamma-1$ is of the same order as $ \sup_x|\hat{w}(x)-w(x)|$,  
and hence the FDR inflation in Theorem~\ref{thm:calib_ite} converges to zero 
if $\hat{w}$ is consistent. 

{As a direct application of WCS, we consider the goal of  
prioritizing drug candidates. 
We focus on two tasks: 
(i) drug property prediction, i.e., 
selecting molecules that bind to a target protein, and 
(ii) drug-target interaction prediction, i.e., 
selecting drug-target pairs with high affinity scores.}  
We use  the
DeepPurpose library~\cite{huang2020deeppurpose} for data pre-processing and model training. 

\subsection{Drug property prediction}
\label{subsec:drug_pred}

Our first goal is to find molecules that may bind to a target protein for HIV. 
Machine learning models 
are  trained on a subset of screened molecules from 
a drug library,  
and  then  used to predict the remaining ones. 


We use the HIV screening dataset in the DeepPurpose library
with a total size of $n_\textrm{tot}=41127$. 
In this dataset, the covariate $X\in \cX$ is 
a string that represents the chemical structure of a molecule 
(encoded by Extended-Connectivity FingerPrints, ECFP), and  
the response $Y\in \{0,1\}$ is binary, indicating whether 
the molecule binds to the target protein. 
Our goal is to select as many new drugs with $Y=1$ as possible while 
controlling the FDR below a specified level. 
This can be viewed as the goal~\eqref{eq:fdr} with $c_{n+j}\equiv 0.5$. 

Oftentimes, experimenters introduce a bias by selecting the first batch of 
molecules to screen (the training data), which results in a covariate shift 
between training (calibration) and test data. 
Here, we mimic an experimentation procedure that builds upon 
a pre-trained prediction model $\hat\mu \colon \cX\to \RR$ 
for binding activity, so that  
those with higher predicted values are more likely 
to be included in the training (calibration) data. 
In our experiment, to reduce computational time, 
we train a single model for both predicting test samples 
and for selecting the calibration fold. 
% 
We take a subset of size $0.4 \times n_{\textrm{tot}}$ as the training set $\cD_\train$, 
on which we train a small neural network $\hat\mu(\cdot)$ 
with three layers trained over three epochs. 
The remaining $0.6 \times n_{\textrm{tot}}$ samples are randomly selected 
as the calibration set $\cD_\calib$ with probability 
$p(x) = \min\{0.8,  \sigma(\hat\mu(x)-\bar\mu)\}$; here, $x\in \cX$, 
$\bar\mu=\frac{1}{|\cD_\train|}\sum_{i\in \cD_{\train}} \hat\mu(X_i)$ 
is the average prediction on the training fold, and $\sigma(t) = e^t/(1+e^t)$ is the sigmoid function. 
The covariate shift~\eqref{eq:cov_shift} is thus of the form $w(x)\propto 1/p(x)$, 
which we assume is known.

We compare the BH procedure with weighted conformal p-values~\eqref{eq:def_wcpval_rand}, 
as well as our Algorithms~\ref{alg:bh} and~\ref{alg:bh_cond}. The last one 
is applicable because we here take $c=0.5$ for binary classification. We consider two scores used in BH and Algorithm~\ref{alg:bh}:
% \vspace{-0.5em}
\begin{itemize}
    \item \texttt{res}:  $V(x,y) = y-\hat\mu(x)$. 
    % \vspace{-1em}
    \item \texttt{clip}: the score $V(x,y) = M\cdot \ind\{y>0\} -\hat\mu(x)$, with $M>2\sup_x|\hat\mu(x)|$.   
\end{itemize}
 % \vspace{-0.5em}
The empirical FDR over $N=200$ independent runs 
for FDR target levels $q\in \{0.1, 0.2,0.5\}$ is summarized 
in Figure~\ref{fig:drug_pred_fdr}. 
All algorithms empirically control the FDR below the nominal levels 
(up to random fluctuation), showing 
the reliability of WCS in 
prioritizing drug discovery under covariate shifts. 
The two scores yield similar power as in Figure~\ref{fig:drug_pred_power}.  
 

\begin{figure}[h]
    \centering
    \includegraphics[width=5.5in]{figs/fdrbar_pred.pdf}
    \caption{Empirical FDR for drug property prediction.
    The label \texttt{wBH} is a shorthand 
    for BH, and \texttt{wCC.*} for 
    Algorithm~\ref{alg:bh} (for \texttt{clip} and \texttt{res}) or 
    Algorithm~\ref{alg:bh_cond} (for \texttt{sub}) 
    with three pruning options $*\in \{\texttt{hete},\texttt{homo},\texttt{dtm}\}$. 
    The red dashed lines 
    are the nominal FDR levels.}
    \label{fig:drug_pred_fdr}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=5.5in]{figs/powerbar_pred.pdf}
    \caption{Empirical power for drug property prediction. 
    Everything else is as in Figure~\ref{fig:drug_pred_fdr}.}
    \label{fig:drug_pred_power}
\end{figure}


\subsection{Drug-target interaction prediction}
\label{subsec:dti}

We then consider drug-target interaction (DTI) prediction, 
where the goal is to select drug-target pairs with a high binding score. 
This task is relevant if a therapeutic company 
is interested in prioritizing resources for 
drug candidates that may be effective for any target they are interested in. 
% 
We use the DAVIS dataset~\cite{davis2011comprehensive}, 
which records real-valued binding affinities for $n_{\textrm{tot}} = 30060$ 
drug-target pairs. 
The drugs and the targets 
are encoded into numeric features using
ECFP and Conjoint triad feature (CTF)~\cite{shen2007predicting,shao2009predicting}.

We essentially follow the same procedure as in Section~\ref{subsec:drug_pred} and only rehearse the main components. 
First, we randomly sample a subset of size $n_{\train} = 0.2\times n_{\textrm{tot}}$ 
as $\cD_\train$, on which we train a regression model $\hat\mu(\cdot)\colon \cX\to \RR$ 
using a $3$-layer neural network with $10$ training epochs. 
This  relatively 
simple model is suitable for numerical experiments on CPUs (one 
can surely use more complicated prediction models in practice). 
As before, we use the same model $\hat\mu(\cdot)$ 
for both predicting test samples and selecting calibration data into first-batch screening (in practice, they can of course be different). We sample a drug-target pair for inclusion in the calibration set  
with probability $p(x) = \sigma(2\hat\mu(x)-\bar\mu)$, 
where $\bar\mu$ is the average training prediction as before. 
Finally, among those not selected in the calibration data, 
we randomly sample a subset of size $m=5000$ as test data. 

We choose a more complicated threshold $c_j$ for the continuous response. For a drug-target pair $X_{n+j}$, 
we set $c_j$ to be the $q_{\textrm{pop}}$-th 
quantile of the binding scores of all drug-target pairs in $\cD_\train$ 
with the same target. We evaluate $q_{\textrm{pop}} \in\{0.7,0.8\}$. 
Thus, $c_j:=c(X_{n+j},\cD_\train,q_{\textrm{pop}})$ where 
$c$ is a mapping that takes both   $\cD_\train$ and 
the target information in $X_{n+j}$ as inputs.  
Lastly, we evaluate the BH procedure and Algorithm~\ref{alg:bh} with scores: 
% \vspace{-0.5em}
\begin{itemize}
    \item \texttt{res}:  $V(x,y) = y-\hat\mu(x)$.
    % \vspace{-1em}
    \item \texttt{clip}: $V(x,y) = M\cdot \ind\{y>c(x,\cD_\train,q_{\textrm{pop}})\} 
    + c(x,\cD_\train,q_{\textrm{pop}}) \ind\{y\leq c(x,\cD_\train,q_{\textrm{pop}})\}-\hat\mu(x)$ in which $M = 100$.
\end{itemize}
% \vspace{-0.5em}
We always use $\cD_\calib$ as the calibration set, 
and set the FDR target as $q\in\{0.1,0.2,0.5\}$.  

Figure~\ref{fig:drug_dti_fdr} shows false discovery proportions (FDPs) 
for $q_{\textrm{pop}}=0.8$ 
in $N=200$ independent. 
Similar results for $q_{\textrm{pop}}=0.7$ 
are presented in Appendix~\ref{app:subsec_dti}. We see that the FDR is controlled at the desired level for all algorithms and nonconformity scores. 
This shows the validity of our algorithms and the 
plausibility of the independence assumptions we make on the drug-target pairs. 
In this task, we do not observe 
much difference between deterministic pruning (\texttt{WCS.dtm}) and 
the other two pruning options. 
Furthermore, we observe that the FDPs across replications 
tightly concentrate especially for \texttt{clip} and $q\in\{0.2,0.5\}$, 
showing that our algorithms are stable with respect to data splitting (i.e., the 
randomness in choosing the initial screening sets and in the training process). 
Comparing the two nonconformity scores, 
we see that \texttt{clip} exploits the error 
budget and obtains a realized FDR, which is very close to the 
nominal level, while \texttt{res} has a much lower FDR in all cases. Not surprisingly,  Figure~\ref{fig:drug_dti_power} shows that  
\texttt{clip} has much higher power. 

\begin{figure}[h]
    \centering
    \includegraphics[width=5.5in]{figs/fdr_dti_qop_8.pdf}
    \caption{Empirical FDR for drug-target interaction prediction  with $q_{\textrm{pop}}=0.8$. The shorthand \texttt{WBH} 
    stands for BH procedure, and \texttt{WCS.*} for 
    Algorithm~\ref{alg:bh}  
    with three pruning options $*\in \{\texttt{hete},\texttt{homo},\texttt{dtm}\}$. 
    The red dashed lines 
    are the nominal FDR levels. Solid lines are empirical averages (here empirical FDR).}
    \label{fig:drug_dti_fdr}
\end{figure}




\begin{figure}[h]
    \centering
    \includegraphics[width=5.5in]{figs/power_dti_qop_8.pdf}
    \caption{Empirical power for drug-target interaction prediction with $q_{\textrm{pop}}=0.8$. 
    Everything else is as in Figure~\ref{fig:drug_dti_fdr}.}
    \label{fig:drug_dti_power}
\end{figure}

Finally, we study the extension of our framework 
for outlier detection, where the calibration inliers may follow 
a distinct distribution compared with test inliers. We discuss a new setup and apply 
a variant of Algorithm~\ref{alg:bh} to bank marketing data.

\subsection{Hypothesis-conditional FDR control}
\label{subsec:hypo_cond_fdr}
  

Assuming access to i.i.d.~training data $\{  Z_i  \}_{i=1}^n $, 
we consider a set of test data $\{ Z_{n+j}  \}_{j=1}^m $ 
for which the $Z_{n+j}$'s may only be partially observed (e.g., if $Z=(X,Y)$ 
we would observe the features $X$ but not the response $Y$).  
We are interested in some null hypotheses $\{H_j \}_{j=1}^m $ 
associated with the test samples. 
As before, whether $H_j$ is true 
can be a random event depending on $Z_{n+j}$; this includes our previous problem 
with $H_j\colon Y_{n+j}\leq c_{n+j}$.

\subsubsection{Hypothesis-conditional covariate shift}
\label{subsec:outlier}
 

\begin{assumption}[Hypothesis-conditional covariate shift] 
  \label{assump:label_conditional}
$\{Z_i\}_{i=1}^n\cup\{Z_{n+j}\}_{j=1}^m$ are mutually independent. 
Also, conditional on the subset $\cH_0\subset\{1,\dots,m\}$ 
of all null hypotheses, it holds that $\{Z_i\}_{i=1}^n\iid P$, 
and $\{Z_{n+j}\}_{j\in \cH_0} \iid Q$, where $ \ud Q/\ud P (Z)=w(X)$ 
for some function $w\colon \cX\to \RR^+$ and $X\subseteq Z$. 
\end{assumption}


A special case of 
problem~\eqref{eq:fdr}  
obeys these assumptions. 

\begin{example}[Binary classification]\normalfont 
\label{ex:binary}
Set $Z=(X,Y)$. where $Y\in\{0,1\}$ is a binary response. 
Suppose the goal is to find positive $Y_{n+j}$, 
e.g.~an active drug or a qualified candidate. We only observe the covariates $\{X_{n+j}\}_{j=1}^m$ for the test samples
$\{(X_{n+j},Y_{n+j})\}_{j=1}^m\iid Q$. 
Consider a reference dataset 
that only preserves positive samples among a set of i.i.d.~data 
from a covariate shifted distribution $P$ ; that is,  
$\cD_\calib = \{Z_i\colon Y_{i}=0\}$ where $\{(X_i,Y_i)\}\iid P$. 
Although the (super-population) covariate shift~\eqref{eq:cov_shift} no longer applies, 
conditional on $\cH_0 = \{j\colon Y_{n+j}=0\}$,  
it still holds that $Z_i\iid P_{Z\given Y=0}$ for $Z_i\in \cD_\calib$, and 
$Z_{n+j}\iid Q_{Z\given Y=0}$ for $j\in \cH_0$.   
\end{example}

The above example also applies to 
candidate screening 
with constant thresholds 
$c_{n+j} \equiv c_0$.  
Setting $\tilde{Y} = \ind\{Y > c_0\}\in\{0,1\}$ 
and $\cD_{\calib} = \{Z_i\colon Y_i\leq c_0\}$, all 
arguments apply similarly to $\tilde{Z}=(X,\tilde{Y})$. 
% 
However, this does not necessarily apply when the  thresholds $c_{n+j}$ are random variables (especially when no 'threshold $c_j$' is observed in the calibration data, such as in the 
counterfactual inference problem we will study later).

Another application is outlier detection under covariate shifts. 

\begin{example}[Outlier detection]\normalfont
We revisit outlier detection~\cite{bates2021testing} 
while allowing for identifiable covariate shifts between the calibration inliers 
and the test inliers. 
Given $\{Z_i\}_{i=1}^n$ drawn i.i.d.~from 
an unknown distribution $P$ 
and a set of test data $\{Z_{n+j}\}_{j=1}^m$, 
we assume the inliers in 
the test data are i.i.d.~from a distribution $Q$ 
with $\ud Q/\ud P(Z)=w(Z)$ for a known function $w$, while allowing 
outliers to be from arbitrary distributions. 
The covariate shift may happen, for example, when 
inliers \emph{were} from $Q$ but 
the calibration set is selected with preferences 
relying on $z$: for instance, 
one may include more female users to balance the gender distribution
when curating 
a reference panel of normal transactions (inliers). 
In this case, $\cH_0 = \{j\colon Z_{n+j}\sim Q\}$ 
is a deterministic set, and Assumption~\ref{assump:label_conditional} clearly holds. 
\end{example}

The outlier detection example is closely related to 
identifying concept drifts. 

\begin{example}[Concept shift detection]\normalfont
Letting $Z=(X,Y)$ where $X\in \cX$ is the family of covariates and 
$Y$ is the response, concept shift 
focuses on potential changes in the conditional distribution of $Y$ given $X$. 
Given calibration data $\{Z_i\}_{i=1}^n\iid P$ 
and independent test data $\{Z_{n+j}\}_{j=1}^m$,  
\cite{hu2020distribution} 
assume $\{Z_{n+j}\}_{j=1}^m \iid Q$,  
and test for the global null $H_0\colon \ud Q/\ud P(Z) = w(X)$ 
for some $w\colon \cX\to \RR^+$. They achieve this by combining independent p-values 
after sample splitting.  
Our framework can be used to test 
individual concept drifts with dependent p-values. For instance, 
assume $\{X_{n+j}\}_{j=1}^m\iid Q_X$ for some unknown (but estimable) $Q_X$, 
we can test whether 
$P_{Y_{n+j}\given X_{n+j}} = P_{Y\given X}$. 
The null hypotheses can be formulated as 
$H_j\colon Z_{n+j}\sim Q$, where $\ud Q/\ud P(Z)=w(X)$ 
for some  $w\colon \cX\to \RR^+$ that is either known 
or can be estimated well under proper conditions.  
\end{example}

\subsubsection{Multiple testing procedure}

Our procedure for outlier detection under covariate shift is detailed 
in Algorithm~\ref{alg:bh_cond}. This 
slightly modifies Algorithm~\ref{alg:bh} 
by removing the thresholds (note differences in lines 1, 2, and 4). 
In the classification or constant threshold problem 
(Example~\ref{ex:binary}), it suffices to set $Z=X$ and leave out $Y$. 

\begin{algorithm}[h]
  \caption{Hypothesis-conditional Weighted Conformalized Selection}\label{alg:bh_cond}
  \begin{algorithmic}[1]
  \REQUIRE Calibration data $\{Z_i\}_{i=1}^n$, 
  test data  $\{Z_{n+j}\}_{j=1}^m$,  
  weight function $w(\cdot)$,
  FDR target $q\in(0,1)$, monotone nonconformity score $V\colon \cX\times\cY\to \RR$, 
  pruning method $\in\{\texttt{hete}, \texttt{homo}, \texttt{dtm}\}$.
  \vspace{0.05in} 
  \STATE Compute $V_i = V(Z_i)$ for $i=1,\dots,n$,  
  and $ {V}_{n+j}= V(Z_{n+j})$ for $j=1,\dots,m$.
  \STATE Construct weighted conformal p-values $\{ p_j\}_{j=1}^m$ as in~\eqref{eq:weighted_pval} with $\hat{V}_{n+j}$ replaced by $V_{n+j}$. 

  \vspace{0.3em}
  \noindent \texttt{- First-step selection -}
  \FOR{$j=1,\dots,m$}
  \STATE Compute p-values $\{ {p}_\ell^{(j)}\}$ as in~\eqref{eq:mod_pval} with $\hat{V}_{n+\ell}$ replaced by $V_{n+\ell}$ for all $\ell=1,\dots,m$.
  \STATE (BH procedure) Compute $k^*_j = \max\big\{k\colon 1 +\sum_{\ell\neq j} \ind\{{p}_\ell^{(j)}\leq qk/m\}\geq k\big\}$. 
  \STATE Compute $\hat{\cR}_{j\to 0} = \{j\}\cup\{\ell \neq j\colon  {p}_\ell^{(j)}\leq q k^*_j /m\}$.
  % \STATE Compute $e_j$ as in~\eqref{eq:eval}.
  \ENDFOR
  \STATE Compute the first-step selection set $\cR^{(1)} = \{j\colon  {p}_j \leq q|\hat\cR_{j\to 0}|/m\}$.
  
  \vspace{0.3em}
  \noindent \texttt{- Second-step selection -}
  \STATE Compute $\cR = \cR_{\textrm{hete}}$  
  or $\cR = \cR_{\textrm{homo}}$  
  or $\cR = \cR_{\textrm{dtm}}$ as in Algorithm~\ref{alg:bh}. 
  \vspace{0.05in}
  \ENSURE Selection set $\cR$.
  \end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:bh_cond} returns to 
the conventional perspective, where the null set is 
deterministic, and the null p-values 
are dominated by Unif$([0,1])$. That is, for $p_j$ constructed 
in Line 2 of Algorithm~\ref{alg:bh_cond}, it holds that 
\$
\PP(p_j \leq t \given j\in \cH_0 ) \leq t\quad \textrm{for all }t\in[0,1]. 
\$
After conditioning on $\cH_0$, 
we no longer need to deal with the randomness of 
the hypotheses  and their interaction with the p-values. 
The only issue is the mutual dependence among the p-values, 
which is addressed using a similar idea as in our theoretical analysis 
of Algorithm~\ref{alg:bh}. 

Using calibration data obeying the covariate shift assumption, Algorithm~\ref{alg:bh_cond} achieves 
a slightly stronger  hypotheses-conditional FDR control. 
The proof of Theorem~\ref{thm:fdr_cond} is in Appendix~\ref{app:thm_outlier}. 
\begin{theorem}\label{thm:fdr_cond}
  Under Assumption~\ref{assump:label_conditional}, Algorithm~\ref{alg:bh_cond} yields 
  \$
\EE\bigg[  \frac{ |\cR\cap \cH_0| }{1\vee |\cR|} \bigggiven \cH_0 \bigg]   \leq q\cdot \frac{|\cH_0|}{m}
\$
for any fixed $q\in(0,1)$, and each  $\cR \in \{\cR_{\homo},\cR_{\hete}, \cR_{\dtm}\}$.
\end{theorem}



\subsubsection{Comparison with Algorithm~\ref{alg:bh}}
\label{subsubsec:compare}

In binary classification, or more generally, 
WCS with a constant threshold, 
we have shown in Example~\ref{ex:binary} that 
Algorithm~\ref{alg:bh_cond} yields FDR control.  
In this case, Algorithms~\ref{alg:bh} and \ref{alg:bh_cond} differ 
in terms of (i) power, and (ii) distributional assumptions. 
We elaborate on these distinctions. 


First, Algorithm~\ref{alg:bh_cond}
only uses a subset of calibration data to construct p-values, 
which leads to a power loss 
for specific choices of nonconformity scores;  
see~\cite[Appendix A.1]{jin2022selection} for the i.i.d.~case.  
% 
Let us consider the binary setting. 
Suppose we have access to a set of calibration data $\{(X_i,Y_i)\}$ 
consisting of both $Y=1$ and $Y=0$ samples. 
As discussed in Example~\ref{ex:binary}, 
Assumption~\ref{alg:bh_cond} holds if we only use data in 
the subset 
$\cI_0 = \{i\colon Y_i=0\}$ as $\cD_\calib$ in Algorithm~\ref{alg:bh_cond}. In contrast,  
Algorithm~\ref{alg:bh} uses all data points.  
Suppose we set $V(x,y) = My - \hat\mu(x)$ 
and $c_{n+j} \equiv 0$ in Algorithm~\ref{alg:bh}, 
where  $\hat\mu(\cdot)$ is  a fitted point prediction, and 
$M>2\sup_{x\in\cX}|\hat\mu(x)|$ is a sufficiently  large constant. 
Similarly, we set $V(x)=M-\hat\mu(x)$ in Algorithm~\ref{alg:bh_cond}. 
This construction ensures  
\#\label{eq:monotone_V}
\inf_{x\in \cX} V(x,1) = M-\sup_{x\in \cX}\hat\mu(x) 
> \sup_{x\in \cX}|\hat\mu(x)| \geq \sup_{x\in \cX} V(x,0).
\#
Theorems~\ref{thm:calib_ite} 
and~\ref{thm:fdr_cond} state that the FDR is controlled 
for both approaches. 
However, letting $p_j$ denote the p-values 
constructed in Algorithm~\ref{alg:bh} and $p_j'$ denote those in 
Algorithm~\ref{alg:bh_cond}, we note that 
\$
p_j &= 
\frac{\sum_{i\in \cI_0} w(X_i)\ind{\{V(X_i,0) < V(X_{n+j},0) \}} 
+  w(X_{n+j}) }{\sum_{i=1}^n w(X_i) + w(X_{n+j})} 
+ \frac{ \sum_{i\in \cI_1} w(X_i)\ind{\{V(X_i,1) < V(X_{n+j},0) \}}  }{\sum_{i=1}^n w(X_i) + w(X_{n+j})}  \\
&=\frac{\sum_{i\in \cI_0} w(X_i)\ind{\{V(X_i,0) < V(X_{n+j},0) \}}   +  w(X_{n+j}) }{\sum_{i=1}^n w(X_i) + w(X_{n+j})} \\ 
&< \frac{\sum_{i\in \cI_0} w(X_i)\ind{\{V(X_i,0) < V(X_{n+j},0) \}}   +  w(X_{n+j}) }{\sum_{i\in \cI_0} w(X_i) + w(X_{n+j})} = p_j',
\$
where the second lines uses~\eqref{eq:monotone_V}. 
That is, with this   nonconformity score (which is shown 
in~\cite{jin2022selection}
to be powerful), the p-values constructed in Algorithm~\ref{alg:bh}
is strictly  smaller than those in Algorithm~\ref{alg:bh_cond}, 
leading to larger rejection sets and higher power. 
Furthermore, in this case, 
\$
\frac{p_j}{p_j'} = \frac{\sum_{i\in \cI_0} w(X_i) + w(X_{n+j})}{\sum_{i=1}^n w(X_i) + w(X_{n+j})}
\$
is the weighted proportion of negative calibration samples. 
Thus, the power gain of Algorithm~\ref{alg:bh} is more significant 
when there are more positive samples in the test distribution. 

Second, Algorithm~\ref{alg:bh_cond} is suitable for dealing with 
imbalanced data, such as those encountered in drug discovery. 
For instance, after selecting a subset of molecules or compounds  for virtual screening, 
the experimenter may discard a few samples with $Y=0$ as she would deem them as uninteresting. 
This bias would make Algorithm~\ref{alg:bh} inapplicable. 
However, if the decision to discard or not
% the second-step exclusion 
does not depend on $X$, 
the  shift between $P_{X\given Y=0}$ 
and $Q_{X\given Y=0}$ remains the same as
the covariate shift incurred by selection into screening, 
and Algorithm~\ref{alg:bh_cond} still provides reliable selection. 


We finally illustrate the hypothesis-conditional variant 
from Algorithm~\ref{alg:bh_cond} on outlier detection tasks, 
where there is a covariate shift between calibration inliers 
and test inliers. We focus on scenarios where the covariate shift 
is known. For instance, if the demographics in normal financial transactions 
are adjusted by rejection sampling 
to balance between male  and female, urban and rural users, and so on, then 
the covariate shift is given by the sampling weights in the adjustment.\footnote{Results in this section can be reproduced at \href{https://github.com/ying531/conformal-selection}{https://github.com/ying531/conformal-selection}.}

\subsection{Simulation studies}

We adapt the simulation setting in~\cite{bates2021testing}
to a scenario with covariate shift induced by rejection sampling.   
We fix a test sample size $n_{\test} = 1000$ and 
calibration sample size $n_\calib=1000$. 

At the beginning of the experiment, we sample a subset $\cW\subseteq \RR^{50}$ with $|\cW|=50$, where each element in $\cW$ is 
independently drawn from $\textrm{Unif}([-3,3]^{50})$, 
and hold it as fixed afterwards. 
Fix a proportion of outliers at $\rho_{\texttt{pop}}\in \{0.1,0.2,\dots,0.5\}$. 
The number of outliers in the test data 
is $n_\test \cdot \rho_{\texttt{pop}}$, 
where each of them is i.i.d.~generated as 
$X_{n+j}=\sqrt{a}V_{n+j}+W_{n+j}$ 
for signal strength $a$ varying in the range $[1,4]$, 
$V_{n+j}\sim N(0,I_{50})$, and $W_{n+j}\sim \textrm{Unif}(\cW)$.  
Following this, the test inliers are i.i.d.~generated as 
$X_{n+j}= V_{n+j}+W_{n+j}$, whose distribution is denoted as $Q_X$. 
The calibration inliers are i.i.d.~generated from
$P_X$ with $\ud Q_X/\ud P_X(x)=w(x)\propto \sigma(x^\top\theta)$ ($\sigma(\cdot)$ is the sigmoid), where $\theta\in \RR^{50}$ and $\theta_j=0.1\cdot\ind\{j\leq 5\}$. 
We also generate $n_{\train}=1000$ training sample from $P_X$. This setting mimics a stylized scenario 
where the calibration inliers are collected in a way such 
that the preference is prescribed by a logistic function of $X$, 
and is known to the data analyst. 

We train a one-class SVM $\hat\mu(x)$ with \texttt{rbf} kernel using 
the \texttt{scikit-learn} Python library, 
and apply Algorithm~\ref{alg:bh_cond} with $Z_i=X_i$ at 
FDR level $q=0.1$
using all the three pruning methods. 
All procedures are repeated $N=1000$ times, 
and the FDPs and proportions of true discoveries are averaged 
to estimate the FDR and power. We also evaluate the BH 
procedure applied to p-values constructed in Line 4 of 
Algorithm~\ref{alg:bh_cond}. 
Figure~\ref{fig:fdr_outlier} shows the 
empirical FDR across runs. In line with Theorem~\ref{thm:fdr_cond}, we see that the FDR is 
always below $(1-\rho_{\texttt{pop}})q$. Also, 
BH applied to weighted conformal p-values 
empirically controls the FDR and demonstrates comparable performance. 

\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{figs/cond_outlier_fdr.pdf}
    \caption{Empirical FDR averaged over $N=1000$ runs under increasing levels $a$ of signal strength. Each subplot corresponds to one value of $\rho_{\texttt{pop}}$. The red dashed lines are at the nominal level $q=0.1$.}
    \label{fig:fdr_outlier}
\end{figure}

Figure~\ref{fig:power_outlier} plots the power of the four methods. 
Interestingly, although they differ in FDR, especially 
when the signal strength is small, they achieve nearly identical power, finding about the same number of true discoveries. 

\begin{figure}[H]
    \centering
    \includegraphics[width=6.5in]{figs/cond_outlier_power.pdf}
    \caption{Empirical power averaged over $N=1000$ runs. Everything else is as in Figure~\ref{fig:fdr_outlier}.}
    \label{fig:power_outlier}
\end{figure}

\subsection{Real data application}

We then apply Algorithm~\ref{alg:bh_cond} to a bank marketing dataset~\cite{moro2014data}; this dataset has been used to benchmark  
outlier detection algorithms~\cite{pang2019deep,pang2021deep}. 
In short, each sample in the dataset represents a phone contact to a potential 
client, whose demographic information such as age, gender, and marital status 
is also recorded in the features.
The outcome is a binary indicator $Y$, where 
$Y=1$ represents a successful compaign. Positive samples are relatively rare, 
accounting for about $10\%$ of all records. 


In our experiments, we always use negative samples as the calibration data. 
We find this dataset interesting because 
the classification and outlier detection perspectives 
are somewhat blurred here. 
Viewing this as a classification task, Example~\ref{ex:binary} 
becomes relevant, and our theory implies FDR control as long as the 
distribution of negative samples 
(i.e., that of $X$ given $Y=0$) admits the covariate shift. 
In the sense of finding positive responses (so as to reach out 
to these promising clients), 
controlling the FDR ensures efficient use of campaign resources. 
Alternatively, from an outlier detection perspective, 
those $Y=1$ are outliers~\cite{pang2019deep} that 
deserve more attention. 
Taking either of the two perspectives leads to the same calibration set 
and the same guarantees following Section~\ref{subsec:hypo_cond_fdr}.  
% 
Perhaps the only methodological distinction is 
whether positive samples are leveraged in the 
training process. Similar considerations  
also appeared in~\cite{liang2022integrative}, but the scenario therein 
is more sophisticated than ours as they also use 
known outliers (positive samples) in 
constructing p-values.  
As a classification problem, we may use positive samples to train 
a classifier and produce nonconformity scores. 
As an outlier detection problem, however, 
the widely-used  one-class classification 
relies exclusively on inliers (negative samples). 
We will evaluate the performance of both 
(i.e., train the nonconformity score with or without positive samples); 
see procedures \texttt{cond\_class} and \texttt{outlier} below. 

For comparison, we additionally evaluate Algorithm~\ref{alg:bh} 
which operates under a covariate shift assumption 
on the joint distribution of $(X,Y)$ (see procedure \texttt{sup\_class} below); this is in contrast 
to Algorithm~\ref{alg:bh_cond} which puts no assumption on positive samples. 

The total sample size is $N=41188$ with $4640$ positive samples. 
We use rejection sampling to create the covariate shift between 
training/calibration and test samples. In details, we use a subset of features $X^*\in \RR^4$ 
representing age and indicators of marriage, 
basic four-year education, and housing. We first randomly select a subset of the original dataset as the test data.
Each sample enters the test data with probability 
$e(x):=0.125\, \sigma({\theta^\top x^*})$, where $\theta=(1,1/2,1/2,1/2)^\top$.  This creates a covariate shift $w(x)\propto e(x)/(1-e(x))$ between 
null samples in calibration and test folds, so that 
the test set contains more senior, married, well-educated and housed people.  

To test the two perspectives, we consider three procedures with FDR levels $q\in \{0.2, 0.5, 0.8\}$:
\begin{enumerate}[(i)]
    \item \texttt{sup\_class}: We randomly split the data that is not in the test fold into two equally-sized halves as the training and calibration folds, $\cD_\train$ and $\cD_\calib$. 
    We use $\cD_\train$ to train an SVM classifier using the \texttt{scikit-learn} Python library with \texttt{rbf} kernel. 
    Then, we apply Algorithm~\ref{alg:bh} with $V(x,y)=100\cdot y - \hat\mu(x)$, where $\hat\mu(\cdot)$ is the class probability output from the SVM classifier. 
    \item \texttt{cond\_class}: With the same random split as in (i), the first half is used as $\cD_\train$, while we set $\cD_\calib$ as the set of negative samples in the second half. We use $\cD_\train$ to train an SVM classifier in the same way as in (i) to obtain the predicted class probability $\hat\mu(\cdot)$. Lastly, we apply Algorithm~\ref{alg:bh_cond} with $V(x)=-\hat\mu(x)$. 
    \item \texttt{outlier}: With the same split, we set $\cD_\train$ and $\cD_\calib$ as the two folds after discarding all the negative samples. We use $\cD_\train$ to train a one-class SVM classifier using the \texttt{scikit-learn} Python library with \texttt{rbf} kernel and obtain the output $\hat\mu(\cdot)$. We then apply Algorithm~\ref{alg:bh_cond} with $V(x)=-\hat\mu(x)$. 
\end{enumerate}

To ensure consistency, in all procedures, 
the SVM-based classifier uses the parameter \texttt{gamma}$=0.01$.\footnote{We find that this parameter leads to the highest power for outlier detection, and is 
also a relatively powerful choice for classification.} 
FDPs and power (proportions of true discoveries) across  $N=1000$ independent runs are 
shown in Figures~\ref{fig:real_outlier_fdr} and~\ref{fig:real_outlier_power}, 
respectively. 

\begin{figure}[h]
    \centering 
    \includegraphics[width=\linewidth]{figs/real_outlier_fdr.pdf}
    \caption{FDPs over $N=1000$ independent data splits. Each subplot corresponds to one nominal FDR level shown by means of red dashed lines.} 
    \label{fig:real_outlier_fdr}
\end{figure}

In Figure~\ref{fig:real_outlier_fdr}, we observe 
FDR control for all the methods in all settings. 
Across the four testing methods, the FDPs are very similar, 
hence all of them are reasonable choices. However, both the values and the variability of the FDP 
vary across settings. The methods 
\texttt{cond\_class} and \texttt{outlier} from Algorithm~\ref{alg:bh_cond} 
do not use all of the error budget---note the $\pi_0$ factor in Theorem~\ref{thm:fdr_cond}. 
In contrast, Algorithm~\ref{alg:bh} leverages super-population structure 
(which is present in this problem) and 
leads to tight FDPs and FDR around the target level. 
In all cases, the outlier detection approach, where 
only negative samples are used in the training process, 
has more variable FDPs. 
With this dataset, 
the variability of FDP around the FDR is visible for $q=0.2$, but it 
decreases as we increase the 
FDR level. 


\begin{figure}[h]
    \centering 
    \includegraphics[width=6.5in]{figs/real_outlier_power.pdf}
    \caption{Proportion of true discoveries (empirical power). Everything else is as in Figure~\ref{fig:real_outlier_fdr}.}
    \label{fig:real_outlier_power}
\end{figure}

In Figure~\ref{fig:real_outlier_power}, the power across 
the four multiple testing methods is similar. 
However, we observe  drastic 
differences among the three settings. 
While using the same classifier, 
\texttt{cond\_class} has lower power than \texttt{sup\_class}, 
per our discussion in Section~\ref{subsubsec:compare}. 
We also see that \texttt{outlier} has much lower power, which may be due to the 
fact that the nonconformity score is not sufficiently powerful
in distinguishing inliers from outliers 
as the training process does not use outliers. 
Our results suggest that in outlier detection problems, 
even when we do not want to impose any distributional assumption 
on outliers (so that Algorithm~\ref{alg:bh} becomes less reasonable), 
utilizing known outliers may be helpful in obtaining better nonconformity 
scores. 

Finally, we observe negligible difference between the rejection sets 
returned by the BH procedure and Algorithm~\ref{alg:bh}. There at most five distinct decisions among $\approx 3700$ test samples.