# Magnetron

**Magnetron** is a lightweight, modern machine learning framework providing tensor computation, a rich set of operators, and neural network building blocks with CPU and CUDA backends.

The core of Magnetron is written in C with zero external dependencies, while a modern Python API offers a PyTorch-like user experience.

## Development üßë‚Äçüíª

Magnetron is a hobby project I develop in my free time out of fun and curiosity.
It‚Äôs not meant to fully replace PyTorch or other frameworks but more as an experimental framework for research of state of the art algorithms, models and kernels for specific hardware.
If you have any questions, suggestions or critique, feel free to open a GitHub issue.
## Backends ‚öôÔ∏è
### CPU
Magnetron‚Äôs CPU backend performs runtime CPU detection and dispatches architecture-specific optimized kernels, enabling efficient execution on hardware ranging from early Intel Core Duo systems to modern high-core-count CPUs such as Threadrippers.

Performance-critical paths use hand-written SIMD intrinsics and a low-overhead multithreading engine, with matrix multiplication outperforming PyTorch on some test systems (Zen 5).
Supported SIMD instruction sets include SSE2‚ÄìSSE4.1, AVX, AVX2, FMA3, AVX-512, and ARM NEON.
### CUDA
Magnetron‚Äôs CUDA backend is currently a work in progress and is expected to be ready for use in early 2026. Most kernels are already implemented; remaining work primarily focuses on the memory management and data transfer layers. The CUDA backend is designed to support a wide range of NVIDIA GPUs, from older GTX-class hardware to modern data-center accelerators such as the B200.