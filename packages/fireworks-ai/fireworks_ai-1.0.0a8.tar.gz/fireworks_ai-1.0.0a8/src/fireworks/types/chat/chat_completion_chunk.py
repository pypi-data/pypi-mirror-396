# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import builtins
from typing import Dict, List, Union, Optional
from typing_extensions import Literal, TypeAlias

from ..._models import BaseModel
from ..shared.chat_completion_message_tool_call import ChatCompletionMessageToolCall

__all__ = [
    "ChatCompletionChunk",
    "Choice",
    "ChoiceDelta",
    "ChoiceLogprobs",
    "ChoiceLogprobsLogProbs",
    "ChoiceLogprobsNewLogProbs",
    "ChoiceLogprobsNewLogProbsContent",
    "ChoiceLogprobsNewLogProbsContentTopLogprob",
    "ChoiceRawOutput",
    "ChoiceRawOutputCompletionLogprobs",
    "ChoiceRawOutputCompletionLogprobsContent",
    "ChoiceRawOutputCompletionLogprobsContentTopLogprob",
    "Usage",
    "UsagePromptTokensDetails",
]


class ChoiceDelta(BaseModel):
    """The message delta"""

    content: Optional[str] = None
    """The contents of the chunk message"""

    reasoning_content: Optional[str] = None
    """The reasoning or thinking process generated by the model.

    This field is only available for certain reasoning models (GLM 4.5, GLM 4.5 Air,
    GPT OSS 120B, GPT OSS 20B) and contains the model's internal reasoning that
    would otherwise appear in `<think></think>` tags within the content field.
    """

    role: Optional[str] = None
    """The role of the author of this message"""

    tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None


class ChoiceLogprobsLogProbs(BaseModel):
    """Legacy log probabilities format"""

    text_offset: Optional[List[int]] = None

    token_ids: Optional[List[int]] = None

    token_logprobs: Optional[List[float]] = None

    tokens: Optional[List[str]] = None

    top_logprobs: Optional[List[Dict[str, float]]] = None


class ChoiceLogprobsNewLogProbsContentTopLogprob(BaseModel):
    token: str

    logprob: float

    token_id: int

    bytes: Optional[List[int]] = None


class ChoiceLogprobsNewLogProbsContent(BaseModel):
    token: str

    bytes: List[int]

    logprob: float

    sampling_logprob: Optional[float] = None

    text_offset: int

    token_id: int

    extra_logprobs: Optional[List[float]] = None

    extra_tokens: Optional[List[int]] = None

    last_activation: Optional[str] = None

    routing_matrix: Optional[str] = None

    top_logprobs: Optional[List[ChoiceLogprobsNewLogProbsContentTopLogprob]] = None


class ChoiceLogprobsNewLogProbs(BaseModel):
    """OpenAI-compatible log probabilities format"""

    content: Optional[List[ChoiceLogprobsNewLogProbsContent]] = None


ChoiceLogprobs: TypeAlias = Union[ChoiceLogprobsLogProbs, ChoiceLogprobsNewLogProbs, None]


class ChoiceRawOutputCompletionLogprobsContentTopLogprob(BaseModel):
    token: str

    logprob: float

    token_id: int

    bytes: Optional[List[int]] = None


class ChoiceRawOutputCompletionLogprobsContent(BaseModel):
    token: str

    bytes: List[int]

    logprob: float

    sampling_logprob: Optional[float] = None

    text_offset: int

    token_id: int

    extra_logprobs: Optional[List[float]] = None

    extra_tokens: Optional[List[int]] = None

    last_activation: Optional[str] = None

    routing_matrix: Optional[str] = None

    top_logprobs: Optional[List[ChoiceRawOutputCompletionLogprobsContentTopLogprob]] = None


class ChoiceRawOutputCompletionLogprobs(BaseModel):
    """OpenAI-compatible log probabilities format"""

    content: Optional[List[ChoiceRawOutputCompletionLogprobsContent]] = None


class ChoiceRawOutput(BaseModel):
    """
    Extension of OpenAI that returns low-level interaction of what the model
    sees, including the formatted prompt and function calls
    """

    completion: str
    """Raw completion produced by the model before any tool calls are parsed"""

    prompt_fragments: List[Union[str, int]]
    """
    Pieces of the prompt (like individual messages) before truncation and
    concatenation. Depending on prompt_truncate_len some of the messages might be
    dropped. Contains a mix of strings to be tokenized and individual tokens (if
    dictated by the conversation template)
    """

    prompt_token_ids: List[int]
    """Fully processed prompt as seen by the model"""

    completion_logprobs: Optional[ChoiceRawOutputCompletionLogprobs] = None
    """OpenAI-compatible log probabilities format"""

    completion_token_ids: Optional[List[int]] = None
    """Token IDs for the raw completion"""

    grammar: Optional[str] = None
    """
    Grammar used for constrained decoding, can be either user provided (directly or
    JSON schema) or inferred by the chat template
    """

    images: Optional[List[str]] = None
    """Images in the prompt"""


class Choice(BaseModel):
    """A streamed chat completion choice."""

    delta: ChoiceDelta
    """The message delta"""

    index: int
    """The index of the chat completion choice"""

    finish_reason: Optional[Literal["stop", "length", "function_call", "tool_calls"]] = None
    """The reason the model stopped generating tokens.

    This will be "stop" if the model hit a natural stop point or a provided stop
    sequence, or "length" if the maximum number of tokens specified in the request
    was reached
    """

    logprobs: Optional[ChoiceLogprobs] = None
    """Legacy log probabilities format"""

    prompt_token_ids: Optional[List[int]] = None
    """Token IDs for the prompt (when return_token_ids=true)"""

    raw_output: Optional[ChoiceRawOutput] = None
    """
    Extension of OpenAI that returns low-level interaction of what the model sees,
    including the formatted prompt and function calls
    """

    token_ids: Optional[List[int]] = None
    """Token IDs for this chunk (when return_token_ids=true)"""


class UsagePromptTokensDetails(BaseModel):
    """Details about prompt tokens, including cached tokens"""

    cached_tokens: Optional[int] = None


class Usage(BaseModel):
    """Usage statistics."""

    prompt_tokens: int
    """The number of tokens in the prompt"""

    total_tokens: int
    """The total number of tokens used in the request (prompt + completion)"""

    completion_tokens: Optional[int] = None
    """The number of tokens in the generated completion"""

    prompt_tokens_details: Optional[UsagePromptTokensDetails] = None
    """Details about prompt tokens, including cached tokens"""


class ChatCompletionChunk(BaseModel):
    """The streamed response message from a /v1/chat/completions call."""

    id: str
    """A unique identifier of the response"""

    choices: List[Choice]
    """The list of streamed chat completion choices"""

    created: int
    """The Unix time in seconds when the response was generated"""

    model: str
    """The model used for the chat completion"""

    object: Optional[str] = None
    """The object type, which is always "chat.completion.chunk" """

    perf_metrics: Optional[Dict[str, builtins.object]] = None
    """See parameter [perf_metrics_in_response](#body-perf-metrics-in-response)"""

    prompt_token_ids: Optional[List[int]] = None
    """Token IDs for the prompt (when return_token_ids=true, sent in first chunk)"""

    usage: Optional[Usage] = None
    """Usage statistics."""
