"""
Core LLM implementation with OpenAI-like API
"""
import json
import logging
from typing import List, Dict, Any, Optional, Union, Callable, Iterator, Tuple, TYPE_CHECKING

from .backends.base import BaseBackend
from .prompt_formatting import get_prompt_formatter
from .memory import MessageHistory
from .function_calling import FunctionRegistry, FunctionCall, parse_function_calls

if TYPE_CHECKING:  # pragma: no cover - import-time convenience for type checkers
    from .backends.transformers import TransformersBackend
    from .backends.llamacpp import LlamaCppBackend

logger = logging.getLogger(__name__)

class LLM:
    """
    Main LLM class that provides an OpenAI-like interface for local language models.
    
    This class handles:
    - Chat and completion generation
    - Prompt formatting for different models
    - Function/tool calling
    - Memory and history management
    - Streaming support
    - Logprobs and token information
    - Structured output (JSON mode)
    """
    
    def __init__(
        self, 
        model_path: str,
        backend: Optional[str] = None,
        context_window: Optional[int] = None,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.95,
        top_k: int = 40,
        repetition_penalty: float = 1.1,
        backend_kwargs: Optional[Dict[str, Any]] = None,
    ):
        """
        Initialize the LLM with a local model.
        
        Args:
            model_path: Path or name of the model to load
            backend: Backend to use ("transformers", "llamacpp", or None for auto-detect)
            context_window: Max context window size (auto-detected if None)
            max_new_tokens: Maximum new tokens to generate per response
            temperature: Sampling temperature (higher = more random)
            top_p: Top-p sampling parameter
            top_k: Top-k sampling parameter
            repetition_penalty: Penalty for repetition
            backend_kwargs: Additional arguments to pass to the backend
        """
        self.model_path = model_path
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.top_k = top_k
        self.repetition_penalty = repetition_penalty
        self.backend_kwargs = backend_kwargs or {}
        self.context_window = context_window
        
        # Initialize backend
        self.backend = self._init_backend(backend)
        
        # Auto-detect context window if not provided
        if not self.context_window:
            self.context_window = self.backend.get_context_window()
            logger.info(f"Auto-detected context window: {self.context_window}")
        
        # Initialize prompt formatter based on model name
        self.prompt_formatter = get_prompt_formatter(model_path)
        
        # Initialize memory for conversation history
        self.memory = MessageHistory(context_window=self.context_window)
        
        # Initialize function registry
        self.function_registry = FunctionRegistry()

    def _init_backend(self, backend_name: Optional[str]) -> BaseBackend:
        """Initialize the appropriate backend based on name or auto-detection."""
        if backend_name is None:
            # Auto-detect based on file extension or model path
            if ".gguf" in self.model_path.lower() or "ggml" in self.model_path.lower():
                backend_name = "llamacpp"
            else:
                backend_name = "transformers"

        if backend_name == "transformers":
            try:
                from .backends.transformers import TransformersBackend
            except ImportError as exc:
                raise ImportError(
                    "Transformers backend requires the 'transformers' and 'torch' packages. "
                    "Install with: pip install \"local-llm-kit[transformers]\""
                ) from exc
            return TransformersBackend(self.model_path, **self.backend_kwargs)
        if backend_name == "llamacpp":
            try:
                from .backends.llamacpp import LlamaCppBackend
            except ImportError as exc:
                raise ImportError(
                    "llama.cpp backend requires the 'llama-cpp-python' package. "
                    "Install with: pip install \"local-llm-kit[llamacpp]\""
                ) from exc
            return LlamaCppBackend(self.model_path, **self.backend_kwargs)
        raise ValueError(f"Unsupported backend: {backend_name}")
    
    def add_function(self, name: str, schema: Dict[str, Any], implementation: Callable):
        """
        Register a function that can be called by the model.
        
        Args:
            name: Name of the function
            schema: JSON Schema describing the function parameters
            implementation: Python function to execute when called
        """
        self.function_registry.add_function(name, schema, implementation)
    
    def chat(
        self, 
        messages: List[Dict[str, str]],
        functions: Optional[List[Dict[str, Any]]] = None,
        function_call: Union[str, Dict[str, str]] = "auto",
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        format: Optional[str] = None,
        logprobs: bool = False,
        top_logprobs: Optional[int] = None,
    ) -> Union[Dict[str, Any], Iterator[Dict[str, Any]]]:
        """
        Generate a chat completion for the provided messages.
        
        Args:
            messages: A list of message dictionaries with "role" and "content" keys
            functions: List of function specifications (optional)
            function_call: Controls when functions are called ("auto", "none", or specific function)
            temperature: Sampling temperature (overrides instance setting)
            max_tokens: Maximum number of tokens to generate (overrides instance setting)
            stream: Whether to stream the response
            format: Output format ("json" for structured JSON output)
            logprobs: Whether to return log probabilities
            top_logprobs: Number of top tokens to return logprobs for
            
        Returns:
            A completion object or iterator of completion chunks if streaming
        """
        # Make local copies of messages and update memory
        message_list = list(messages)
        self.memory.add_messages(message_list)
        
        # Get functions from registry if not provided
        if functions is None and self.function_registry.has_functions():
            functions = self.function_registry.get_schema_list()
        
        # Prepare generation parameters
        gen_params = {
            "temperature": temperature if temperature is not None else self.temperature,
            "max_new_tokens": max_tokens if max_tokens is not None else self.max_new_tokens,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "repetition_penalty": self.repetition_penalty,
            "stream": stream,
            "logprobs": logprobs,
            "top_logprobs": top_logprobs or 5 if logprobs else None,
        }
        
        # Format prompt according to model's template
        prompt = self.prompt_formatter.format_messages(
            message_list, 
            functions=functions, 
            function_call=function_call,
            json_mode=(format == "json")
        )
        
        if stream:
            return self._stream_chat_response(prompt, gen_params, format, functions, function_call)
        else:
            return self._generate_chat_response(prompt, gen_params, format, functions, function_call)
    
    def _generate_chat_response(
        self, 
        prompt: str, 
        gen_params: Dict[str, Any],
        format: Optional[str], 
        functions: Optional[List[Dict[str, Any]]], 
        function_call: Union[str, Dict[str, str]]
    ) -> Dict[str, Any]:
        """Generate a complete chat response."""
        # Generate response from the backend
        result = self.backend.generate(prompt, **gen_params)
        text = result["text"]
        
        # Handle JSON formatting
        if format == "json":
            text = self._ensure_json_output(text)
            
        # Handle function calling
        function_calls = None
        if functions and function_call != "none":
            try:
                function_calls = parse_function_calls(text)
                if function_calls and function_call != "none":
                    # Execute function if auto or specifically requested
                    if function_call == "auto" or (
                        isinstance(function_call, dict) and 
                        function_call.get("name") == function_calls[0].name
                    ):
                        func_result = self._execute_function_call(function_calls[0])
                        
                        # Add function result to messages
                        self.memory.add_messages([
                            {"role": "assistant", "content": None, "function_call": function_calls[0].to_dict()},
                            {"role": "function", "name": function_calls[0].name, "content": func_result}
                        ])
                        
                        # Generate a new response that includes the function result
                        new_prompt = self.prompt_formatter.format_messages(
                            self.memory.get_messages(),
                            functions=functions,
                            function_call=function_call,
                            json_mode=(format == "json")
                        )
                        result = self.backend.generate(new_prompt, **gen_params)
                        text = result["text"]
            except Exception as e:
                logger.warning(f"Error parsing function calls: {e}")
                
        # Prepare response object
        response = {
            "id": f"chatcmpl-{id(self)}-{id(prompt)}",
            "object": "chat.completion",
            "created": self.backend.get_timestamp(),
            "model": self.model_path,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": text
                },
                "finish_reason": "stop"
            }]
        }
        
        # Add function call to response if detected
        if function_calls:
            response["choices"][0]["message"]["function_call"] = function_calls[0].to_dict()
            response["choices"][0]["message"]["content"] = None
        
        # Add logprobs if requested
        if gen_params.get("logprobs"):
            response["choices"][0]["logprobs"] = result.get("logprobs", {})
            
        # Add usage information
        response["usage"] = {
            "prompt_tokens": self.backend.count_tokens(prompt),
            "completion_tokens": self.backend.count_tokens(text),
            "total_tokens": self.backend.count_tokens(prompt) + self.backend.count_tokens(text)
        }
            
        return response
        
    def _stream_chat_response(
        self, 
        prompt: str, 
        gen_params: Dict[str, Any],
        format: Optional[str], 
        functions: Optional[List[Dict[str, Any]]], 
        function_call: Union[str, Dict[str, str]]
    ) -> Iterator[Dict[str, Any]]:
        """Stream a chat response token by token."""
        response_id = f"chatcmpl-{id(self)}-{id(prompt)}"
        created = self.backend.get_timestamp()
        
        # Start generation stream
        stream = self.backend.generate_stream(prompt, **gen_params)
        accumulated_text = ""
        
        for chunk in stream:
            delta_text = chunk["text"]
            accumulated_text += delta_text
            
            # Prepare chunk response
            response_chunk = {
                "id": response_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": self.model_path,
                "choices": [{
                    "index": 0,
                    "delta": {
                        "content": delta_text
                    },
                    "finish_reason": None
                }]
            }
            
            # Add logprobs if requested
            if gen_params.get("logprobs") and "logprobs" in chunk:
                response_chunk["choices"][0]["logprobs"] = chunk["logprobs"]
                
            yield response_chunk
            
        # Final chunk with finish reason
        if functions and function_call != "none":
            try:
                function_calls = parse_function_calls(accumulated_text)
                if function_calls:
                    # Last chunk contains function call
                    yield {
                        "id": response_id,
                        "object": "chat.completion.chunk",
                        "created": created,
                        "model": self.model_path,
                        "choices": [{
                            "index": 0,
                            "delta": {
                                "content": None,
                                "function_call": function_calls[0].to_dict()
                            },
                            "finish_reason": "function_call"
                        }]
                    }
                    return
            except Exception as e:
                logger.warning(f"Error parsing function calls in stream: {e}")
        
        # Last chunk with finish reason
        yield {
            "id": response_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": self.model_path,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
    
    def _execute_function_call(self, function_call: FunctionCall) -> str:
        """Execute a function call and return the result as a string."""
        if not self.function_registry.has_function(function_call.name):
            return f"Error: Function {function_call.name} not found"
        
        try:
            result = self.function_registry.execute(
                function_call.name, 
                function_call.arguments
            )
            
            # Convert result to string (usually JSON)
            if not isinstance(result, str):
                result = json.dumps(result)
                
            return result
        except Exception as e:
            return f"Error executing function {function_call.name}: {str(e)}"
    
    def _ensure_json_output(self, text: str) -> str:
        """Ensure the output is valid JSON, retry if needed."""
        try:
            # Try to parse as JSON
            json.loads(text)
            return text
        except json.JSONDecodeError:
            # Extract JSON-like content if the model added extra text
            import re
            json_match = re.search(r'(\{|\[).*?(\}|\])', text, re.DOTALL)
            if json_match:
                try:
                    json_str = json_match.group(0)
                    json.loads(json_str)  # Validate it's proper JSON
                    return json_str
                except json.JSONDecodeError:
                    pass
            
            # If extraction failed, retry with a correction prompt
            correction_prompt = f"""The previous response was not valid JSON. 
Please respond with only a valid JSON object or array.

Previous invalid response:
{text}

Valid JSON response:"""
            
            result = self.backend.generate(
                correction_prompt,
                temperature=0.2,  # Lower temperature for more precise output
                max_new_tokens=self.max_new_tokens,
                top_p=0.95,
                repetition_penalty=1.2
            )
            
            try:
                json.loads(result["text"])
                return result["text"]
            except json.JSONDecodeError:
                # If still failing, try one more extraction
                json_match = re.search(r'(\{|\[).*?(\}|\])', result["text"], re.DOTALL)
                if json_match:
                    try:
                        json_str = json_match.group(0)
                        json.loads(json_str)
                        return json_str
                    except json.JSONDecodeError:
                        pass
                
                # Return a simple valid JSON as fallback
                return '{"error": "Failed to generate valid JSON", "attempted_response": ' + json.dumps(text) + '}'
    
    def complete(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        logprobs: bool = False,
        top_logprobs: Optional[int] = None,
        format: Optional[str] = None,
    ) -> Union[Dict[str, Any], Iterator[Dict[str, Any]]]:
        """
        Generate a completion for the provided prompt.
        
        Args:
            prompt: The prompt to complete
            temperature: Sampling temperature (overrides instance setting)
            max_tokens: Maximum number of tokens to generate (overrides instance setting)
            stream: Whether to stream the response
            logprobs: Whether to return log probabilities
            top_logprobs: Number of top tokens to return logprobs for
            format: Output format ("json" for structured JSON output)
            
        Returns:
            A completion object or iterator of completion chunks if streaming
        """
        # Add text mode instruction for JSON if needed
        if format == "json":
            prompt = f"You must respond with valid JSON only, no other text.\n\n{prompt}"
            
        # Prepare generation parameters
        gen_params = {
            "temperature": temperature if temperature is not None else self.temperature,
            "max_new_tokens": max_tokens if max_tokens is not None else self.max_new_tokens,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "repetition_penalty": self.repetition_penalty,
            "stream": stream,
            "logprobs": logprobs,
            "top_logprobs": top_logprobs or 5 if logprobs else None,
        }
        
        if stream:
            return self._stream_completion_response(prompt, gen_params, format)
        else:
            return self._generate_completion_response(prompt, gen_params, format)
    
    def _generate_completion_response(
        self, 
        prompt: str, 
        gen_params: Dict[str, Any],
        format: Optional[str]
    ) -> Dict[str, Any]:
        """Generate a complete text completion response."""
        # Generate response from the backend
        result = self.backend.generate(prompt, **gen_params)
        text = result["text"]
        
        # Handle JSON formatting
        if format == "json":
            text = self._ensure_json_output(text)
            
        # Prepare response object
        response = {
            "id": f"cmpl-{id(self)}-{id(prompt)}",
            "object": "text_completion",
            "created": self.backend.get_timestamp(),
            "model": self.model_path,
            "choices": [{
                "text": text,
                "index": 0,
                "finish_reason": "stop"
            }]
        }
        
        # Add logprobs if requested
        if gen_params.get("logprobs"):
            response["choices"][0]["logprobs"] = result.get("logprobs", {})
            
        # Add usage information
        response["usage"] = {
            "prompt_tokens": self.backend.count_tokens(prompt),
            "completion_tokens": self.backend.count_tokens(text),
            "total_tokens": self.backend.count_tokens(prompt) + self.backend.count_tokens(text)
        }
            
        return response
        
    def _stream_completion_response(
        self, 
        prompt: str, 
        gen_params: Dict[str, Any],
        format: Optional[str]
    ) -> Iterator[Dict[str, Any]]:
        """Stream a text completion response token by token."""
        response_id = f"cmpl-{id(self)}-{id(prompt)}"
        created = self.backend.get_timestamp()
        
        # Start generation stream
        stream = self.backend.generate_stream(prompt, **gen_params)
        
        for chunk in stream:
            delta_text = chunk["text"]
            
            # Prepare chunk response
            response_chunk = {
                "id": response_id,
                "object": "text_completion.chunk",
                "created": created,
                "model": self.model_path,
                "choices": [{
                    "text": delta_text,
                    "index": 0,
                    "finish_reason": None
                }]
            }
            
            # Add logprobs if requested
            if gen_params.get("logprobs") and "logprobs" in chunk:
                response_chunk["choices"][0]["logprobs"] = chunk["logprobs"]
                
            yield response_chunk
            
        # Last chunk with finish reason
        yield {
            "id": response_id,
            "object": "text_completion.chunk",
            "created": created,
            "model": self.model_path,
            "choices": [{
                "text": "",
                "index": 0,
                "finish_reason": "stop"
            }]
        } 
