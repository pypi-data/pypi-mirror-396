# Changelog

All notable changes to Local LLM Kit will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- Future features and improvements will be listed here

## [0.1.3] - 2025-12-13

### Changed
- Switched to PEP 621 `pyproject.toml` packaging with lazy backend imports for optional deps
- Cleaned README feature list to ASCII and refreshed helper verification script

### Fixed
- Packaging artifacts and docs build flow to simplify release to PyPI

## [0.1.2] - 2024-03-15

### Added
- Comprehensive documentation in PyPI package
- Additional documentation files included in distribution

### Fixed
- Documentation visibility on PyPI

## [0.1.1] - 2024-03-15

### Added
- Comprehensive documentation system
- Detailed API reference documentation
- Step-by-step tutorials with code examples
- Best practices guide
- Supported models documentation
- Contributing guidelines
- PyPI packaging improvements

### Changed
- Reorganized documentation structure
- Enhanced README with more examples

## [0.1.0] - 2023-11-25

### Added
- Initial release of Local LLM Kit
- Chat and Completion API (OpenAI-compatible)
- Function calling with automatic execution
- Multiple model backend support:
  - Hugging Face Transformers
  - llama.cpp (GGUF models)
- Streaming response support
- Memory management with auto-truncation
- JSON mode and structured output
- Prompt formatting for different model architectures:
  - Llama 2 Chat
  - Mistral Instruct
  - Vicuna
  - ChatML
  - Plain Instruct
- Command line interface
- Basic examples and documentation

### Changed
- N/A (Initial release)

### Deprecated
- N/A (Initial release)

### Removed
- N/A (Initial release)

### Fixed
- N/A (Initial release)

### Security
- N/A (Initial release) 
