"""
ServiÃ§o do Chatbot IA - Design System ENAP
"""

import google.generativeai as genai
import json
import re
from django.conf import settings
from django.urls import reverse
from wagtail.models import Page
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from ..models import ChatbotConfig, ChatbotWidget, PaginaIndexada, ConversaChatbot


class ChatbotService:
    def __init__(self):
        self.config = ChatbotConfig.objects.first()
        if self.config and self.config.api_key_google:
            genai.configure(api_key=self.config.api_key_google)
            self.model = genai.GenerativeModel(self.config.modelo_ia)

    def indexar_todas_paginas(self):
        """Indexa todas as pÃ¡ginas pÃºblicas do site"""
        print("ğŸš€ Iniciando indexaÃ§Ã£o das pÃ¡ginas...")
        
        paginas = Page.objects.live().public()
        total_paginas = paginas.count()
        processadas = 0
        
        for pagina in paginas:
            try:
                # Extrai conteÃºdo da pÃ¡gina
                conteudo = self._extrair_conteudo_pagina(pagina)
                
                if not conteudo.strip():
                    print(f"âš ï¸  Pulando (sem conteÃºdo): {pagina.title}")
                    continue
                
                # Atualiza ou cria registro
                pagina_indexada, created = PaginaIndexada.objects.update_or_create(
                    pagina=pagina,
                    defaults={
                        'titulo': pagina.title,
                        'conteudo_texto': conteudo,
                        'url': pagina.full_url or pagina.url,
                        'tags': json.dumps(self._extrair_tags(pagina)),
                        'ativa': True
                    }
                )
                
                processadas += 1
                status = "âœ… Criada" if created else "ğŸ”„ Atualizada"
                print(f"{status}: {pagina.title} ({processadas}/{total_paginas})")
                
            except Exception as e:
                print(f"âŒ Erro ao indexar {pagina.title}: {e}")
        
        print(f"ğŸ‰ IndexaÃ§Ã£o concluÃ­da! {processadas} pÃ¡ginas processadas.")

    def _extrair_conteudo_pagina(self, pagina):
        """Extrai texto da pÃ¡gina"""
        conteudo = []
        
        # TÃ­tulo
        conteudo.append(pagina.title)
        
        # SEO Title e Description
        if hasattr(pagina, 'seo_title') and pagina.seo_title:
            conteudo.append(pagina.seo_title)
        if hasattr(pagina, 'search_description') and pagina.search_description:
            conteudo.append(pagina.search_description)
        
        # StreamField content (body)
        if hasattr(pagina, 'body'):
            for block in pagina.body:
                texto_block = self._extrair_texto_block(block)
                if texto_block:
                    conteudo.append(texto_block)
        
        # Outros campos de texto comuns
        campos_texto = [
            'intro', 'description', 'content', 'resumo', 'objetivo',
            'metodologia', 'publico_alvo', 'pre_requisitos', 'observacoes'
        ]
        
        for campo in campos_texto:
            if hasattr(pagina, campo):
                try:
                    valor = getattr(pagina, campo)
                    if isinstance(valor, str) and valor.strip():
                        # Remove HTML se houver
                        texto_limpo = BeautifulSoup(valor, 'html.parser').get_text()
                        if texto_limpo.strip():
                            conteudo.append(texto_limpo.strip())
                except:
                    continue
        
        return ' '.join(conteudo)

    def _extrair_texto_block(self, block):
        """Extrai texto de um StreamField block"""
        texto_extraido = []
        
        if hasattr(block, 'value'):
            if isinstance(block.value, str):
                # Remove HTML
                texto_limpo = BeautifulSoup(block.value, 'html.parser').get_text()
                texto_extraido.append(texto_limpo.strip())
            
            elif isinstance(block.value, dict):
                # Navega pelo dicionÃ¡rio procurando texto
                for key, value in block.value.items():
                    if isinstance(value, str) and value.strip():
                        texto_limpo = BeautifulSoup(value, 'html.parser').get_text()
                        if texto_limpo.strip():
                            texto_extraido.append(texto_limpo.strip())
        
        return ' '.join(texto_extraido)

    def _extrair_tags(self, pagina):
        """Extrai tags/palavras-chave da pÃ¡gina"""
        tags = []
        
        # Tags do Wagtail se existirem
        if hasattr(pagina, 'tags') and hasattr(pagina.tags, 'all') and pagina.tags.exists():
            tags.extend([str(tag.name) for tag in pagina.tags.all()])
        
        # Categoria se existir
        if hasattr(pagina, 'categoria') and pagina.categoria:
            tags.append(str(pagina.categoria))
        
        # Tipo da pÃ¡gina (convertido para string)
        if hasattr(pagina._meta, 'verbose_name'):
            tags.append(str(pagina._meta.verbose_name))
        
        tags.append(str(pagina.__class__.__name__))
        
        # Garante que todas as tags sÃ£o strings
        tags = [str(tag) for tag in tags if tag]
        
        return tags

    def buscar_paginas_relevantes(self, pergunta, limite=5):
        """Busca pÃ¡ginas mais relevantes para a pergunta"""
        paginas = PaginaIndexada.objects.filter(ativa=True)
        
        if not paginas.exists():
            return []
        
        # Prepara textos para anÃ¡lise
        documentos = []
        paginas_list = list(paginas)
        
        for pagina in paginas_list:
            texto_completo = f"{pagina.titulo} {pagina.conteudo_texto}"
            documentos.append(texto_completo)
        
        # Adiciona a pergunta
        documentos.append(pergunta)
        
        # Calcula similaridade usando TF-IDF
        try:
            vectorizer = TfidfVectorizer(
                stop_words=None,  # VocÃª pode adicionar stop words em portuguÃªs
                max_features=5000,
                ngram_range=(1, 2),
                lowercase=True
            )
            
            tfidf_matrix = vectorizer.fit_transform(documentos)
            
            # Calcula similaridade da pergunta com cada pÃ¡gina
            pergunta_vector = tfidf_matrix[-1]  # Ãšltimo Ã© a pergunta
            paginas_vectors = tfidf_matrix[:-1]  # Todos menos o Ãºltimo
            
            similarities = cosine_similarity(pergunta_vector, paginas_vectors).flatten()
            
            # Ordena por similaridade
            indices_ordenados = similarities.argsort()[::-1][:limite]
            
            # Filtra apenas pÃ¡ginas com similaridade > 0.1
            paginas_relevantes = []
            for idx in indices_ordenados:
                if similarities[idx] > 0.1:
                    paginas_relevantes.append({
                        'pagina': paginas_list[idx],
                        'score': float(similarities[idx])
                    })
            
            return paginas_relevantes
            
        except Exception as e:
            print(f"Erro na busca semÃ¢ntica: {e}")
            # Fallback: busca simples por palavras-chave
            return self._busca_simples(pergunta, paginas_list, limite)

    def _busca_simples(self, pergunta, paginas_list, limite):
        """Busca simples por palavras-chave como fallback"""
        palavras = pergunta.lower().split()
        resultados = []
        
        for pagina in paginas_list:
            texto_busca = f"{pagina.titulo} {pagina.conteudo_texto}".lower()
            score = sum(1 for palavra in palavras if palavra in texto_busca)
            
            if score > 0:
                resultados.append({
                    'pagina': pagina,
                    'score': score / len(palavras)
                })
        
        # Ordena por score e retorna os melhores
        resultados.sort(key=lambda x: x['score'], reverse=True)
        return resultados[:limite]

    def gerar_resposta(self, pergunta, sessao_id=None, user_ip=None):
        """Gera resposta usando Google AI"""
        if not self.config or not self.config.api_key_google:
            return {
                'resposta': 'Chatbot nÃ£o configurado. Configure a API key do Google AI Studio no admin.',
                'paginas': []
            }
        
        # Busca pÃ¡ginas relevantes
        paginas_relevantes = self.buscar_paginas_relevantes(pergunta)
        
        # Monta contexto
        contexto = self._montar_contexto(paginas_relevantes)
        
        # Monta prompt
        prompt = f"""
{self.config.prompt_sistema}

CONTEXTO DAS PÃGINAS DO SITE ENAP:
{contexto}

PERGUNTA DO USUÃRIO: {pergunta}

INSTRUÃ‡Ã•ES:
1. Responda de forma clara e objetiva em portuguÃªs
2. Use apenas informaÃ§Ãµes do contexto fornecido
3. Se mencionar uma pÃ¡gina, indique que hÃ¡ links relacionados disponÃ­veis
4. Se nÃ£o souber a resposta com base no contexto, seja honesto
5. Mantenha um tom profissional e amigÃ¡vel da ENAP
6. Limite a resposta a 200 palavras

RESPOSTA:
"""
        
        try:
            # Gera resposta com Gemini
            response = self.model.generate_content(prompt)
            resposta_texto = response.text
            
            # Salva conversa
            if sessao_id:
                ConversaChatbot.objects.create(
                    sessao_id=sessao_id,
                    usuario_ip=user_ip,
                    mensagem_usuario=pergunta,
                    resposta_bot=resposta_texto,
                    paginas_referenciadas=json.dumps([
                        {'titulo': p['pagina'].titulo, 'url': p['pagina'].url} 
                        for p in paginas_relevantes
                    ])
                )
            
            return {
                'resposta': resposta_texto,
                'paginas': [
                    {
                        'titulo': p['pagina'].titulo,
                        'url': p['pagina'].url,
                        'score': p['score']
                    }
                    for p in paginas_relevantes
                ]
            }
            
        except Exception as e:
            return {
                'resposta': f'Desculpe, ocorreu um erro ao processar sua pergunta. Tente novamente em alguns instantes.',
                'paginas': []
            }

    def _montar_contexto(self, paginas_relevantes):
        """Monta contexto das pÃ¡ginas para o prompt"""
        if not paginas_relevantes:
            return "Nenhuma pÃ¡gina relevante encontrada no site da ENAP."
        
        contexto_parts = []
        for item in paginas_relevantes[:3]:  # Limita a 3 pÃ¡ginas para nÃ£o estourar o contexto
            pagina = item['pagina']
            # Limita o texto para nÃ£o estourar o limite do modelo
            conteudo_resumido = pagina.conteudo_texto[:800] + "..." if len(pagina.conteudo_texto) > 800 else pagina.conteudo_texto
            
            contexto_parts.append(f"""
PÃGINA: {pagina.titulo}
URL: {pagina.url}
CONTEÃšDO: {conteudo_resumido}
---
""")
        
        return '\n'.join(contexto_parts)
    

    def indexar_pagina_especifica(self, page):
        """Indexa uma pÃ¡gina especÃ­fica do Wagtail (para uso com signals)"""
        try:
            # SÃ³ indexa se for pÃ¡gina live e pÃºblica
            if not (page.live and hasattr(page, 'get_url_parts')):
                return False
                
            # Extrai conteÃºdo usando o mÃ©todo existente
            conteudo = self._extrair_conteudo_pagina(page)
            
            if not conteudo.strip():
                print(f"âš ï¸  PÃ¡gina sem conteÃºdo: {page.title}")
                return False
                
            # Tenta obter URL
            try:
                url = page.full_url if hasattr(page, 'full_url') and page.full_url else page.url
            except:
                url = f"/pagina/{page.id}/"  # URL fallback
            
            # Atualiza ou cria registro (usando a mesma lÃ³gica do mÃ©todo existente)
            pagina_indexada, created = PaginaIndexada.objects.update_or_create(
                pagina=page,  # Sua estrutura usa 'pagina' como FK
                defaults={
                    'titulo': page.title,
                    'conteudo_texto': conteudo,
                    'url': url,
                    'tags': json.dumps(self._extrair_tags(page)),
                    'ativa': True
                }
            )
            
            status = "âœ… Criada" if created else "ğŸ”„ Atualizada"
            print(f"ğŸ¤– {status}: {page.title}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Erro ao indexar pÃ¡gina {page.title}: {e}")
            return False