#!/usr/bin/env python3
"""
PDFä¸‹è½½å™¨ä¸»ç¨‹åº
ç‹¬ç«‹çš„æ–‡çŒ®PDFä¸‹è½½å·¥å…·
"""

import argparse
import json
import logging
import time
from pathlib import Path

from .config import DEFAULT_SEARCH_LIMIT, DEFAULT_SOURCE, DELAY, TIMEOUT
from .counter import PMCIDCounter
from .fetcher import PaperFetcher
from .formatter import StatsFormatter
from .logger import get_main_logger
from .manager import UnifiedDownloadManager


def log_download_stats(logger, results: list[dict]) -> dict:
    """è®°å½•ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯å¹¶è¿”å›ç»Ÿè®¡ç»“æœ"""
    success_count = sum(1 for r in results if r.get("success"))
    pdf_count = sum(1 for r in results if r.get("path"))
    html_count = sum(1 for r in results if r.get("full_text_url"))

    logger.info("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
    logger.info(f"   æ€»è®¡: {len(results)}")
    logger.info(f"   æˆåŠŸ: {success_count}")
    logger.info(f"   PDF: {pdf_count}")
    logger.info(f"   HTML: {html_count}")
    logger.info(f"   å¤±è´¥: {len(results) - success_count}")

    return {
        "total": len(results),
        "success_count": success_count,
        "pdf_count": pdf_count,
        "html_count": html_count,
    }


def main() -> None:
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="PDFæ–‡çŒ®ä¸‹è½½å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ç»Ÿè®¡æ–‡çŒ®çš„PMCIDæƒ…å†µ
  python -m pdfget -s "machine learning cancer" -l 5000

  # æœç´¢å¹¶ä¸‹è½½å‰Nç¯‡æ–‡çŒ®
  python -m pdfget -s "deep learning" -l 20 -d

  # å¹¶å‘ä¸‹è½½ï¼ˆå¤šçº¿ç¨‹ï¼‰
  python -m pdfget -s "cancer immunotherapy" -l 20 -d -t 5
  python -m pdfget -i dois.csv -t 3

  # ä¸‹è½½å•ä¸ªæ–‡çŒ®
  python -m pdfget --doi 10.1016/j.cell.2020.01.021
        """,
    )

    # è¾“å…¥é€‰é¡¹
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--doi", help="å•ä¸ªDOI")
    group.add_argument("-i", help="è¾“å…¥æ–‡ä»¶ï¼ˆCSVæˆ–TXTï¼‰")
    group.add_argument("-s", help="æœç´¢æ–‡çŒ®")

    # å¯é€‰å‚æ•°

    parser.add_argument("-c", default="doi", help="CSVåˆ—åï¼ˆé»˜è®¤: doiï¼‰")
    parser.add_argument("-o", default="data/pdfs", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--delay", type=float, default=DELAY, help="è¯·æ±‚å»¶è¿Ÿç§’æ•°")
    parser.add_argument(
        "-l", type=int, default=DEFAULT_SEARCH_LIMIT, help="è¦å¤„ç†çš„æ–‡çŒ®æ•°é‡"
    )
    parser.add_argument("-d", action="store_true", help="ä¸‹è½½PDF")
    parser.add_argument("-t", type=int, default=3, help="å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤3ï¼‰")
    parser.add_argument("-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    parser.add_argument(
        "-S",
        choices=["pubmed", "europe_pmc", "both"],
        default=DEFAULT_SOURCE,
        help=f"æ•°æ®æºï¼ˆé»˜è®¤: {DEFAULT_SOURCE}ï¼‰",
    )
    parser.add_argument(
        "--format",
        choices=["console", "json", "markdown"],
        help="ç»Ÿè®¡è¾“å‡ºæ ¼å¼",
    )

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    logger = get_main_logger()
    if args.v:
        logger.setLevel(logging.DEBUG)

    # åˆå§‹åŒ–ä¸‹è½½å™¨
    fetcher = PaperFetcher(
        cache_dir="data/cache", output_dir="data/pdfs", default_source=args.S
    )

    logger.info("ğŸš€ PDFä¸‹è½½å™¨å¯åŠ¨")
    logger.info(f"   è¾“å‡ºç›®å½•: {args.o}")

    try:
        if args.doi:
            # å•ä¸ªDOIä¸‹è½½
            logger.info(f"\nğŸ“„ ä¸‹è½½å•ä¸ªæ–‡çŒ®: {args.doi}")

            # å…ˆæœç´¢è·å–PMCID
            papers = fetcher.search_papers(
                args.doi, limit=1, source=fetcher.default_source
            )
            if papers and papers[0].get("pmcid"):
                paper = papers[0]
                pmcid = paper["pmcid"]
                doi = paper["doi"]

                # ä½¿ç”¨PDFDownloaderç›´æ¥ä¸‹è½½
                result = fetcher.pdf_downloader.download_pdf(pmcid, doi)

                # åˆå¹¶ç»“æœä¿¡æ¯
                result["doi"] = doi
                result["pmcid"] = pmcid
                result["title"] = paper.get("title")

                if result.get("success"):
                    logger.info("âœ… ä¸‹è½½æˆåŠŸ!")
                    if result.get("path"):
                        logger.info(f"   PDFè·¯å¾„: {result['path']}")
                    else:
                        logger.info(f"   HTMLé“¾æ¥: {result.get('full_text_url')}")
                else:
                    logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {result.get('error', 'Unknown error')}")
            else:
                logger.error(f"âŒ æœªæ‰¾åˆ°æ–‡çŒ®æˆ–æ— PMCID: {args.doi}")

        elif args.s:
            # æœç´¢æ–‡çŒ®
            logger.info(f"\nğŸ” æœç´¢æ–‡çŒ®: {args.s} (æ•°æ®æº: {args.S})")

            # å¦‚æœéœ€è¦ä¸‹è½½PDFï¼Œåˆ™åªæœç´¢å°‘é‡æ–‡çŒ®
            # å¦‚æœä¸éœ€è¦ä¸‹è½½ï¼Œåˆ™è¿›è¡Œå…¨é‡ç»Ÿè®¡
            if args.d:
                # ä¸‹è½½æ¨¡å¼ï¼šåªè·å–å‰lç¯‡æ–‡çŒ®
                fetch_pmcid = args.S == "pubmed"
                papers = fetcher.search_papers(
                    args.s, limit=args.l, source=args.S, fetch_pmcid=fetch_pmcid
                )

                if not papers:
                    logger.error("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡çŒ®")
                    exit(1)

                # æ˜¾ç¤ºæœç´¢ç»“æœ
                logger.info(f"\nğŸ“Š æœç´¢ç»“æœ ({len(papers)} ç¯‡):")
                for i, paper in enumerate(papers, 1):
                    logger.info(f"\n{i}. {paper['title']}")
                    logger.info(
                        f"   ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}"
                    )
                    logger.info(f"   æœŸåˆŠ: {paper['journal']} ({paper['year']})")
                    if paper["doi"]:
                        logger.info(f"   DOI: {paper['doi']}")
                    logger.info(f"   PMCID: {paper.get('pmcid', 'æ— ')}")
                    logger.info(f"   å¼€æ”¾è·å–: {'æ˜¯' if paper.get('pmcid') else 'å¦'}")

                # ä¿å­˜æœç´¢ç»“æœ
                search_results_file = (
                    Path(args.o) / f"search_results_{int(time.time())}.json"
                )
                search_results_file.parent.mkdir(parents=True, exist_ok=True)

                with open(search_results_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "query": args.s,
                            "timestamp": time.time(),
                            "total": len(papers),
                            "results": papers,
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )

                logger.info(f"\nğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {search_results_file}")

            else:
                # ç»Ÿè®¡æ¨¡å¼ï¼šè·å–å…¨éƒ¨æ–‡çŒ®çš„PMCIDä¿¡æ¯
                try:
                    import config

                    email = getattr(config, "NCBI_EMAIL", None)
                    api_key = getattr(config, "NCBI_API_KEY", None)
                except ImportError:
                    # å¦‚æœæ²¡æœ‰å¤–éƒ¨é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    email = None
                    api_key = None

                counter = PMCIDCounter(email=email, api_key=api_key)

                # æ‰§è¡Œç»Ÿè®¡
                stats = counter.count_pmcid(args.s, limit=args.l)

                # æ ¼å¼åŒ–è¾“å‡º
                if args.format and args.format != "console":
                    formatted_output = StatsFormatter.format(stats, args.format)
                    print(formatted_output)

                    # ä¿å­˜æŠ¥å‘Š
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    filename = f"pmcid_stats_{timestamp}"
                    StatsFormatter.save_report(stats, filename, args.format)
                else:
                    # ç®€å•çš„æ§åˆ¶å°è¾“å‡º
                    print("\nğŸ“ˆ PMCIDç»Ÿè®¡ç»“æœ:")
                    print(f"   æŸ¥è¯¢: {stats['query']}")
                    print(f"   æ€»æ–‡çŒ®æ•°: {stats['total']:,} ç¯‡")
                    print(f"   æ£€æŸ¥äº†: {stats['checked']:,} ç¯‡ (ç”±-lå‚æ•°æŒ‡å®š)")
                    print(
                        f"   å…¶ä¸­æœ‰PMCID: {stats['with_pmcid']:,} ç¯‡ ({stats['rate']:.1f}%)"
                    )
                    print(f"   æ— PMCID: {stats['without_pmcid']:,} ç¯‡")
                    print(f"   è€—æ—¶: {stats['elapsed_seconds']:.1f} ç§’")
                    print(
                        f"   å¤„ç†é€Ÿåº¦: {stats['checked'] / stats['elapsed_seconds']:.1f} ç¯‡/ç§’"
                    )

                    if stats["with_pmcid"] > 0:
                        print("\nğŸ’¾ å¦‚æœä¸‹è½½æ‰€æœ‰å¼€æ”¾è·å–æ–‡çŒ®:")
                        print(f"   æ–‡ä»¶æ•°é‡: {stats['with_pmcid']:,} ä¸ªPDF")
                        size_mb = stats["estimated_size_mb"]
                        size_gb = size_mb / 1024
                        print(f"   ä¼°ç®—å¤§å°: {size_mb:.1f} MB ({size_gb:.2f} GB)")

                    # å¦‚æœæ£€æŸ¥çš„æ ·æœ¬æ•°å°äºæ€»æ•°ï¼Œæä¾›è¯´æ˜
                    if stats["checked"] < stats["total"]:
                        print(
                            f"\nğŸ“ è¯´æ˜: ä»…æ£€æŸ¥äº†å‰ {stats['checked']:,} ç¯‡æ–‡çŒ®çš„PMCIDçŠ¶æ€"
                        )

                return

            # ä¸‹è½½PDF
            logger.info("\nğŸ“¥ å¼€å§‹ä¸‹è½½PDF...")

            # åªä¸‹è½½æœ‰PMCIDçš„å¼€æ”¾è·å–æ–‡çŒ®
            oa_papers = [p for p in papers if p.get("pmcid")]
            logger.info(f"   æ‰¾åˆ° {len(oa_papers)} ç¯‡å¼€æ”¾è·å–æ–‡çŒ®")

            if oa_papers:
                # ä½¿ç”¨ç»Ÿä¸€ä¸‹è½½ç®¡ç†å™¨
                download_manager = UnifiedDownloadManager(
                    fetcher=fetcher,
                    max_workers=args.t,
                    base_delay=args.delay,
                )
                results = download_manager.download_batch(oa_papers, timeout=TIMEOUT)

                # ç»Ÿè®¡ç»“æœ
                stats = log_download_stats(logger, results)

                # ä¿å­˜ä¸‹è½½ç»“æœ
                if stats["success_count"] > 0:
                    download_results_file = Path(args.o) / "download_results.json"
                    with open(download_results_file, "w", encoding="utf-8") as f:
                        json.dump(
                            {
                                "timestamp": time.time(),
                                "total": stats["total"],
                                "success": stats["success_count"],
                                "results": results,
                            },
                            f,
                            indent=2,
                            ensure_ascii=False,
                        )

                    logger.info(f"\nğŸ’¾ ä¸‹è½½ç»“æœå·²ä¿å­˜åˆ°: {download_results_file}")

        else:
            # æ‰¹é‡ä¸‹è½½
            logger.info(f"\nğŸ“š æ‰¹é‡ä¸‹è½½: {args.i}")

            # è¯»å–DOIåˆ—è¡¨
            input_path = Path(args.i)
            if not input_path.exists():
                logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.i}")
                exit(1)

            if input_path.suffix.lower() == ".csv":
                # è¯»å–CSVæ–‡ä»¶
                import pandas as pd

                try:
                    df = pd.read_csv(input_path)
                    if args.c not in df.columns:
                        logger.error(f"âŒ CSVæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°åˆ—: {args.c}")
                        exit(1)

                    dois = df[args.c].dropna().unique().tolist()
                    logger.info(f"   æ‰¾åˆ° {len(dois)} ä¸ªå”¯ä¸€DOI")

                except Exception as e:
                    logger.error(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
                    exit(1)

            else:
                # è¯»å–æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªDOIï¼‰
                try:
                    with open(input_path) as f:
                        dois = [line.strip() for line in f if line.strip()]
                    logger.info(f"   æ‰¾åˆ° {len(dois)} ä¸ªDOI")

                except Exception as e:
                    logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                    exit(1)

            # ä½¿ç”¨ç»Ÿä¸€ä¸‹è½½ç®¡ç†å™¨
            download_manager = UnifiedDownloadManager(
                fetcher=fetcher,
                max_workers=args.t,
                base_delay=args.delay,
            )
            results = download_manager.download_batch(dois, timeout=TIMEOUT)

            # ç»Ÿè®¡ç»“æœ
            stats = log_download_stats(logger, results)

            # ä¿å­˜ç»“æœ
            if stats["success_count"] > 0:
                output_file = Path(args.o) / "download_results.json"
                output_file.parent.mkdir(parents=True, exist_ok=True)

                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "timestamp": time.time(),
                            "total": stats["total"],
                            "success": stats["success_count"],
                            "results": results,
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )

                logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        exit(1)
    except Exception as e:
        logger.error(f"\nğŸ’¥ å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        exit(1)

    logger.info("\nâœ¨ ä¸‹è½½å®Œæˆ")
    exit(0)


if __name__ == "__main__":
    main()
