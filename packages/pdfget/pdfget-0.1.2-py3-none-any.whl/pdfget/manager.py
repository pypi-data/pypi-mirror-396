#!/usr/bin/env python3
"""
ç»Ÿä¸€ä¸‹è½½ç®¡ç†å™¨
æ ¹æ®å‚æ•°è‡ªåŠ¨é€‰æ‹©å•çº¿ç¨‹æˆ–å¤šçº¿ç¨‹ä¸‹è½½ç­–ç•¥
"""

import random
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any

from .config import DOWNLOAD_BASE_DELAY, DOWNLOAD_RANDOM_DELAY
from .fetcher import PaperFetcher
from .logger import get_logger


class UnifiedDownloadManager:
    """ç»Ÿä¸€ä¸‹è½½ç®¡ç†å™¨ï¼Œæ”¯æŒå•çº¿ç¨‹å’Œå¤šçº¿ç¨‹ä¸‹è½½"""

    def __init__(
        self,
        fetcher: PaperFetcher,
        max_workers: int = 1,
        base_delay: float = DOWNLOAD_BASE_DELAY,
        random_delay_range: float = DOWNLOAD_RANDOM_DELAY,
    ):
        """
        åˆå§‹åŒ–ä¸‹è½½ç®¡ç†å™¨

        Args:
            fetcher: PaperFetcherå®ä¾‹
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆ1è¡¨ç¤ºå•çº¿ç¨‹ï¼‰
            base_delay: åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            random_delay_range: éšæœºå»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
        """
        self.logger = get_logger(__name__)
        self.fetcher = fetcher
        self.max_workers = max_workers
        self.base_delay = base_delay
        self.random_delay_range = random_delay_range

        # çº¿ç¨‹å®‰å…¨çš„è¿›åº¦è·Ÿè¸ªï¼ˆä»…å¤šçº¿ç¨‹ä½¿ç”¨ï¼‰
        self._lock = threading.Lock()
        self._completed = 0
        self._successful = 0
        self._failed = 0
        self._pdf_count = 0
        self._total = 0

    def _normalize_input(
        self, items: list[str] | list[dict]
    ) -> tuple[list[dict], list[str]]:
        """
        æ ‡å‡†åŒ–è¾“å…¥æ ¼å¼

        Args:
            items: DOIåˆ—è¡¨æˆ–è®ºæ–‡ä¿¡æ¯åˆ—è¡¨

        Returns:
            (è®ºæ–‡ä¿¡æ¯åˆ—è¡¨, DOIåˆ—è¡¨)
        """
        papers: list[dict] = []
        if items and isinstance(items[0], dict):
            # è¾“å…¥æ˜¯è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            papers = items  # type: ignore
            dois = [p["doi"] for p in papers if p.get("doi")]  # type: ignore
        else:
            # è¾“å…¥æ˜¯DOIåˆ—è¡¨
            papers = [{"doi": d} for d in items]
            dois = items  # type: ignore

        return papers, dois

    def _get_delay(self) -> float:
        """è·å–éšæœºå»¶è¿Ÿæ—¶é—´ï¼Œé¿å…åŒæ­¥è¯·æ±‚"""
        if self.random_delay_range > 0:
            random_delay = random.uniform(0, self.random_delay_range)
            return self.base_delay + random_delay
        return self.base_delay

    def _download_sequential(
        self, papers: list[dict], delay: float = 1.0, timeout: int = 30
    ) -> list[dict]:
        """
        å•çº¿ç¨‹é¡ºåºä¸‹è½½

        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            delay: è¯·æ±‚é—´å»¶è¿Ÿï¼ˆç§’ï¼‰
            timeout: å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´

        Returns:
            ä¸‹è½½ç»“æœåˆ—è¡¨
        """
        self.logger.info(f"ğŸš€ å•çº¿ç¨‹ä¸‹è½½ {len(papers)} ç¯‡æ–‡çŒ®")
        results = []

        for i, paper in enumerate(papers, 1):
            doi = paper["doi"] if isinstance(paper, dict) else paper
            pmcid = paper.get("pmcid") if isinstance(paper, dict) else None

            self.logger.info(f"\nğŸ“„ è¿›åº¦: {i}/{len(papers)}")

            try:
                # ç›´æ¥ä½¿ç”¨PDFDownloaderä¸‹è½½
                if pmcid:
                    result = self.fetcher.pdf_downloader.download_pdf(pmcid, doi)
                else:
                    # å¦‚æœæ²¡æœ‰PMCIDï¼Œå°è¯•æœç´¢
                    papers = self.fetcher.search_papers(doi, limit=1)
                    if papers and papers[0].get("pmcid"):
                        pmcid = papers[0]["pmcid"]
                        result = self.fetcher.pdf_downloader.download_pdf(pmcid, doi)
                    else:
                        result = {"success": False, "error": "No PMCID found"}

                # æ·»åŠ å¿…è¦çš„ä¿¡æ¯
                result["doi"] = doi
                result["pmcid"] = pmcid or ""
                results.append(result)
            except Exception as e:
                self.logger.error(f"è·å–æ–‡çŒ®å¤±è´¥ ({doi}): {e}")
                results.append({"doi": doi, "success": False, "error": str(e)})

            # å»¶è¿Ÿï¼Œé¿å…è¢«é™åˆ¶
            if i < len(papers):
                time.sleep(delay)

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r.get("success"))
        self.logger.info(f"\nğŸ“Š å•çº¿ç¨‹ä¸‹è½½å®Œæˆ: {success_count}/{len(papers)} æˆåŠŸ")

        return results

    def _update_progress(
        self, success: bool = False, pdf_downloaded: bool = False
    ) -> None:
        """çº¿ç¨‹å®‰å…¨çš„è¿›åº¦æ›´æ–°"""
        with self._lock:
            self._completed += 1
            if success:
                self._successful += 1
                if pdf_downloaded:
                    self._pdf_count += 1
            else:
                self._failed += 1

            # ç®€å•çš„è¿›åº¦æ˜¾ç¤º
            progress = (self._completed / self._total) * 100
            self.logger.info(
                f"  è¿›åº¦: {self._completed}/{self._total} ({progress:.1f}%) "
                f"æˆåŠŸ: {self._successful} PDF: {self._pdf_count} å¤±è´¥: {self._failed}"
            )

    def _create_thread_fetcher(self) -> PaperFetcher:
        """ä¸ºçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„fetcherå®ä¾‹"""
        # å¤åˆ¶åŸºç¡€é…ç½®ï¼Œä½†åˆ›å»ºæ–°çš„session
        fetcher = PaperFetcher(
            cache_dir=str(self.fetcher.cache_dir),
            output_dir=str(self.fetcher.output_dir),
        )
        return fetcher

    def _download_concurrent(self, papers: list[dict], timeout: int = 30) -> list[dict]:
        """
        å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½

        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            timeout: å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´

        Returns:
            ä¸‹è½½ç»“æœåˆ—è¡¨
        """
        self.logger.info(
            f"ğŸš€ å¯åŠ¨å¹¶å‘ä¸‹è½½ï¼š{len(papers)} ç¯‡æ–‡çŒ®ï¼Œ{self.max_workers} ä¸ªå¹¶å‘çº¿ç¨‹"
        )

        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
        self._total = len(papers)
        self._completed = 0
        self._successful = 0
        self._failed = 0
        self._pdf_count = 0

        results = []
        doi_list = [p["doi"] for p in papers]

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘ä¸‹è½½
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_doi = {}

            for paper in papers:
                # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„fetcher
                thread_fetcher = self._create_thread_fetcher()
                doi = paper["doi"] if isinstance(paper, dict) else paper
                pmcid = paper.get("pmcid") if isinstance(paper, dict) else None

                future = executor.submit(
                    self._download_single_task,
                    doi,
                    pmcid,
                    thread_fetcher,
                    timeout,
                )
                future_to_doi[future] = doi

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_doi):
                doi = future_to_doi[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"å¹¶å‘ä¸‹è½½å¼‚å¸¸ ({doi}): {str(e)}")
                    results.append({"doi": doi, "success": False, "error": str(e)})

        # æŒ‰åŸå§‹DOIé¡ºåºé‡æ–°æ’åˆ—ç»“æœ
        doi_to_result = {r["doi"]: r for r in results}
        ordered_results = [
            doi_to_result.get(doi, {"doi": doi, "success": False, "error": "Not found"})
            for doi in doi_list
        ]

        # æœ€ç»ˆç»Ÿè®¡
        self.logger.info("\nğŸ“Š å¹¶å‘ä¸‹è½½å®Œæˆ:")
        self.logger.info(f"   æ€»è®¡: {len(ordered_results)}")
        self.logger.info(f"   æˆåŠŸ: {self._successful}")
        self.logger.info(f"   PDF: {self._pdf_count}")
        self.logger.info(f"   å¤±è´¥: {self._failed}")
        if len(ordered_results) > 0:
            self.logger.info(
                f"   æˆåŠŸç‡: {(self._successful / len(ordered_results)) * 100:.1f}%"
            )

        return ordered_results

    def _download_single_task(
        self, doi: str, pmcid: str | None, fetcher: PaperFetcher, timeout: int = 30
    ) -> dict[str, Any]:
        """å•ä¸ªæ–‡çŒ®çš„ä¸‹è½½ä»»åŠ¡ï¼ˆçº¿ç¨‹æ± ä¸­çš„ä»»åŠ¡ï¼‰"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            time.sleep(self._get_delay())

            # ç›´æ¥ä½¿ç”¨PDFDownloaderä¸‹è½½
            if pmcid:
                result = fetcher.pdf_downloader.download_pdf(pmcid, doi)
            else:
                # å¦‚æœæ²¡æœ‰PMCIDï¼Œå°è¯•æœç´¢
                papers = fetcher.search_papers(doi, limit=1)
                if papers and papers[0].get("pmcid"):
                    pmcid = papers[0]["pmcid"]
                    result = fetcher.pdf_downloader.download_pdf(pmcid, doi)
                else:
                    result = {"success": False, "error": "No PMCID found"}

            # æ·»åŠ å¿…è¦çš„ä¿¡æ¯
            result["doi"] = doi
            result["pmcid"] = pmcid or ""

            # æ›´æ–°è¿›åº¦
            success = result.get("success", False)
            pdf_downloaded = bool(result.get("path"))
            self._update_progress(success, pdf_downloaded)

            return result

        except Exception as e:
            self.logger.debug(f"ä¸‹è½½å¤±è´¥ ({doi}): {str(e)}")
            self._update_progress(False)
            return {"doi": doi, "success": False, "error": str(e)}

    def download_batch(
        self,
        items: list[str] | list[dict],
        delay: float | None = None,
        timeout: int = 30,
    ) -> list[dict[str, Any]]:
        """
        æ‰¹é‡ä¸‹è½½æ–‡çŒ®ï¼ˆè‡ªåŠ¨é€‰æ‹©å•çº¿ç¨‹æˆ–å¤šçº¿ç¨‹ï¼‰

        Args:
            items: DOIåˆ—è¡¨æˆ–è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            delay: è¯·æ±‚é—´å»¶è¿Ÿï¼ˆç§’ï¼Œä»…å•çº¿ç¨‹ä½¿ç”¨ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            timeout: å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´

        Returns:
            ä¸‹è½½ç»“æœåˆ—è¡¨
        """
        if not items:
            return []

        # æ ‡å‡†åŒ–è¾“å…¥
        papers, dois = self._normalize_input(items)

        if not dois:
            self.logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„DOIå¯ä»¥ä¸‹è½½")
            return []

        # æ ¹æ®å‚æ•°é€‰æ‹©ä¸‹è½½ç­–ç•¥
        if self.max_workers > 1 and len(papers) > 1:
            # å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½
            return self._download_concurrent(papers, timeout)
        else:
            # å•çº¿ç¨‹é¡ºåºä¸‹è½½
            use_delay = delay if delay is not None else self.base_delay
            return self._download_sequential(papers, use_delay, timeout)
