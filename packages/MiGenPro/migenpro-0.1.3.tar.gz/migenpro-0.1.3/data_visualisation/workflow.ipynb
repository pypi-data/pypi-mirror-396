{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MiGenPro Notebook Workflow\n",
    "#### Set your configuration below and run each step as needed\n"
   ],
   "id": "b28e35febcce4b3f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from migenpro.ml.parameter_optimisation import ParameterOptimisation\n",
    "from migenpro.querying.query_executor import QueryExecutor\n",
    "from migenpro.querying.genome_annotation import GenomeAnnotationWorkflow\n",
    "from migenpro.querying.query_executor import QueryExecutor\n",
    "from migenpro.ml.machine_learning_main import FeatureMatrix, PhenotypeMatrix, MachineLearningModels\n",
    "from migenpro.querying.query_parser import QueryParser\n",
    "import os\n",
    "\n",
    "\n",
    "# Configuration - Set your parameters here\n",
    "config = {\n",
    "    \"output_dir\": \"./output/\",  # Main output directory\n",
    "    \"debug\": True,             # Enable debug mode\n",
    "\n",
    "    \"dataset_bin\": \"../binaries/datasets\",        # Path to ncbi-datasets-cli\n",
    "    \"threads\": 2,\n",
    "\n",
    "    # Data formatting parameters\n",
    "    \"phenotype_query_file\": \"sparql_phenotype:motility.sparql\",\n",
    "    \"sapp_jar\": \"./binaries/SAPP-2.0.jar\",\n",
    "    \"phenotype_hdt_file\": \"../data/bacdive.hdt.gz\",\n",
    "    \"abs_frequency\": 200,\n",
    "    \"species_frequency\": 10,\n",
    "\n",
    "    # Genome querying parameters\n",
    "    \"genome_query_file\": \"sparql_genome:DomainCopyNumber.sparql\",\n",
    "\n",
    "    # Machine learning parameters\n",
    "    \"feature_matrix\": \"./output/feature_matrix.tsv\",\n",
    "    \"phenotype_matrix_ml\": \"./output/phenotype_matrix.tsv\",\n",
    "    \"load_model\": None,\n",
    "    \"param_grids\": \"../tests/resources/param_grids.json\",\n",
    "    \"dt_depth\": 5,\n",
    "    \"rf_depth\": 10,\n",
    "    \"gb_depth\": 5,\n",
    "    \"num_trees\": 100,\n",
    "    \"max_iter\": 1000,\n",
    "    \"proportion_train\": 0.8,\n",
    "    \"rf_min_leaf\": 1,\n",
    "    \"rf_min_split\": 2,\n",
    "    \"gb_min_samples\": 2,\n",
    "    \"gb_learning_rate\": 0.1,\n",
    "    \"sampling_type\": \"under_sample\",\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ----------------------------\n",
    "### Data Formatting Workflow\n",
    "## ----------------------------"
   ],
   "id": "e5032aa4da69eda5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Starting data formatting workflow...\")\n",
    "\n",
    "# Create QueryExecutor instance\n",
    "qe = QueryExecutor(\n",
    "    config.get('phenotype_query_file'),\n",
    "    config.get('sapp_jar'),\n",
    "    debug=config[\"debug\"]\n",
    ")\n",
    "\n",
    "# Execute query and process results\n",
    "phenotype_file_path = os.path.join(config[\"output_dir\"], \"phenotype.tsv\")\n",
    "qe.execute_sapp_locally_file(\n",
    "    hdt_file=config.get('phenotype_hdt_file'),\n",
    "    output_file=phenotype_file_path\n",
    ")\n",
    "\n",
    "# Parse and filter results\n",
    "phenotype_output = QueryParser(file_path=phenotype_file_path)\n",
    "\n",
    "if config.get('rel_frequency'):\n",
    "    phenotype_output.filter_by_relative_frequency(config.get('rel_frequency'))\n",
    "\n",
    "if config.get('abs_frequency'):\n",
    "    phenotype_output.filter_by_absolute_frequency(config.get('abs_frequency'))\n",
    "\n",
    "phenotype_output.filter_by_species_frequency(config.get('species_frequency', 10))\n",
    "phenotype_output.convert_to_phenotype_matrix()\n",
    "\n",
    "# Save phenotype matrix\n",
    "output_path = phenotype_file_path.replace(\"phenotype.tsv\", \"phenotype_matrix.tsv\")\n",
    "phenotype_output.write_phenotype_matrix_to_file(output_path)\n",
    "print(f\"Phenotype matrix created at: {output_path}\")"
   ],
   "id": "10078c66c6803a3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ----------------------------\n",
    "### Annotation Workflow\n",
    "## ----------------------------"
   ],
   "id": "82216dee4c44fd21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "print(\"Starting genome annotation workflow...\")\n",
    "\n",
    "workflow = GenomeAnnotationWorkflow(\n",
    "    output_dir= config[\"output_dir\"],\n",
    "    threads= config.get('threads'),\n",
    "    debug=config[\"debug\"],\n",
    "    ncbi_dataset_bin=config.get('dataset_bin')\n",
    ")\n",
    "\n",
    "phenotype_df = pd.read_csv(os.path.join(config.get('output_dir'),\"phenotype_matrix.tsv\"), index_col=0, sep=\"\\t\")\n",
    "genome_identifiers = phenotype_df.index.to_list()\n",
    "\n",
    "print(list(genome_identifiers)[0])\n",
    "if config.get('dataset_bin') and os.path.exists(config.get('dataset_bin')):\n",
    "    print(\"Downloading genomes using ncbi-datasets-cli\")\n",
    "    fasta_genome_paths = workflow.download_genomes_from_genome_identifier(genome_identifiers[:100])\n",
    "else:\n",
    "    if len(genome_identifiers) > 100:\n",
    "        print(\"Warning: Downloading >100 genomes from NCBI may be slow\")\n",
    "    genome_identifiers = [\n",
    "        gid.split('.')[0] for gid in genome_identifiers\n",
    "    ]\n",
    "    fasta_genome_paths = [\n",
    "        f\"http://www.ebi.ac.uk/ena/browser/api/fasta/{gid}?download=true&gzip=true\"\n",
    "        for gid in genome_identifiers\n",
    "    ]\n",
    "\n",
    "\n",
    "print(fasta_genome_paths[:2])"
   ],
   "id": "704bc1cb5f063dbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if len(genome_identifiers):\n",
    "    genome_hdt_files = workflow.process_batch(fasta_genome_paths)\n",
    "    print(f\"Annotation complete. Results in: {config['output_dir']}\")"
   ],
   "id": "7e523024b989aca9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ----------------------------\n",
    "### Genome Querying Workflow\n",
    "## ----------------------------"
   ],
   "id": "2ad65fe054495bd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Starting genome querying workflow...\")\n",
    "\n",
    "# Run genome querying\n",
    "from migenpro.querying.query_executor import QueryExecutor\n",
    "\n",
    "# Create QueryExecutor instance\n",
    "qe = QueryExecutor(\n",
    "    config.get('genome_query_file'),\n",
    "    config.get('sapp_jar'),\n",
    "    debug=config[\"debug\"]\n",
    ")\n",
    "\n",
    "# Create genome directory\n",
    "genome_dir = os.path.join(config[\"output_dir\"], \"genomes\")\n",
    "os.makedirs(genome_dir, exist_ok=True)\n",
    "\n",
    "# Execute queries\n",
    "individual_genome_feature_paths = qe.execute_sapp_locally_directory(genome_dir)\n",
    "\n",
    "# Create feature matrix\n",
    "feature_matrix_path = os.path.join(config[\"output_dir\"], \"feature_matrix.tsv\")\n",
    "qe.summarise_feature_importance_files(individual_genome_feature_paths, feature_matrix_path)\n",
    "print(f\"Feature matrix created at: {feature_matrix_path}\")\n"
   ],
   "id": "33f324f435244cb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ----------------------------\n",
    "### Machine Learning Workflow\n",
    "## ----------------------------"
   ],
   "id": "c9109ed962cdd3a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Starting machine learning workflow...\")\n",
    "\n",
    "# Run ML workflow\n",
    "\n",
    "# Load matrices\n",
    "fm = FeatureMatrix(config.get('feature_matrix'))\n",
    "pm = PhenotypeMatrix(config.get('phenotype_matrix_ml'))\n",
    "fm.load_matrix()\n",
    "pm.load_matrix()\n",
    "\n",
    "# Get common genomes\n",
    "intersect_genomes = pm.get_intersected_genomes(fm.file_df)\n",
    "fm_subset = fm.create_subset(intersect_genomes)\n",
    "pm_subset = pm.create_subset(intersect_genomes)\n",
    "\n",
    "# Setup ML models\n",
    "ml_models = ParameterOptimisation(\n",
    "    dt_depth=config.get('dt_depth', 5),\n",
    "    rf_depth=config.get('rf_depth', 10),\n",
    "    gb_depth=config.get('gb_depth', 5),\n",
    "    num_trees=config.get('num_trees', 100),\n",
    "    max_iter=config.get('max_iter', 1000),\n",
    "    output=config.get('ml_output', \"./output/ml_results\"),\n",
    "    proportion_train=config.get('proportion_train', 0.8),\n",
    "    rf_min_leaf=config.get('rf_min_leaf', 1),\n",
    "    rf_min_split=config.get('rf_min_split', 2),\n",
    "    gb_min_samples=config.get('gb_min_samples', 2),\n",
    "    gb_learning_rate=config.get('gb_learning_rate', 0.1),\n",
    "    debug=config[\"debug\"]\n",
    ")\n",
    "\n",
    "# Configure datasets\n",
    "ml_models.set_datasets(\n",
    "    observed_values=fm_subset,\n",
    "    observed_results=pm_subset,\n",
    "    sampling_type=config.get('sampling_type', 'under_sample'),\n",
    "    threads=config.get('threads', 4)\n",
    ")\n",
    "\n",
    "# Parameter tuning\n",
    "if config.get('param_grids'):\n",
    "    with open(config.get('param_grids'), \"r\") as f:\n",
    "        param_grids = json.load(f)\n",
    "    optimized_params = ml_models.perform_halving_grid_search_search(param_grids=param_grids)\n",
    "    ml_models = MachineLearningModels(parameter_dictionary=optimized_params)\n",
    "\n",
    "# Training/prediction\n",
    "if config.get('train', True):\n",
    "    ml_models.train_models()\n",
    "    ml_models.save_models()\n",
    "elif config.get('load_model'):\n",
    "    ml_models.load_model(config.get('load_model'))\n",
    "\n",
    "if config.get('predict'):\n",
    "    ml_models.predict_models_test()\n",
    "    ml_models.predict_models_train()\n",
    "\n",
    "print(f\"ML results saved to: {ml_models.output}\")"
   ],
   "id": "5bd6df6b5566e78e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ----------------------------\n",
    "### Feature Importance Analysis\n",
    "## ----------------------------"
   ],
   "id": "88bccf8e8223c0ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Starting feature importance analysis...\")\n",
    "\n",
    "# Run feature importance\n",
    "from migenpro.post_analysis.ml_model_analysis import LoadedMachineLearningModel, ModelAnalysis\n",
    "\n",
    "# Load matrices\n",
    "fm = FeatureMatrix(config.get('feature_matrix'))\n",
    "pm = PhenotypeMatrix(config.get('phenotype_matrix_ml'))\n",
    "fm.load_matrix()\n",
    "pm.load_matrix()\n",
    "\n",
    "# Get common genomes\n",
    "intersect_genomes = pm.get_intersected_genomes(fm.file_df)\n",
    "fm_subset = fm.create_subset(intersect_genomes)\n",
    "pm_subset = pm.create_subset(intersect_genomes)\n",
    "\n",
    "# Analyze each model\n",
    "model_dir = config.get('models_dir', \"./output/ml_results/models\")\n",
    "model_files = [\n",
    "    os.path.join(model_dir, f)\n",
    "    for f in os.listdir(model_dir)\n",
    "    if f.endswith('.pkl')\n",
    "]\n",
    "\n",
    "for model_path in model_files:\n",
    "    model = LoadedMachineLearningModel(model_path)\n",
    "    print(f\"Analyzing model: {model.model_name}\")\n",
    "\n",
    "    analysis = ModelAnalysis(\n",
    "        \"target_phenotype\",\n",
    "        model,\n",
    "        fm_subset,\n",
    "        pm_subset\n",
    "    )\n",
    "\n",
    "    if model.gini:\n",
    "        analysis.plot_gini_feature_importance(\n",
    "            save_path=os.path.join(config[\"output_dir\"], f\"gini_{model.model_name}.png\")\n",
    "        )\n",
    "\n",
    "    if model.permutation:\n",
    "        analysis.permutation_feature_importance(\n",
    "            save_path=os.path.join(config[\"output_dir\"], f\"permutation_{model.model_name}.png\")\n",
    "        )\n",
    "\n",
    "print(\"Feature importance analysis complete\")"
   ],
   "id": "71fa5276363ec483",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ----------------------------\n",
    "### ML Results summary\n",
    "## ----------------------------"
   ],
   "id": "df3be1d2dfea2370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from migenpro.post_analysis.ml_summarise import MachineLearningData, SummaryGraphs\n",
    "\n",
    "machine_learning_output_data = MachineLearningData(config[\"output_dir\"])\n",
    "for scenario in [\"scenario\", \"train\"]:\n",
    "    summary_graphs = SummaryGraphs(machine_learning_output_data, config['output_dir'], debug=config['debug'])\n",
    "    summary_graphs.analyse_classifiers(scenario=scenario)\n",
    "    summary_graphs.make_method_summary_graphs()\n",
    "    summary_graphs.output_scores_to_table()\n",
    "\n",
    "print(f\"Summarised results are located in {config['output_dir']}.\")\n"
   ],
   "id": "2f856b60f77d4e94",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
