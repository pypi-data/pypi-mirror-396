{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct graphs machine learning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# File management\n",
    "import glob # Determine what files exist within certain directories.\n",
    "import json\n",
    "import os.path\n",
    "from os import path, sep, listdir, makedirs, getcwd\n",
    "import re\n",
    "# Graphs\n",
    "import pandas as pd # Standard format for the training and testing data.\n",
    "import matplotlib.pyplot as plt # Graph plotting.\n",
    "import seaborn as sns\n",
    "from Bio import Entrez\n",
    "from ete4 import NCBITaxa\n",
    "import math\n",
    "from collections import Counter\n",
    "import warnings # Ignore warnings, don't read too much into this. "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#                _                               _       _     _           \n",
    "#               (_)                             (_)     | |   | |          \n",
    "#   __ _ ___ ___ _  __ _ _ __   __   ____ _ _ __ _  __ _| |__ | | ___  ___ \n",
    "#  / _` / __/ __| |/ _` | '_ \\  \\ \\ / / _` | '__| |/ _` | '_ \\| |/ _ \\/ __|\n",
    "# | (_| \\__ \\__ \\ | (_| | | | |  \\ V / (_| | |  | | (_| | |_) | |  __/\\__ \\\n",
    "#  \\__,_|___/___/_|\\__, |_| |_|   \\_/ \\__,_|_|  |_|\\__,_|_.__/|_|\\___||___/\n",
    "#                   __/ |                                                  \n",
    "#                  |___/                                                   \n",
    "Entrez.email = \"\"\n",
    "ncbi = NCBITaxa()\n",
    "phenotypes = [\"gram\", \"motility\", \"spore\", \"oxygen\",\"temperature\"]\n",
    "metrics = ['Accuracy_data', 'AUC', 'F1', 'Precision', 'Recall', 'Matthews']\n",
    "bar_width = 0.15 \n",
    "classifiers = [\"DecisionTreeClassifier\", \"GradientBoostingClassifier\", \"RandomForestClassifier\"]\n",
    "dir_with_multiple_phenotype_outputs = path.abspath(\"../data/output_MI\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def rgb_to_hex(rgb_color):\n",
    "    \"\"\"\n",
    "    Convert RGB to hex.\n",
    "    \n",
    "    Parameters:\n",
    "    - rgb_color: tuple representing rgb values.\n",
    "    \n",
    "    Returns:\n",
    "    - String of color hex. \n",
    "    \"\"\"\n",
    "    return '#%02x%02x%02x' % rgb_color\n",
    "\n",
    "def interpolate_color(color_start_rgb, color_end_rgb, t):\n",
    "    \"\"\"\n",
    "    Interpolate between two RGB colors.\n",
    "    \n",
    "    Parameters:\n",
    "    - color_start_rgb: Tuple of integers representing the starting RGB color.\n",
    "    - color_end_rgb: Tuple of integers representing the ending RGB color.\n",
    "    - t: Float representing the interpolation factor between 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple representing the interpolated RGB color.\n",
    "    \"\"\"\n",
    "    return tuple(int(start_val + (end_val - start_val) * t) for start_val, end_val in zip(color_start_rgb, color_end_rgb))\n",
    "\n",
    "\n",
    "def create_gradient_image():\n",
    "    width = 800\n",
    "    height = 100\n",
    "\n",
    "    gradient = np.linspace(0, 1, width)\n",
    "    gradient_rgb = np.zeros((height, width, 3))\n",
    "    gradient_rgb[:, :, 0] = np.flip(gradient)\n",
    "    gradient_rgb[:, :, 1] = gradient\n",
    "\n",
    "    # Create the figure and display the gradient\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.imshow(gradient_rgb, aspect='auto')\n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.savefig('red_to_green_gradient.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "class TaxonomyFrom:\n",
    "    def __init__(self, phenotype_file_path: str):\n",
    "        phenotype_pd = pd.read_csv(phenotype_file_path, delimiter=\"\\t\")\n",
    "        phenotype_pd[\"?accession\"] = phenotype_pd[\"?accession\"].str.replace('.', '_')        \n",
    "        self.genome_to_taxa =  dict(zip(phenotype_pd['?accession'], phenotype_pd['?taxid']))\n",
    "\n",
    "    def genome(self, genome_accession: str):\n",
    "        return self.genome_to_taxa.get(genome_accession) if genome_accession in self.genome_to_taxa else None\n",
    "\n",
    "class TreeWalk:\n",
    "    def __init__(self, tree):\n",
    "        self.tree = tree\n",
    "        self.taxonomy = [\"domain\", \"kingdom\", \"phyum\", \"class\", \"order\", \"family\", \"genus\", \"subgenus\", \"specices group\", \"species\", \"subspecies\"]\n",
    "\n",
    "    def rank_limit(self, rank: str):\n",
    "        for node in self.tree.traverse(\"postorder\"):\n",
    "            if str(node.rank) == rank:\n",
    "                for child in node.children:\n",
    "                    print(child.rank)\n",
    "                    tree.prune(child.name)\n",
    "\n",
    "    def calculate_confidence_for_tree(self, merged_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Process the entire tree to update taxid_confidence based on the merged_df's 'ConfidencePrediction' column \n",
    "        and calculate confidence intervals.\n",
    "        \"\"\"\n",
    "        # Step 1: Initialize a dictionary to store confidence values for each taxid\n",
    "        taxid_confidence = {}\n",
    "\n",
    "        # Step 2: Iterate over the lineage column and aggregate confidence values\n",
    "        for index, row in merged_df.iterrows():\n",
    "            lineage = row.lineage\n",
    "            confidence = row.ConfidencePrediction\n",
    "\n",
    "            if pd.notna(lineage) and pd.notna(confidence):\n",
    "                taxids = lineage.split(\";\")\n",
    "                for taxid in taxids: # Go over all taxids\n",
    "                    taxid = int(taxid) # ncbi magic. \n",
    "                    if taxid not in taxid_confidence:\n",
    "                        taxid_confidence[str(taxid)] = [confidence]\n",
    "                    else:\n",
    "                        taxid_confidence[str(taxid)].append(confidence)\n",
    "\n",
    "        # Step 3: Calculate the average confidence for each taxid\n",
    "        for taxid, confidences in taxid_confidence.items():\n",
    "            taxid_confidence[str(taxid)] = sum(confidences) / len(confidences)\n",
    "\n",
    "        # Step 4: Check if all nodes are represented in the confidence dict. \n",
    "        for node in self.tree.traverse(\"postorder\"):\n",
    "            assert str(node.taxid) in taxid_confidence\n",
    "\n",
    "        return taxid_confidence\n",
    "\n",
    "        \n",
    "    def calculate_genomes_used(self, merged_df: pd.DataFrame):\n",
    "        # Initialize a Counter object to count occurrences of each taxid\n",
    "        taxid_to_abundance = Counter()\n",
    "\n",
    "        # Iterate over the lineage column\n",
    "        for lineage in merged_df['lineage']:\n",
    "            if pd.notna(lineage):\n",
    "                # Split the lineage string into taxids\n",
    "                taxids = lineage.split(\";\")\n",
    "                # Update the count for each taxid in the lineage\n",
    "                taxid_to_abundance.update(taxids)\n",
    "\n",
    "        # Having integers as keys is a disaster in Python so here is my magic. \n",
    "        taxid_to_abundance_str_keys = {str(key): value for key, value in taxid_to_abundance.items()}\n",
    "\n",
    "        # Check \n",
    "        for node in self.tree.traverse(\"postorder\"):\n",
    "            assert str(node.taxid) in taxid_to_abundance_str_keys\n",
    "\n",
    "        return taxid_to_abundance_str_keys\n",
    "\n",
    "    @property\n",
    "    def get_taxid_confidence(self):\n",
    "        return self.taxid_confidence\n",
    "\n",
    "    @property\n",
    "    def get_tree(self):\n",
    "        return self.tree\n",
    "\n",
    "# Function to get lineage as semicolon-separated string of names\n",
    "def get_lineage_string(taxid):\n",
    "    # Get the lineage taxids for the given taxid\n",
    "    lineage_taxids = [str(taxid) for taxid in ncbi.get_lineage(taxid)]\n",
    "    return \";\".join(lineage_taxids)\n",
    "\n",
    "\n",
    "def make_sectional_table(df: pd.DataFrame, merge_column_1: str, merge_column_2: str):\n",
    "    \"\"\"\n",
    "    Sorts a DataFrame by two specified columns and modifies it in place.\n",
    "    Converts the DataFrame into a sectional table format where unique rows from\n",
    "    the first column span multiple values in the second column.\n",
    "\n",
    "    Example Input:\n",
    "    # Col1  | Col2  | Col3\n",
    "    # Gram  | DT    | 0.8\n",
    "    # Gram  | RF    | 0.8\n",
    "    # Motility | RF | 0.9\n",
    "\n",
    "    Example Output:\n",
    "          Col2    | Col3\n",
    "    Gram\n",
    "           DT      0.8\n",
    "           RF      0.8\n",
    "    Motility\n",
    "           RF     0.9\n",
    "\n",
    "    Parameters:\n",
    "    df : pd.DataFrame\n",
    "        The pandas DataFrame to be sorted. This input DataFrame is modified in place.\n",
    "\n",
    "    merge_column_1 : str\n",
    "        The name of the primary column to sort by.\n",
    "\n",
    "    merge_column_2 : str\n",
    "        The name of the secondary column to add hierarchical distinctions.\n",
    "    \"\"\"\n",
    "    df.sort_values(by=[merge_column_1, merge_column_2], inplace=True)\n",
    "    # Ensure merge col 1 and -2 are nex to each other in the first and second place.\n",
    "    cols  = df.columns.to_list()\n",
    "    cols.remove(merge_column_1)\n",
    "    cols.remove(merge_column_2)\n",
    "\n",
    "    # Create a new list of columns with merge_column_1 and merge_column_2 at the beginning\n",
    "    cols_new = [merge_column_1, merge_column_2] + cols\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    df = df[cols_new]\n",
    "\n",
    "    column_1_blocks = {}\n",
    "    for n_row, row in enumerate(df.iterrows()):\n",
    "        col_1_value = row[1][merge_column_1]\n",
    "        if col_1_value not in column_1_blocks:\n",
    "            column_1_blocks[col_1_value] = n_row\n",
    "\n",
    "    new_row = [\" \"] * (df.columns.size - 1)\n",
    "    new_list = []\n",
    "\n",
    "    for n_row, row in enumerate(df.iterrows()):\n",
    "        row = row[1]\n",
    "        if n_row == column_1_blocks.get(row[merge_column_1]):\n",
    "            new_row[0] = \"\\\\rowcolor[HTML]{EFEFEF} \\n \" + row[merge_column_1]\n",
    "            new_list.append( new_row.copy())\n",
    "\n",
    "        row_list_new = row.tolist()[1:]\n",
    "        row_list_new[0] = \"\\hspace{1em} \" + str(row_list_new[0])\n",
    "        new_list.append(row_list_new)\n",
    "\n",
    "    new_df = pd.DataFrame(new_list)\n",
    "    new_df.columns = df.columns[1:]\n",
    "\n",
    "    latex_table = new_df.reset_index(drop=True).set_index(new_df.columns.to_list()[0]).to_latex()\n",
    "    latex_table = re.sub(r\"^(([0-9]| )*?&)\", \"\", latex_table).replace(\"l\"*len(new_df.columns), \"l\"*(len(new_df.columns) -1) ) # remove first row\n",
    "    latex_table = latex_table.replace(\"nan\", \"-\")\n",
    "    return latex_table"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cwd = getcwd()\n",
    "cwd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dfs = []\n",
    "for phenotype in phenotypes:\n",
    "    for classifier in classifiers:\n",
    "        # Generate the list of scenario output file paths for different iterations\n",
    "        test_outputs = [f\"{dir_with_multiple_phenotype_outputs}/{phenotype}/iteration_{iteration}/protein_domains/{classifier}/{classifier}-test.tsv\" for iteration in range(1, 6)]\n",
    "        \n",
    "        for iteration, file_path in enumerate(test_outputs):\n",
    "            # Read the TSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path, sep='\\t').loc[:,[\"Genomes\", \"Observation\", \"ObservedString\", \"Prediction\", \"PredictedString\", \"ConfidencePrediction\"]]\n",
    "            \n",
    "            # Add a column for the iteration number\n",
    "            df['iteration'] = iteration + 1\n",
    "            \n",
    "            # Add columns for phenotype and classifier to differentiate in the final DataFrame\n",
    "            df['phenotype'] = phenotype\n",
    "            df['classifier'] = classifier\n",
    "            \n",
    "            # Append the DataFrame to the list\n",
    "            dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print or work with the combined DataFrame\n",
    "combined_df# .to_csv(\"all_outputs.tsv\", sep=\"\\t\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Get the used genomes and their respective phenotypes."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from migenpro.ml.machine_learning_main import *\n",
    "\n",
    "for phenotype in phenotypes:\n",
    "    feature_matrix_path = f\"{dir_with_multiple_phenotype_outputs}/{phenotype}/iteration_1/observed_results_before_train_test_split.tsv\"\n",
    "    phenotype_matrix_path = f\"{dir_with_multiple_phenotype_outputs}/{phenotype}/phenotype_matrix.tsv\"\n",
    "    phenotype_path = f\"{dir_with_multiple_phenotype_outputs}/{phenotype}/phenotype.tsv\"\n",
    "\n",
    "    feature_matrix = FeatureMatrix(feature_matrix_path)\n",
    "    feature_matrix.load_matrix()\n",
    "    phenotype_matrix = PhenotypeMatrix(phenotype_matrix_path)\n",
    "    phenotype_matrix.load_matrix()\n",
    "    intersect_genomes_phenotype = phenotype_matrix.get_intersected_genomes(feature_matrix.file_df)\n",
    "    intersect_genomes_feature = feature_matrix.get_intersected_genomes(phenotype_matrix.file_df)\n",
    "    feature_matrix_subset = feature_matrix.create_subset(intersect_genomes_feature)\n",
    "    phenotype_matrix_subset = phenotype_matrix.create_subset(intersect_genomes_phenotype)\n",
    "\n",
    "    # Automatically remove duplicate indexes, keeping the first instance\n",
    "    phenotype_matrix_subset = phenotype_matrix_subset[~phenotype_matrix_subset.index.duplicated(keep='first')]\n",
    "\n",
    "    genomes = phenotype_matrix_subset.index.to_list()\n",
    "    print(len(genomes))\n",
    "\n",
    "    phenotype_frequencies = phenotype_matrix_subset.value_counts()\n",
    "    print(phenotype_frequencies)\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = []\n",
    "for phenotype in phenotypes:\n",
    "    # Generate the list of scenario output file paths for different iterations\n",
    "    test_outputs = [f\"{dir_with_multiple_phenotype_outputs}/{phenotype}/iteration_{iteration}/graphs/test-summary.tsv\" for iteration in range(1, 6)]\n",
    "\n",
    "    for iteration, file_path in enumerate(test_outputs):\n",
    "\n",
    "        # Load the file and extract metrics\n",
    "        with open(file_path, \"r\") as f:\n",
    "            # Assuming the file content is in the format you provided\n",
    "            # metrics = list(map(float, content.split(\",\")[1:7]))  # Adjust indices as needed\n",
    "            for line in f:\n",
    "                line = f.readline().split(\",\")\n",
    "                classifier = line[0].split(\"/\")[-2]\n",
    "                # Extract the metrics\n",
    "                f1_score = line[1]\n",
    "                accuracy = line[2]\n",
    "                precision = line[3]\n",
    "                recall = line[4]\n",
    "                mcc = line[5]\n",
    "                auc = line[6]\n",
    "\n",
    "                # Append to results\n",
    "                results.append({\n",
    "                    \"Phenotype\": phenotype,\n",
    "                    \"iteration:\": iteration,\n",
    "                    \"Classifier\": classifier,\n",
    "                    \"F1 Score\": f1_score,\n",
    "                    \"Accuracy\": accuracy,\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall,\n",
    "                    \"MCC\": mcc,\n",
    "                    \"AUC\": auc,\n",
    "                })\n",
    "\n",
    "\n",
    "metrics_df_all = pd.DataFrame(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Define the columns you want to aggregate\n",
    "metric_cols = [\"F1 Score\", \"Accuracy\", \"Precision\", \"Recall\", \"MCC\", \"AUC\"]\n",
    "\n",
    "# 2. Force convert these columns to numeric types\n",
    "# errors='coerce' turns un-convertible text into NaN, preventing crashes\n",
    "for col in metric_cols:\n",
    "    metrics_df_all[col] = pd.to_numeric(metrics_df_all[col], errors='coerce')\n",
    "\n",
    "# 3. Now run your original groupby code\n",
    "summary_df = metrics_df_all.groupby([\"Phenotype\", \"Classifier\"]).agg({\n",
    "    \"F1 Score\": [\"mean\", \"std\"],\n",
    "    \"Accuracy\": [\"mean\", \"std\"],\n",
    "    \"Precision\": [\"mean\", \"std\"],\n",
    "    \"Recall\": [\"mean\", \"std\"],\n",
    "    \"MCC\": [\"mean\", \"std\"],\n",
    "    \"AUC\": [\"mean\", \"std\"],\n",
    "}).reset_index()\n",
    "\n",
    "# 4. Flatten the MultiIndex columns\n",
    "summary_df.columns = [\" \".join(col).strip() if col[1] else col[0] for col in summary_df.columns.values]\n",
    "\n",
    "# Optional: Inspect the result\n",
    "summary_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "formatted_df = summary_df[[\"Phenotype\", \"Classifier\"]].copy()\n",
    "\n",
    "for metric in metric_cols:\n",
    "    mean_col = f\"{metric} mean\"\n",
    "    std_col = f\"{metric} std\"\n",
    "\n",
    "    # Check if the columns exist in the summary dataframe\n",
    "    if mean_col in summary_df.columns and std_col in summary_df.columns:\n",
    "        formatted_df[metric] = summary_df.apply(\n",
    "            lambda x: f\"{x[mean_col]:.2f}Â±{x[std_col]:.2f}\", axis=1\n",
    "        )\n",
    "\n",
    "formatted_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(make_sectional_table(formatted_df, \"Phenotype\", \"Classifier\"))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dir_with_multiple_phenotype_outputs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    " ## Get all feature importance metrics per phenotype\n",
    "for phenotype in phenotypes:\n",
    "    phenotype_feature_importance_df = pd.DataFrame()\n",
    "    for classifier in classifiers:\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "         # Per iteration\n",
    "        test_outputs = [f\"{dir_with_multiple_phenotype_outputs}/{phenotype}/iteration_{iteration}/protein_domains/{classifier}/gini_feature_importance_summary_{classifier}.tsv\" for iteration in range(1, 6)]\n",
    "        feature_importances_graph_ready = pd.DataFrame()\n",
    "        for iteration, file_path in enumerate(test_outputs):\n",
    "            iteration = iteration + 1 # to max actual iteration number used.\n",
    "            feature_importance_df = pd.read_csv(file_path, delimiter=\"\\t\")\n",
    "            importance_column_name=  f\"importance_{classifier}_{iteration}\"\n",
    "            feature_importance_df.rename(columns={'importance': importance_column_name}, inplace=True)\n",
    "\n",
    "            # Cleanup\n",
    "            phenotype_feature_importance_df = (pd.concat([phenotype_feature_importance_df, feature_importance_df], ignore_index=True, sort=False))\n",
    "\n",
    "            # For heatmap\n",
    "            phenotype_specific_feature_importance_this_iteration = phenotype_feature_importance_df.loc[pd.notna(phenotype_feature_importance_df[f\"importance_{classifier}_{iteration}\"])]\n",
    "            filtered_df = phenotype_specific_feature_importance_this_iteration[phenotype_specific_feature_importance_this_iteration[importance_column_name] >= 0.01]\n",
    "            phenotype_specific_feature_importance_this_iteration.set_index(\"feature_name\", inplace=True)\n",
    "            # phenotype_specific_feature_importance_this_iteration\n",
    "            feature_importances_graph_ready = pd.concat([feature_importances_graph_ready, phenotype_specific_feature_importance_this_iteration[f\"importance_{classifier}_{iteration}\"]], axis = 1, join=\"outer\")\n",
    "\n",
    "        # heatmap = sns.heatmap(feature_importances_graph_ready, annot=True)\n",
    "        # plt.title(f\"Gini feature importance {classifier} for {phenotype}\")\n",
    "        # plt.xticks(ticks=range(len(feature_importances_graph_ready.columns)),\n",
    "        #    labels=[f'Iteration {i}' for i in range(1, 6)],\n",
    "        #    rotation=45)\n",
    "        # plt.savefig(f\"{phenotype}_{classifier}\", dpi=300, bbox_inches='tight')\n",
    "        # plt.close()\n",
    "\n",
    "phenotype_feature_importance_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get all feature importance metrics per phenotype\n",
    "# for phenotype in phenotypes:\n",
    "phenotype = \"motility\"\n",
    "print(f\"\\n=== Processing {phenotype} ===\")\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print(f\"\\nClassifier: {classifier}\")\n",
    "\n",
    "    # Store all iterations data for this classifier\n",
    "    all_iterations_data = pd.DataFrame()\n",
    "\n",
    "    # Per iteration\n",
    "    test_outputs = [f\"{dir_with_multiple_phenotype_outputs}/{phenotype}/iteration_{iteration}/protein_domains/{classifier}/gini_feature_importance_summary_{classifier}.tsv\" for iteration in range(1, 6)]\n",
    "\n",
    "    for iteration, file_path in enumerate(test_outputs):\n",
    "        iteration_num = iteration + 1  # to match actual iteration number used\n",
    "\n",
    "        try:\n",
    "            feature_importance_df = pd.read_csv(file_path, delimiter=\"\\t\")\n",
    "\n",
    "            # Sort by importance (descending) and add rank\n",
    "            feature_importance_df = feature_importance_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "            feature_importance_df[f'rank_iter_{iteration_num}'] = range(1, len(feature_importance_df) + 1)\n",
    "\n",
    "            # Select only domain name and rank for this iteration\n",
    "            iteration_ranks = feature_importance_df[['feature_name', f'rank_iter_{iteration_num}']]\n",
    "\n",
    "            # Join with accumulated data\n",
    "            if all_iterations_data.empty:\n",
    "                all_iterations_data = iteration_ranks\n",
    "            else:\n",
    "                all_iterations_data = all_iterations_data.merge(iteration_ranks, on='feature_name', how='outer')\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate rank sum\n",
    "    if not all_iterations_data.empty:\n",
    "        rank_cols = [col for col in all_iterations_data.columns if col.startswith('rank_iter_')]\n",
    "        all_iterations_data[rank_cols] = all_iterations_data[rank_cols].fillna(1000)\n",
    "\n",
    "        # Calculate rank sum\n",
    "        all_iterations_data['rank_sum'] = all_iterations_data[rank_cols].sum(axis=1)\n",
    "\n",
    "        # Sort by rank sum (lower is better)\n",
    "        result_df = all_iterations_data[['feature_name', 'rank_sum']].sort_values('rank_sum')\n",
    "\n",
    "        # Create the simple table format you want\n",
    "        print(f\"\\n| Domain | Rank Sum |\")\n",
    "        print(f\"|--------|----------|\")\n",
    "        for _, row in result_df.head(20).iterrows():  # Show top 20\n",
    "            print(f\"| {row['feature_name']} | {int(row['rank_sum'])} |\")\n",
    "\n",
    "        # Also save to file\n",
    "        result_df.to_csv(f\"{phenotype}_{classifier}_rank_sums.tsv\", sep='\\t', index=False)\n",
    "\n",
    "print(\"Analysis completed!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# phenotype = phenotypes[1]\n",
    "# print(phenotype)\n",
    "# phenotype_feature_importance_df = pd.DataFrame()\n",
    "# for classifier in classifiers:\n",
    "#     feature_importance_df = pd.DataFrame()\n",
    "#      # Per iteration\n",
    "#     test_outputs = [f\"{dir_with_multiple_phenotype_outputs}/{phenotype}_output/mloutput/iteration_{iteration}/gini_feature_importance_summary__iteration_{iteration}{classifier}_{phenotype}.tsv\" for iteration in range(1, 6)]\n",
    "#     for iteration, file_path in enumerate(test_outputs):\n",
    "#         iteration = iteration + 1 # to max actual iteration number used.\n",
    "#         print(iteration)\n",
    "#         feature_importance_df = pd.read_csv(file_path, delimiter=\"\\t\")\n",
    "#         feature_importance_df.rename(columns={'importance': f\"importance_{classifier}_{iteration}\"}, inplace=True)\n",
    "#         phenotype_feature_importance_df = (pd.concat([phenotype_feature_importance_df, feature_importance_df], ignore_index=True, sort=False))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "# classifiers = [\"DecisionTreeClassifier\", \"GradientBoostingClassifier\", \"RandomForestClassifier\"]\n",
    "# metrics = ['Accuracy_data', 'AUC', 'F1', 'Precision', 'Recall', 'Matthews']\n",
    "#\n",
    "# for phenotype in phenotypes:\n",
    "#\n",
    "#     bar_width = 0.15  # Width of the bars\n",
    "#\n",
    "#     taxonomy_from = TaxonomyFrom(os.path.join(dir_with_multiple_phenotype_outputs, f\"{phenotype}\", \"phenotype.tsv\"))\n",
    "#     for classifier in classifiers:\n",
    "#         #######################\n",
    "#         ## Phylogenetic tree ##\n",
    "#         #######################\n",
    "#\n",
    "#         # Get the list of taxids and create the taxid set with the entire lineage\n",
    "#         # result = pd.concat([prediction_and_probability_df, genome_tax_df], axis=1)\n",
    "#         summary_file =os.path.join(dir_with_multiple_phenotype_outputs, f\"{phenotype}\", \"iteration_1\", \"protein_domains\", classifier, f\"{classifier}-scenario.tsv\")\n",
    "#         if not path.isfile(summary_file):\n",
    "#             raise FileNotFoundError(summary_file)\n",
    "#\n",
    "#         prediction_and_probability_df = pd.read_csv(summary_file, delimiter=\"\\t\")\n",
    "#         prediction_and_probability_df.drop(\"Unnamed: 0\", inplace=True, axis=1)\n",
    "#         genomes = prediction_and_probability_df[\"Genomes\"].to_list()\n",
    "#         prediction_and_probability_df.set_index(\"Genomes\", inplace=True)\n",
    "#         genome_tax = {}\n",
    "#\n",
    "#         for genome in genomes:\n",
    "#             genome_tax[genome] = taxonomy_from.genome(genome)\n",
    "#\n",
    "#         genome_tax_df = pd.DataFrame.from_dict(genome_tax, orient='index', columns=[\"taxid\"])\n",
    "#         # result = pd.concat([prediction_and_probability_df, genome_tax_df], axis=1)\n",
    "#\n",
    "#         merged_df = prediction_and_probability_df.join(genome_tax_df)\n",
    "#         total_counts = len(merged_df.index)\n",
    "#\n",
    "#         merged_df['lineage'] = merged_df.taxid.apply(get_lineage_string)\n",
    "#         merged_df.to_csv(\"merged.csv\")\n",
    "#\n",
    "#         taxids = merged_df.taxid.to_list()\n",
    "#         tree = ncbi.get_topology(taxids, rank_limit=\"order\")\n",
    "#         taxid_set = set()\n",
    "#         for taxid in taxids:\n",
    "#             lineage = ncbi.get_lineage(taxid)\n",
    "#             for lineage_taxid in lineage:\n",
    "#                 taxid_set.add(lineage_taxid)\n",
    "#\n",
    "#         # Retrieve the taxonomic names for all relevant taxids\n",
    "#         taxid_to_name = ncbi.get_taxid_translator(taxids=taxid_set)\n",
    "#\n",
    "#         ncbi.annotate_tree(tree, tax2name=taxid_to_name)\n",
    "#\n",
    "#         tw_pruned = TreeWalk(tree)\n",
    "#         taxid_confidence = tw_pruned.calculate_confidence_for_tree(merged_df)\n",
    "#         counts_per_taxid = tw_pruned.calculate_genomes_used(merged_df)\n",
    "#\n",
    "#         counts_df = pd.DataFrame(list(counts_per_taxid.items()), columns=[\"taxid\", \"genome_count\"])\n",
    "#         counts_df.to_csv(\"counts.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a list to store hyperparameters\n",
    "hyperparameters = []\n",
    "\n",
    "# Loop over phenotypes\n",
    "for phenotype in phenotypes:\n",
    "    pattern = os.path.join(dir_with_multiple_phenotype_outputs, phenotype, \"iteration_*\", \"optimised_params.json\")\n",
    "    phenotype_optimal_params_json = glob.glob(pattern)\n",
    "    if not (phenotype_optimal_params_json):\n",
    "        raise FileNotFoundError(f\"No JSON files found for phenotype {phenotype}\")\n",
    "\n",
    "    for param_json_path in phenotype_optimal_params_json:\n",
    "        with open(param_json_path, 'r') as f:\n",
    "            params = json.load(f)\n",
    "\n",
    "        # Extract hyperparameters\n",
    "        dt_params = params.get(\"DecisionTreeClassifier\", {})\n",
    "        rt_params = params.get(\"RandomForestClassifier\", {})\n",
    "        gb_params = params.get(\"GradientBoostingClassifier\", {})\n",
    "\n",
    "        iteration = os.path.basename(os.path.dirname(param_json_path))\n",
    "\n",
    "        hyperparameters.append({\n",
    "            \"Phenotype\": phenotype,\n",
    "            \"Iteration\": iteration,\n",
    "            \"Model\": \"DT\",\n",
    "            \"max_depth\": dt_params.get(\"max_depth\"),\n",
    "        })\n",
    "        hyperparameters.append({\n",
    "            \"Phenotype\": phenotype,\n",
    "            \"Iteration\": iteration,\n",
    "            \"Model\": \"RF\",\n",
    "            \"max_depth\": rt_params.get(\"max_depth\"),\n",
    "            \"n_estimators\": rt_params.get(\"n_estimators\"),\n",
    "            \"min_samples_leaf\": rt_params.get(\"min_samples_leaf\"),\n",
    "        })\n",
    "        hyperparameters.append({\n",
    "            \"Phenotype\": phenotype,\n",
    "            \"Iteration\": iteration,\n",
    "            \"Model\": \"GB\",\n",
    "            \"max_depth\": gb_params.get(\"max_depth\"),\n",
    "            \"n_estimators\": gb_params.get(\"n_estimators\"),\n",
    "            \"learning_rate\": gb_params.get(\"learning_rate\"),\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(hyperparameters)\n",
    "df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Summary statistics for each hyperparameter\n",
    "summary_stats = df.groupby([\"Phenotype\", \"Model\"]).agg({\n",
    "    \"max_depth\": [\"mean\", \"median\", \"min\", \"max\", \"std\"],\n",
    "    \"n_estimators\": [\"mean\", \"median\", \"min\", \"max\", \"std\"],\n",
    "    \"min_samples_leaf\": [\"mean\", \"median\", \"min\", \"max\", \"std\"],\n",
    "    \"learning_rate\": [\"mean\", \"median\", \"min\", \"max\", \"std\"]\n",
    "}).round(2)\n",
    "\n",
    "# Display summary statistics\n",
    "print(summary_stats)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x=\"Phenotype\",\n",
    "    y=\"max_depth\",\n",
    "    hue=\"Model\",\n",
    "    palette=\"Set2\",\n",
    "    width=0.6,\n",
    ")\n",
    "plt.title(\"Distribution of Optimal max_depth per Phenotype\")\n",
    "plt.xlabel(\"Phenotype\")\n",
    "plt.ylabel(\"max_depth\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Model\", loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"max_depth_per_phenotype.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(\n",
    "    data=df[df[\"Model\"].isin([\"RT\", \"GB\"])],\n",
    "    x=\"Phenotype\",\n",
    "    y=\"n_estimators\",\n",
    "    hue=\"Model\",\n",
    "    palette=\"Set2\",\n",
    "    width=0.6,\n",
    ")\n",
    "plt.title(\"Distribution of Optimal n_estimators per Phenotype\")\n",
    "plt.xlabel(\"Phenotype\")\n",
    "plt.ylabel(\"n_estimators\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Model\", loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"n_estimators_per_phenotype.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(\n",
    "    data=df[df[\"Model\"] == \"GB\"],\n",
    "    x=\"Phenotype\",\n",
    "    y=\"learning_rate\",\n",
    "    color=\"lightblue\",\n",
    "    width=0.6,\n",
    ")\n",
    "plt.title(\"Distribution of Optimal learning_rate per Phenotype (GB)\")\n",
    "plt.xlabel(\"Phenotype\")\n",
    "plt.ylabel(\"learning_rate\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"learning_rate_per_phenotype.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(\n",
    "    data=df[df[\"Model\"] == \"RT\"],\n",
    "    x=\"Phenotype\",\n",
    "    y=\"min_samples_leaf\",\n",
    "    color=\"lightgreen\",\n",
    "    width=0.6,\n",
    ")\n",
    "plt.title(\"Distribution of Optimal min_samples_leaf per Phenotype (RT)\")\n",
    "plt.xlabel(\"Phenotype\")\n",
    "plt.ylabel(\"min_samples_leaf\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"min_samples_leaf_per_phenotype.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
