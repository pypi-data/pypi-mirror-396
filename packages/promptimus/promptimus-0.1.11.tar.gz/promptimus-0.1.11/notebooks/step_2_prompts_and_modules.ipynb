{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a289d5c2-0cf1-447f-b58d-3cc8021c1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6a09b-9232-4afc-bf5b-ef11fab5c2ca",
   "metadata": {},
   "source": [
    "# Prompts & modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cc2aad-f521-49dd-89a6-8b8b40659b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import promptimus as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da56c8f9-aecf-4537-9cc4-d3222950aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a provider\n",
    "llm = pm.llms.OpenAILike(\n",
    "    model_name=\"gemma3:4b\", base_url=\"http://lilan:11434/v1\", api_key=\"DUMMY\"\n",
    ")\n",
    "embedder = pm.embedders.OpenAILikeEmbedder(\n",
    "    model_name=\"mxbai-embed-large\", base_url=\"http://lilan:11434/v1\", api_key=\"DUMMY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccca2e0-e99a-455c-91a2-9029087ac817",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "A `Prompt` encapsulates the system prompt and `Provider`, allowing to call LLM with pre-defined behavior, constraints, and response style. \n",
    "Core Functionality\n",
    "\n",
    "-  Encapsulates the system prompt, enforcing predefined behavior.\n",
    "-  Requires an LLM provider to execute and generate responses.\n",
    "-  Processes message sequences asynchronously.\n",
    "-  Preferred to be embedded in a Module for persistence and configuration.\n",
    "\n",
    "A Prompt is the primary mechanism for conditioning model output, by desing it's similar to a pytorch Parameter - https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f12419-173b-412b-a3f8-b7fb5395372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a prompt\n",
    "prompt = pm.Prompt(\"You are an AI assitant with name Henry\").with_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be3f6d8e-9a3a-4717-808d-be387750ac18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mrole\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMessageRole.ASSISTANT:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'assistant'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'My name is Henry. Itâ€™s nice to meet you! ðŸ˜Š \\n\\nHow can I help you today?'\u001b[0m,\n",
       "    \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mtool_call_id\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await prompt.forward(\n",
    "    [\n",
    "        pm.Message(\n",
    "            role=pm.MessageRole.USER,\n",
    "            content=\"What is your name?\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366a1e4-873b-44e9-b896-47507b7b5e15",
   "metadata": {},
   "source": [
    "## **Modules**  \n",
    "\n",
    "A `Module` serves as a container for integrating multiple components, including `Prompts`, other `Modules`, and **state management** or **additional logic**. It encapsulates logic for handling inputs and outputs, organizing them into reusable and configurable components, for more complex workflows.\n",
    "\n",
    "Within a `Module`, submodules and prompts can be defined, and each submodule is configured with the same `LLMProvider` as the parent module, ensuring consistency across the module's components.\n",
    "\n",
    "Modules also support serialization, to store and load the content of a `Prompt`. The idea is to separate `code` logic from `text` prompts.  \n",
    "\n",
    "A `Module` mimics the design of PyTorch's `nn.Module` ([PyTorch nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)), serving as an abstraction for defining, organizing, and managing components. Like `nn.Module`, it provides a convenient interface for model components, ensuring modularity, reusability, and extensibility, as well as supporting the management of submodules and serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa87a8f-6343-4ad4-8fa2-54f113766ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple module with memory\n",
    "\n",
    "\n",
    "class AssistantWithMemory(pm.Module):\n",
    "    \"\"\"Simple module with memory\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # call init just like in pytorch\n",
    "        super().__init__()\n",
    "\n",
    "        self.chat = pm.Prompt(\"Act as an assistant\")\n",
    "        self.memory = []\n",
    "\n",
    "    async def forward(self, question: str) -> str:\n",
    "        \"\"\"Implement the async forward function with custom logic.\"\"\"\n",
    "        self.memory.append(pm.Message(role=pm.MessageRole.USER, content=question))\n",
    "        response = await self.chat.forward(self.memory)\n",
    "        self.memory.append(response)\n",
    "        return response.content\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d40afc-764a-48dd-a786-2e98f377ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object and set provider to all prompts\n",
    "assistant = AssistantWithMemory().with_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c664021-1778-413e-9194-54091acc9664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Hello Ailadin! Itâ€™s lovely to meet you. What can I do for you today? ðŸ˜Š \\n\\nDo you want to:\\n\\n*   Tell me about yourself?\\n*   Ask me a question?\\n*   Play a game?\\n*   Just chat?'\u001b[0m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# talk to your assistant\n",
    "await assistant.forward(\"Hi my name is ailadin!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e8a22c6-9eec-43ca-8c89-5ad2216e31d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Your name is Ailadin! ðŸ˜Š I just confirmed it with you. \\n\\nIs there anything else youâ€™d like to tell me about yourself, or would you like to ask me something?'\u001b[0m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await assistant.forward(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62fdfc69-a795-45c1-8622-f26d46ac83c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[chat]\n",
      "prompt = \"\"\"\n",
      "Act as an assistant\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "system\n",
      "\"\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can store and load prompts\n",
    "print(assistant.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb231e8a-f766-4539-9d16-dc13f6410eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[chat]\n",
      "prompt = \"\"\"\n",
      "Act as an pirate assistant\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "system\n",
      "\"\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant = assistant.load_dict(\n",
    "    {\n",
    "        \"params\": {},\n",
    "        \"submodules\": {\n",
    "            \"chat\": {\n",
    "                \"params\": {\"prompt\": \"Act as an pirate assistant\", \"role\": \"system\"},\n",
    "                \"submodules\": {},\n",
    "            }\n",
    "        },\n",
    "    }\n",
    ")\n",
    "print(assistant.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "142b68a2-7779-454a-980b-ebe34e01de80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Shiver me timbers, Ailadin! That's a fine question! \\n\\nNo, it isnâ€™t quite right to say â€œships swim.â€ Ships donâ€™t *swim* like fish. They float on the water, thanks to their construction and the way they displace the water. \\n\\nWe say they â€œsailâ€ or â€œdriftâ€ on the water. \\n\\nA clever question, that! You've got a good eye for details, Ailadin.  Do you want to tell me why you asked?\"\u001b[0m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await assistant.forward(\"Is it correct to say thay ships swim?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f14e9e57-12ee-4780-8b3c-8a806d016f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a module with a submodule\n",
    "\n",
    "\n",
    "class PrimitiveRagRetriever(pm.Module):\n",
    "    def __init__(self, top_k: int = 3, similarity_thr: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.top_k = pm.Parameter(top_k)\n",
    "        self.similarity_thr = pm.Parameter(similarity_thr)\n",
    "\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "\n",
    "    async def set_documents(self, documents: list[str]):\n",
    "        self.documents = documents\n",
    "        self.embeddings = await self.embedder.aembed_batch(documents)\n",
    "\n",
    "    async def forward(self, query: str) -> list[str]:\n",
    "        q_embedding = await self.embedder.aembed(query)\n",
    "        print(\"Query:\", query)\n",
    "        similarities = [\n",
    "            pm.embedders.ops.cosine(e, q_embedding) for e in self.embeddings\n",
    "        ]\n",
    "        print(\"Similarities:\", similarities)\n",
    "        argsorted = sorted(range(len(similarities)), key=lambda x: -similarities[x])\n",
    "\n",
    "        return [\n",
    "            self.documents[idx]\n",
    "            for idx in argsorted[: self.top_k.value]\n",
    "            if similarities[idx] > self.similarity_thr.value\n",
    "        ]\n",
    "\n",
    "\n",
    "class PrimitiveRagQA(pm.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        top_k: int = 3,\n",
    "        similarity_thr: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.retriver = PrimitiveRagRetriever(top_k, similarity_thr)\n",
    "        self.query_generator = pm.Prompt(\n",
    "            \"Generate a qery for RAG based on this question: `{question}`. Return only query without additional explanations.\",\n",
    "            role=pm.MessageRole.USER,\n",
    "        )\n",
    "        self.responder = pm.Prompt(\n",
    "            \"Act as an assistant, you have access to a RAG, information from it are prefixed with `Observation:` and not visible to user.\"\n",
    "        )\n",
    "\n",
    "    async def forward(self, question: str) -> str:\n",
    "        \"\"\"Implement the async forward function with custom logic.\"\"\"\n",
    "        query = await self.query_generator.forward(question=question)\n",
    "        context = await self.retriver.forward(query.content)\n",
    "\n",
    "        print(\"Context:\", context)\n",
    "\n",
    "        response = await self.responder.forward(\n",
    "            [\n",
    "                pm.Message(\n",
    "                    role=pm.MessageRole.USER,\n",
    "                    content=question,\n",
    "                ),\n",
    "                pm.Message(\n",
    "                    role=pm.MessageRole.USER,\n",
    "                    content=\"Obervation:\\n\" + \"\\n\".join(context),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1d810e3-3247-4c09-bfce-320ed6214838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[retriver]\n",
      "top_k = 1\n",
      "similarity_thr = 0.5\n",
      "\n",
      "[query_generator]\n",
      "prompt = \"\"\"\n",
      "Generate a qery for RAG based on this question: `{question}`. Return only query without additional explanations.\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "user\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "[responder]\n",
      "prompt = \"\"\"\n",
      "Act as an assistant, you have access to a RAG, information from it are prefixed with `Observation:` and not visible to user.\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "system\n",
      "\"\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa = PrimitiveRagQA(top_k=1).with_llm(llm).with_embedder(embedder)\n",
    "await qa.retriver.set_documents(\n",
    "    [\n",
    "        \"Glumbor is the capital city of the fictional country Nandor.\",\n",
    "        \"Nandor's main export is glowberries, a rare fruit used in luxury cosmetics.\",\n",
    "        \"In Nandor, the Festival of Lights is celebrated every March 3rd.\",\n",
    "    ]\n",
    ")\n",
    "print(qa.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6316fd77-a68c-432c-b685-9446f2412aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: When was [Holiday Name] celebrated in Nandor?\n",
      "\n",
      "Similarities: [0.6450673111903641, 0.5861606444083303, 0.7977003210598459]\n",
      "Context: ['In Nandor, the Festival of Lights is celebrated every March 3rd.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Okay! The Festival of Lights in Nandor is celebrated on March 3rd. Do you have any other questions about it, or would you like to explore something else?'\u001b[0m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await qa.forward(\"Hi, im interested in date of a specific holiday in Nandor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76b4f947-b179-4dfd-88ef-bdc66bc0fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is the capital?\n",
      "\n",
      "Similarities: [0.5496870513796827, 0.3114991881178381, 0.3673076383558104]\n",
      "Context: ['Glumbor is the capital city of the fictional country Nandor.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'The capital of Nandor is Glumbor.'\u001b[0m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await qa.forward(\"can you help me remember its capital?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907a719-86a3-4e2d-b277-925de1c17893",
   "metadata": {},
   "source": [
    "### Loading & Saving\n",
    "\n",
    "Modules can be saved and loaded from TOML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07f5d1fb-9b29-489e-bafe-bccf192d687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.save(\"assets/step_2_qa.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73248033-6fde-48c9-9225-7398abe8a8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[retriver]\n",
      "top_k = 1\n",
      "similarity_thr = 0.5\n",
      "\n",
      "[query_generator]\n",
      "prompt = \"\"\"\n",
      "Generate a qery for RAG based on this question: `{question}`. Return only query without additional explanations.\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "user\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "[responder]\n",
      "prompt = \"\"\"\n",
      "Act as an assistant, you have access to a RAG, information from it are prefixed with `Observation:` and not visible to user.\n",
      "\"\"\"\n",
      "\n",
      "role = \"\"\"\n",
      "system\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat assets/step_2_qa.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "218c2bf8-2557-4359-8421-7db94f7fc2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = qa.load(\"assets/step_2_qa.toml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptimus",
   "language": "python",
   "name": "promptimus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
