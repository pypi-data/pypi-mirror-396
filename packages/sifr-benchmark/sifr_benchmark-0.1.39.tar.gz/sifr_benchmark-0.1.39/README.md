# SiFR Benchmark

**How well do AI agents understand web UI?**

Benchmark comparing SiFR vs HTML vs AXTree vs Screenshots across complex websites.

> âš ï¸ **This is an example run, not a definitive study.** The benchmark is fully reproducible â€” run it yourself on your sites, your models, your use cases.

## Results

Tested on Amazon with **300KB token budget**, compound tasks (understand â†’ act).

| Format | Understand | Act | Combined | Tokens |
|--------|-----------|-----|----------|--------|
| **SiFR** | **100%** | 25% | **25%** | 173K |
| HTML | 100% | 0% | 0% | 194K |
| AXTree | 100% | 25% | 25% | 27K |
| Screenshot | 75% | 0% | 0% | 51K |

**Key insight:** HTML understands perfectly but can't act. Screenshot sees the page but has no element IDs. **Only SiFR and AXTree can both understand AND act.**

### Budget Matters

| Budget | SiFR Combined | HTML Combined | Winner |
|--------|--------------|---------------|--------|
| 300KB | **25%** | 0% | **SiFR** |
| 100KB | 0% | **50%** | **HTML** |

- **Large pages (300KB+)**: SiFR wins â€” structure survives truncation
- **Small pages (100KB)**: HTML wins â€” less overhead, more content

## What is SiFR?

**Structured Interface Format for Representation** â€” JSON format optimized for LLM understanding of web UI.

```json
{
  "id": "a015",
  "tag": "a",
  "text": "Add to Cart",
  "bbox": [500, 300, 120, 40],
  "children": []
}
```

Key advantages:

- **Actionable IDs**: Every element gets a unique ID (`a015`, `btn003`)
- **Bounding boxes**: Pixel-perfect positions for design tasks
- **Structured JSON**: LLMs understand JSON natively
- **Hierarchical**: Parent-child relationships preserved

## Installation

```bash
pip install sifr-benchmark
```

### Prerequisites

1. **[Element-to-LLM Chrome Extension](https://github.com/anthropics/anthropic-quickstarts)** â€” captures pages in SiFR format

2. **API Keys**
```bash
export OPENAI_API_KEY=sk-...
export ANTHROPIC_API_KEY=sk-ant-...  # optional
```

3. **Playwright**
```bash
playwright install chromium
```

## Quick Start

```bash
sifr-bench full-benchmark-e2llm https://www.amazon.com \
  -e /path/to/element-to-llm-extension \
  -s 300 \
  --mode compound \
  -v
```

## How It Works

### Single Session Architecture

The benchmark runs in a **single page session** â€” no reload between capture and verification:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SINGLE PAGE SESSION                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Load page         â†’ page.goto(url)                  â”‚
â”‚  2. Capture formats   â†’ SiFR, HTML, AXTree, Screenshot  â”‚
â”‚  3. Generate tasks    â†’ GPT-4o vision                   â”‚
â”‚  4. Query LLM         â†’ understand + act                â”‚
â”‚  5. Verify on page    â†’ Playwright trial click          â”‚
â”‚  6. Next URL          â†’ repeat                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this matters:** Dynamic pages (carousels, recommendations, A/B tests) change on reload. Single session ensures the element IDs from capture match the actual page during verification.

### Verification Pipeline

Act success is measured by **functional testing**, not text matching:

```
LLM Response    â†’    Resolve ID    â†’    Trial Click    â†’    Success?
   "a012"       â†’    "#product-1"   â†’    click(trial)   â†’    âœ“/âœ—
```

Verification stages:
1. **Parse** â€” extract element ID from response
2. **Resolve** â€” ID â†’ CSS selector (via SiFR data)
3. **Find** â€” selector â†’ element on page
4. **Visible** â€” element is visible?
5. **Click** â€” element is clickable?

Use `--debug` to see exactly where verification fails.

## Benchmark Modes

### ğŸ¤– Compound Tasks (AI Agents)

Understanding â†’ Action pairs for autonomous agents.

```bash
sifr-bench full-benchmark-e2llm https://amazon.com -e /path/to/ext --mode compound
```

Tasks:
- "Which product has the highest rating?" â†’ "Click on it"
- "Find items under $50" â†’ "Add to cart"
- "What's the top news story?" â†’ "Open comments"

### ğŸ‘¨â€ğŸ’» Dev Tasks (Frontend Developers)

Selectors, accessibility, structure analysis.

```bash
sifr-bench full-benchmark-e2llm https://stripe.com -e /path/to/ext --mode dev
```

Tasks:
- "What's a stable selector for the login button?" â†’ `btn042`
- "Which images are missing alt text?" â†’ `3 images`
- "List all form inputs on the page" â†’ `email, password, submit`

### ğŸ¨ Design Tasks (UI/UX Designers)

Spacing, typography, consistency checks.

```bash
sifr-bench full-benchmark-e2llm https://stripe.com -e /path/to/ext --mode design
```

Tasks:
- "What's the height of the hero section?" â†’ `~500px`
- "Are all cards the same width?" â†’ `Yes, 4 columns`
- "How many button variants exist?" â†’ `3 styles`

### ğŸ”„ Combined Mode

Run all task types at once.

```bash
sifr-bench full-benchmark-e2llm https://stripe.com -e /path/to/ext --mode combined -v
```

## Options

| Option | Description | Default |
|--------|-------------|---------|
| `-e, --extension` | Path to E2LLM extension | required |
| `-s, --target-size` | Token budget in KB | 400 |
| `-m, --models` | Models to test (comma-separated) | gpt-4o-mini |
| `--mode` | Task type: compound/dev/design/combined | compound |
| `-v, --verbose` | Show per-task results | false |
| `--debug` | Enable verification logging | false |

## Multi-Model Comparison

```bash
sifr-bench full-benchmark-e2llm https://amazon.com \
  -e /path/to/ext \
  -s 300 \
  -m gpt-4o-mini,gpt-4o,claude-haiku
```

## Supported Models

| Model | Alias | Vision |
|-------|-------|--------|
| GPT-4o | `gpt-4o` | âœ… |
| GPT-4o Mini | `gpt-4o-mini` | âœ… |
| GPT-4 Turbo | `gpt-4-turbo` | âœ… |
| Claude Sonnet 4 | `claude-sonnet` | âœ… |
| Claude Haiku 4.5 | `claude-haiku` | âœ… |
| Claude Opus 4 | `claude-opus` | âœ… |

## Output Examples

### Compound Tasks

```
Understanding + Action Results: amazon.com
â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Format     â”ƒ Understand â”ƒ Act â”ƒ Combined â”ƒ  Tokens â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ sifr       â”‚       100% â”‚ 25% â”‚      25% â”‚ 172,794 â”‚
â”‚ html_raw   â”‚       100% â”‚  0% â”‚       0% â”‚ 194,367 â”‚
â”‚ axtree     â”‚       100% â”‚ 25% â”‚      25% â”‚  27,223 â”‚
â”‚ screenshot â”‚        75% â”‚  0% â”‚       0% â”‚  51,162 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Verbose Output (-v)

```
â”â”â” https://amazon.com â”â”â”
  Loading page...
  âœ“ Captured (SiFR: 287KB)
  âœ“ 4 tasks
  Running benchmark...
    cmp_01 [sifr]: Uâœ… Aâœ… | Shop gifts by cate... â†’ a001
    cmp_02 [sifr]: Uâœ… AâŒ | Popular products... â†’ a012
      â†³ visible: Element not visible (hidden or off-screen)
    cmp_03 [html_raw]: Uâœ… AâŒ | Wireless Earbuds... â†’ .product-card
      â†³ find: Element not found on page
```

### Debug Output (--debug)

```
14:23:01 [sifr.verification] [SiFR] Resolved a012 â†’ #product-link-xyz
14:23:01 [sifr.verification] [Verify] Found 1 element(s)
14:23:01 [sifr.verification] [Verify] Not visible: #product-link-xyz
14:23:01 [sifr.verification] [sifr] FAIL: âœ— [visible] Element not visible | id=a012 â†’ sel=#product-link-xyz â†’ found=1
```

## Run Directory Structure

```
benchmark_runs/run_20251208_093517/
â”œâ”€â”€ captures/
â”‚   â”œâ”€â”€ sifr/*.sifr
â”‚   â”œâ”€â”€ html/*.html
â”‚   â”œâ”€â”€ axtree/*.json
â”‚   â””â”€â”€ screenshots/*.png
â”œâ”€â”€ ground-truth/*.json
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ raw_results.json      # Full results with verification details
â”‚   â””â”€â”€ summary.json
â””â”€â”€ run_meta.json             # Includes "single_session": true
```

## Why Each Format Fails

| Format | Understand | Act | Why |
|--------|-----------|-----|-----|
| **SiFR** | âœ… JSON structure | âœ… Has IDs | Best of both worlds |
| **HTML** | âœ… Full content | âŒ No stable IDs | Can read, can't click |
| **AXTree** | âœ… Semantic | âš ï¸ Own IDs | IDs don't match page |
| **Screenshot** | âœ… Visual | âŒ No IDs at all | Sees but can't act |

## Other Commands

```bash
# List all benchmark runs
sifr-bench list-runs

# Validate SiFR files
sifr-bench validate examples/

# Show help
sifr-bench info
```

## Use Cases

### For AI Agent Developers
- Test agent accuracy before deployment
- Compare different LLM backends
- Benchmark against baselines

### For Frontend Developers
- Generate stable test selectors
- Audit accessibility issues
- Analyze component structure

### For UI/UX Designers
- Verify design system consistency
- Check spacing and typography
- Audit visual hierarchy

## Contributing

- **Add test sites**: Run benchmark on more URLs
- **Improve ground truth**: Manual verification
- **New models**: Add support in `models.py`
- **Bug reports**: Open an issue

## License

MIT
