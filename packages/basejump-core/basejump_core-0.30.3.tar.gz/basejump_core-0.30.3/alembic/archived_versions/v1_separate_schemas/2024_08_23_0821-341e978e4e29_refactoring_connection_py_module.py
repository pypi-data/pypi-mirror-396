"""Refactoring connection.py module

Revision ID: 341e978e4e29
Revises: 5c32ec2d4beb
Create Date: 2024-08-23 08:21:09.327672

"""

import uuid
from datetime import datetime
from enum import Enum
from typing import Optional, Sequence, Union

import sqlalchemy as sa
from alembic import op
from sqlalchemy import TIMESTAMP, UUID, Column, MetaData, Table, insert, text
from sqlalchemy.dialects import postgresql
from sqlalchemy.dialects.postgresql import BYTEA
from sqlalchemy.ext.asyncio import AsyncAttrs
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from sqlalchemy.sql import func
from sqlalchemy.types import BIGINT, DateTime, Integer
from typing_extensions import Annotated

# revision identifiers, used by Alembic.
revision: str = "341e978e4e29"
down_revision: Union[str, None] = "5c32ec2d4beb"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # NOTE: Create any new schemas
    print("Starting...")

    from basejump.core.database.setup_db.db_connect import (
        ConnectDB,
        conn_params_noasync,
    )
    from sqlalchemy import text

    conn_db = ConnectDB(conn_params=conn_params_noasync)
    basejump_engine = conn_db.connect_db()
    # for suffix in ["", "0", "10", "11"]:
    for suffix in ["", "0"]:
        for schema in ["ai_model", "connect", "transaction", "account", "migration"]:
            with basejump_engine.begin() as conn:
                conn.execute(text(f"CREATE SCHEMA IF NOT EXISTS {schema}{suffix}"))
        # ### commands auto generated by Alembic - please adjust! ###

        # === Define the tables using static reference of db_models.py === #
        print(f"\n\nRunning suffix #{suffix}...")

        op.alter_column(
            table_name="database",
            column_name="conn_id",
            new_column_name="db_id",
            schema=f"connect{suffix}",
        )

        op.add_column(
            "database",
            sa.Column("db_uuid", sa.UUID(), server_default=sa.text("gen_random_uuid()"), nullable=False),
            schema=f"connect{suffix}",
        )

        # op.drop_constraint(
        #     "sql_vector_sql_conn_id_fkey", "sql_vector", schema=f"connect{suffix}", type_="foreignkey"
        # )

        op.drop_constraint(
            "database_tables_conn_id_fkey", "database_tables", schema=f"connect{suffix}", type_="foreignkey"
        )

        op.alter_column(
            table_name="database_tables",
            column_name="conn_id",
            new_column_name="db_id",
            schema=f"connect{suffix}",
        )
        op.create_foreign_key(
            "database_tables_conn_id_fkey",
            "database_tables",
            "database",
            ["db_id"],
            ["db_id"],
            source_schema=f"connect{suffix}",
            referent_schema=f"connect{suffix}",
            ondelete="CASCADE",
        )

        # Create the DB Conn table
        op.create_table(
            "db_conn",
            sa.Column("conn_id", sa.Integer(), nullable=False),
            sa.Column("db_id", sa.Integer(), nullable=False),
            sa.Column("username", sa.LargeBinary(), nullable=False),
            sa.Column("password", sa.LargeBinary(), nullable=False),
            sa.Column("timestamp", sa.TIMESTAMP(timezone=True), server_default=sa.text("now()"), nullable=False),
            sa.ForeignKeyConstraint(["conn_id"], [f"connect{suffix}.connection.conn_id"], ondelete="CASCADE"),
            sa.ForeignKeyConstraint(["db_id"], [f"connect{suffix}.database.db_id"], ondelete="CASCADE"),
            sa.PrimaryKeyConstraint("conn_id"),
            schema=f"connect{suffix}",
        )

        # Alter the columns to use a timestamp instead of etl_date
        print("Altering columns...")

        # Manually skip alters for the shared schema
        if suffix in ["", "0"]:
            op.alter_column(
                table_name="client", column_name="etl_date", new_column_name="timestamp", schema=f"account{suffix}"
            )

            op.alter_column(
                table_name="subscription",
                column_name="etl_date",
                new_column_name="timestamp",
                schema=f"account{suffix}",
            )

            op.alter_column(
                table_name="subscription_plan",
                column_name="etl_date",
                new_column_name="timestamp",
                schema=f"account{suffix}",
            )

            op.alter_column(
                table_name="team", column_name="etl_date", new_column_name="timestamp", schema=f"account{suffix}"
            )

            op.alter_column(
                table_name="user", column_name="etl_date", new_column_name="timestamp", schema=f"account{suffix}"
            )

        op.alter_column(
            table_name="chat_history", column_name="etl_date", new_column_name="timestamp", schema=f"ai_model{suffix}"
        )

        op.alter_column(
            table_name="result_history",
            column_name="etl_date",
            new_column_name="timestamp",
            schema=f"ai_model{suffix}",
        )

        op.alter_column(
            table_name="token_count", column_name="etl_date", new_column_name="timestamp", schema=f"ai_model{suffix}"
        )

        op.alter_column(
            table_name="user_chat_id", column_name="etl_date", new_column_name="timestamp", schema=f"ai_model{suffix}"
        )

        op.add_column("sql_vector", sa.Column("db_id", sa.Integer(), nullable=False), schema=f"connect{suffix}")

        op.alter_column(
            table_name="vector_db", column_name="etl_date", new_column_name="timestamp", schema=f"connect{suffix}"
        )
        op.alter_column(
            table_name="basejump_credit_ledger",
            column_name="etl_date",
            new_column_name="timestamp",
            schema=f"transaction{suffix}",
        )
        op.alter_column(
            table_name="deposit_history",
            column_name="etl_date",
            new_column_name="timestamp",
            schema=f"transaction{suffix}",
        )
        op.alter_column(
            table_name="withdraw_history",
            column_name="etl_date",
            new_column_name="timestamp",
            schema=f"transaction{suffix}",
        )
        print("Creating foreign key...")
        op.create_foreign_key(
            None,
            "sql_vector",
            "database",
            ["db_id"],
            ["db_id"],
            source_schema=f"connect{suffix}",
            referent_schema=f"connect{suffix}",
            ondelete="CASCADE",
        )
        op.drop_column("sql_vector", "sql_conn_id", schema=f"connect{suffix}")
        print("Done...")

        # ### end Alembic commands ###


def downgrade() -> None:
    # NOTE: Create any new schemas

    raise Exception(
        "downgrade not yet implemented for this version. Please refer to the upgrade above to construct the downgrade"
    )
    from basejump.core.database.setup_db.db_connect import (
        ConnectDB,
        conn_params_noasync,
    )
    from sqlalchemy import text

    conn_db = ConnectDB(conn_params=conn_params_noasync)
    basejump_engine = conn_db.connect_db()
    for suffix in ["11", "10", "0", ""]:
        for schema in ["ai_model", "connect", "transaction", "account", "migration"]:
            with basejump_engine.begin() as conn:
                conn.execute(text(f"CREATE SCHEMA IF NOT EXISTS {schema}{suffix}"))
        # ### commands auto generated by Alembic - please adjust! ###
        op.add_column(
            "withdraw_history",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("CURRENT_TIMESTAMP"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"transaction{suffix}",
        )
        op.drop_column("withdraw_history", "timestamp", schema=f"transaction{suffix}")
        op.add_column(
            "deposit_history",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("CURRENT_TIMESTAMP"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"transaction{suffix}",
        )
        op.drop_column("deposit_history", "timestamp", schema=f"transaction{suffix}")
        op.add_column(
            "basejump_credit_ledger",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("CURRENT_TIMESTAMP"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"transaction{suffix}",
        )
        op.drop_column("basejump_credit_ledger", "timestamp", schema=f"transaction{suffix}")
        op.add_column(
            "vector_db",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"connect{suffix}",
        )
        op.drop_column("vector_db", "timestamp", schema=f"connect{suffix}")
        op.add_column(
            "sql_vector",
            sa.Column("sql_conn_id", sa.INTEGER(), autoincrement=False, nullable=False),
            schema=f"connect{suffix}",
        )
        op.drop_constraint(None, "sql_vector", schema=f"connect{suffix}", type_="foreignkey")
        op.create_foreign_key(
            "sql_vector_sql_conn_id_fkey",
            "sql_vector",
            "sql_db",
            ["sql_conn_id"],
            ["sql_conn_id"],
            source_schema=f"connect{suffix}",
            referent_schema=f"connect{suffix}",
            ondelete="CASCADE",
        )

        op.drop_column("sql_vector", "db_id", schema=f"connect{suffix}")
        op.add_column(
            "user_chat_id",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("CURRENT_TIMESTAMP"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"ai_model{suffix}",
        )
        op.drop_column("user_chat_id", "timestamp", schema=f"ai_model{suffix}")
        op.add_column(
            "token_count",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("CURRENT_TIMESTAMP"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"ai_model{suffix}",
        )
        op.drop_column("token_count", "timestamp", schema=f"ai_model{suffix}")
        op.add_column(
            "result_history",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("CURRENT_TIMESTAMP"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"ai_model{suffix}",
        )
        op.drop_column("result_history", "timestamp", schema=f"ai_model{suffix}")
        op.add_column(
            "chat_history",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("CURRENT_TIMESTAMP"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"ai_model{suffix}",
        )
        op.drop_column("chat_history", "timestamp", schema=f"ai_model{suffix}")
        op.add_column(
            "user",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"account{suffix}",
        )
        op.drop_column("user", "timestamp", schema=f"account{suffix}")
        op.add_column(
            "team",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"account{suffix}",
        )
        op.drop_column("team", "timestamp", schema=f"account{suffix}")
        op.add_column(
            "subscription_plan",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("CURRENT_TIMESTAMP"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"account{suffix}",
        )
        op.drop_column("subscription_plan", "timestamp", schema=f"account{suffix}")
        op.add_column(
            "subscription",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("CURRENT_TIMESTAMP"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"account{suffix}",
        )
        op.drop_column("subscription", "timestamp", schema=f"account{suffix}")
        op.add_column(
            "client",
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                autoincrement=False,
                nullable=False,
            ),
            schema=f"account{suffix}",
        )
        op.drop_column("client", "timestamp", schema=f"account{suffix}")
        op.create_table(
            "team_tables",
            sa.Column("sql_tbl_id", sa.INTEGER(), autoincrement=False, nullable=False),
            sa.Column("team_id", sa.INTEGER(), autoincrement=False, nullable=False),
            sa.ForeignKeyConstraint(
                ["sql_tbl_id"],
                [f"connect{suffix}.sql_db_tables.sql_tbl_id"],
                name="team_tables_sql_tbl_id_fkey",
                ondelete="CASCADE",
            ),
            sa.ForeignKeyConstraint(
                ["team_id"], ["account.team.team_id"], name="team_tables_team_id_fkey", ondelete="CASCADE"
            ),
            sa.PrimaryKeyConstraint("sql_tbl_id", "team_id", name="team_tables_pkey"),
            schema=f"connect{suffix}",
        )
        op.create_table(
            "sql_db",
            sa.Column("sql_conn_id", sa.INTEGER(), autoincrement=False, nullable=False),
            sa.Column("database_type", postgresql.BYTEA(), autoincrement=False, nullable=False),
            sa.Column("drivername", postgresql.BYTEA(), autoincrement=False, nullable=False),
            sa.Column("username", postgresql.BYTEA(), autoincrement=False, nullable=False),
            sa.Column("password", postgresql.BYTEA(), autoincrement=False, nullable=False),
            sa.Column("host", postgresql.BYTEA(), autoincrement=False, nullable=False),
            sa.Column("database_name", postgresql.BYTEA(), autoincrement=False, nullable=False),
            sa.Column("port", postgresql.BYTEA(), autoincrement=False, nullable=False),
            sa.Column("query", postgresql.BYTEA(), autoincrement=False, nullable=True),
            sa.Column("schemas", postgresql.BYTEA(), autoincrement=False, nullable=True),
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                autoincrement=False,
                nullable=False,
            ),
            sa.ForeignKeyConstraint(
                ["sql_conn_id"],
                [f"connect{suffix}.connection.conn_id"],
                name="sql_db_sql_conn_id_fkey",
                ondelete="CASCADE",
            ),
            sa.PrimaryKeyConstraint("sql_conn_id", name="sql_db_pkey"),
            schema=f"connect{suffix}",
            postgresql_ignore_search_path=False,
        )
        op.create_table(
            "sql_db_tables",
            sa.Column("sql_tbl_id", sa.INTEGER(), autoincrement=True, nullable=False),
            sa.Column(
                "sql_tbl_uuid",
                sa.UUID(),
                server_default=sa.text("gen_random_uuid()"),
                autoincrement=False,
                nullable=False,
            ),
            sa.Column("sql_conn_id", sa.INTEGER(), autoincrement=False, nullable=False),
            sa.Column("table_name", sa.VARCHAR(), autoincrement=False, nullable=False),
            sa.Column("context", sa.VARCHAR(), autoincrement=False, nullable=True),
            sa.Column(
                "etl_date",
                postgresql.TIMESTAMP(timezone=True),
                server_default=sa.text("now()"),
                autoincrement=False,
                nullable=False,
            ),
            sa.ForeignKeyConstraint(
                ["sql_conn_id"],
                [f"connect{suffix}.sql_db.sql_conn_id"],
                name="sql_db_tables_sql_conn_id_fkey",
                ondelete="CASCADE",
            ),
            sa.PrimaryKeyConstraint("sql_tbl_id", name="sql_db_tables_pkey"),
            schema=f"connect{suffix}",
        )
        op.drop_table("db_conn", schema=f"connect{suffix}")
        op.drop_table("database_tables", schema=f"connect{suffix}")
        op.drop_table("database", schema=f"connect{suffix}")
        # ### end Alembic commands ###


def upgrade_migration(suffix, basejump_engine):
    from basejump.core.database.setup_db.db_connect import ConnectDB

    # New Tables
    class ConnectionType(Enum):
        """Connection type"""

        SQL: str = "sql"
        VECTOR: str = "vector_db"

    big_int = Annotated[int, "bigint"]

    class Base(AsyncAttrs, DeclarativeBase):
        type_annotation_map = {
            dict: postgresql.JSONB,
            datetime: TIMESTAMP(timezone=True),
            list: postgresql.ARRAY(Integer),
            big_int: BIGINT,
        }

    class ConnTeamAssociation(Base):
        """An association table to add and delete users from teams"""

        __tablename__ = "team_connection"
        __table_args__ = {"schema": f"connect{suffix}"}

        conn_id: Mapped[int] = mapped_column(
            primary_key=True,
        )
        team_id: Mapped[int] = mapped_column(
            primary_key=True,
        )

    class SQLVectorAssociation(Base):
        __tablename__ = "sql_vector"
        __table_args__ = {"schema": f"connect{suffix}"}

        db_id: Mapped[int] = mapped_column(
            primary_key=True,
        )
        vector_conn_id: Mapped[int] = mapped_column(
            primary_key=True,
        )

    class Connection(Base):
        __tablename__ = "connection"
        __table_args__ = {"schema": f"connect{suffix}"}

        conn_id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
        conn_uuid: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), server_default=text("gen_random_uuid()"))
        client_id: Mapped[int]

        conn_type: Mapped[str]
        data_source_desc: Mapped[str]

    class DBParams(Base):
        """Holds the information to connect to a SQL database"""

        __tablename__ = "database"
        __table_args__ = {"schema": f"connect{suffix}"}

        db_id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
        db_uuid: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), server_default=text("gen_random_uuid()"))
        database_type: Mapped[bytes]
        drivername: Mapped[bytes]
        host: Mapped[bytes]
        database_name: Mapped[bytes]
        port: Mapped[bytes]
        database_desc: Mapped[bytes]
        query: Mapped[Optional[bytes]]
        schemas: Mapped[Optional[bytes]]
        include_non_schemas: Mapped[Optional[bytes]]
        timestamp: Mapped[datetime] = mapped_column(server_default=func.now())

    class DBConn(Base):
        """Holds the information to connect to a SQL database"""

        __tablename__ = "db_conn"
        __table_args__ = {"schema": f"connect{suffix}"}

        conn_id: Mapped[int] = mapped_column(primary_key=True)
        db_id: Mapped[int]
        username: Mapped[bytes]
        password: Mapped[bytes]
        timestamp: Mapped[datetime] = mapped_column(server_default=func.now())

    class VectorDB(Base):
        """
        Holds the information of where the vector indexes are stored
        along with the information to connect to them
        """

        __tablename__ = "vector_db"
        __table_args__ = {"schema": f"connect{suffix}"}

        vector_conn_id: Mapped[int] = mapped_column(primary_key=True)
        vector_database_vendor: Mapped[str]
        vector_datasource_type: Mapped[str]
        api_key: Mapped[Optional[str]]
        environment: Mapped[Optional[str]]
        index_name: Mapped[str]
        vector_metadata: Mapped[Optional[dict]]
        # TODO: Set all times to be stored in UTC as a standard
        # This can be done like this: server_default=text("timezone('utc', now())")
        timestamp: Mapped[datetime] = mapped_column(server_default=func.now())

    class SQLTables(Base):
        """Holds the SQL tables related to a SQL DB connection"""

        __tablename__ = "database_tables"
        __table_args__ = {"schema": f"connect{suffix}"}

        sql_tbl_id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
        sql_tbl_uuid: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), server_default=text("gen_random_uuid()"))
        db_id: Mapped[int] = mapped_column()
        table_name: Mapped[str]
        context: Mapped[Optional[str]]
        timestamp: Mapped[datetime] = mapped_column(server_default=func.now())

    # Old tables
    class SQLTablesPrevious(Base):
        """Holds the SQL tables related to a SQL DB connection"""

        __tablename__ = "sql_db_tables"
        __table_args__ = {"schema": f"connect{suffix}"}

        sql_tbl_id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
        sql_tbl_uuid: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), server_default=text("gen_random_uuid()"))
        sql_conn_id: Mapped[int]
        table_name: Mapped[str]
        context: Mapped[Optional[str]]
        etl_date: Mapped[datetime] = mapped_column(server_default=func.now())

    class SQLDB(Base):
        """Holds the information to connect to a SQL database"""

        __tablename__ = "sql_db"
        __table_args__ = {"schema": f"connect{suffix}"}

        sql_conn_id: Mapped[int] = mapped_column(primary_key=True)
        database_type: Mapped[bytes]
        drivername: Mapped[bytes]
        username: Mapped[bytes]
        password: Mapped[bytes]
        host: Mapped[bytes]
        database_name: Mapped[bytes]
        port: Mapped[bytes]
        query: Mapped[Optional[bytes]]
        schemas: Mapped[Optional[bytes]]
        etl_date: Mapped[datetime] = mapped_column(server_default=func.now())

    # Insert into DBParams (connect.database) from sql_db

    # == Create an ad hoc table with the SQL conn ID and the encrypted database description ==
    print("Encrypting data source desc...")

    stmt = sa.select(
        SQLDB.sql_conn_id,
        SQLDB.database_type,
        SQLDB.drivername,
        SQLDB.username,
        SQLDB.password,
        SQLDB.host,
        SQLDB.database_name,
        SQLDB.port,
        SQLDB.query,
        SQLDB.schemas,
        SQLDB.etl_date,
        Connection.data_source_desc,
        func.rank()
        .over(
            partition_by=[
                SQLDB.database_type,
                SQLDB.drivername,
                SQLDB.host,
                SQLDB.database_name,
                SQLDB.port,
                SQLDB.query,
                SQLDB.schemas,
            ],
            order_by=SQLDB.sql_conn_id,
        )
        .label("db_id"),
    ).join(Connection, SQLDB.sql_conn_id == Connection.conn_id)

    results_base = op.execute(stmt)
    results = results_base.fetchall() if results_base else []

    # Encrypt the results
    for result in results:
        encrypted_desc = ConnectDB.encrypt_db({"data_source_desc": result.data_source_desc})
        result.data_source_desc = encrypted_desc["data_source_desc"]

    print("Creating DB ID and data source desc migration table...")

    metadata = MetaData()
    # Create a table with only a single description per DB param ID
    adhoc_desc_table = Table(
        f"{revision}_sql_db_desc",  # Name of the new table
        metadata,
        Column("db_id", Integer, primary_key=True),
        Column("database_desc", BYTEA),
        schema=f"migration{suffix}",
    )

    # Create table for SQLDB + SQL Param ID
    adhoc_sqldb_table = Table(
        f"{revision}_sql_db_mapping",  # Name of the new table
        metadata,
        Column("sql_conn_id", Integer, primary_key=True),
        Column("database_type", BYTEA),
        Column("drivername", BYTEA),
        Column("username", BYTEA),
        Column("password", BYTEA),
        Column("host", BYTEA),
        Column("database_name", BYTEA),
        Column("port", BYTEA),
        Column("query", BYTEA, nullable=True),
        Column("schemas", BYTEA, nullable=True),
        Column("etl_date", DateTime),
        Column("data_source_desc", BYTEA),
        Column("db_id", Integer),
        schema=f"migration{suffix}",
    )

    # Create the tables in the database
    metadata.create_all(basejump_engine)

    # Insert the results into the new table
    if results:
        # Insert to the desc table and dedup different database desc
        insert_stmt = adhoc_desc_table.insert()
        db_ids = set([result.db_id for result in results])
        insert_list = []
        for db_id in db_ids:
            for result in results:
                if result.db_id == db_id:
                    insert_list.append({"db_id": result.db_id, "database_desc": result.database_desc})

        op.execute(insert_stmt, insert_list)

        # Insert to the SQLDB table with new DB IDs
        insert_stmt = adhoc_sqldb_table.insert()
        op.execute(insert_stmt, [result._asdict() for result in results])

        # == Insert into the DBParams table ==
        print("Inserting into DBParams...")
        stmt = (
            sa.select(
                adhoc_sqldb_table.c.database_type,
                adhoc_sqldb_table.c.drivername,
                adhoc_sqldb_table.c.host,
                adhoc_sqldb_table.c.database_name,
                adhoc_sqldb_table.c.port,
                adhoc_desc_table.c.database_desc,
                adhoc_sqldb_table.c.query,
                adhoc_sqldb_table.c.schemas,
                adhoc_sqldb_table.c.etl_date.label("timestamp"),
            )
            .distinct()
            .join(adhoc_desc_table, adhoc_desc_table.c.db_id == adhoc_sqldb_table.c.db_id)
        )

        insert(DBParams).from_select(
            names=[
                "database_type",
                "drivername",
                "host",
                "database_name",
                "port",
                "database_desc",
                "query",
                "schemas",
                "timestamp",
            ],
            select=stmt,
        )

    print("Inserting into DBConn...")

    # Insert into the DB Conn table
    stmt = sa.select(
        adhoc_sqldb_table.c.sql_conn_id.label("conn_id"),
        adhoc_sqldb_table.c.db_id,
        adhoc_sqldb_table.c.username,
        adhoc_sqldb_table.c.password,
    )
    insert(DBConn).from_select(
        names=["conn_id", "db_id", "username", "password"],
        select=stmt,
    )

    print("Inserting into DBTables...")
    # Insert into database from sql_db_tables
    stmt = sa.select(
        SQLTablesPrevious.sql_tbl_id,
        SQLTablesPrevious.sql_tbl_uuid,
        adhoc_sqldb_table.c.db_id,
        SQLTablesPrevious.table_name,
        SQLTablesPrevious.context,
        SQLTablesPrevious.etl_date.label("timestamp"),
    ).join(adhoc_sqldb_table, adhoc_sqldb_table.c.sql_conn_id == SQLTablesPrevious.sql_conn_id)

    insert(SQLTables).from_select(
        names=["sql_tbl_id", "sql_tbl_uuid", "db_id", "table_name", "context", "timestamp"],
        select=stmt,
    )
    op.execute(f"DROP TABLE IF EXISTS connect{suffix}.sql_db_tables CASCADE;")
    op.execute(f"DROP TABLE IF EXISTS connect{suffix}.sql_db CASCADE;")
    op.execute(f"DROP TABLE IF EXISTS connect{suffix}.team_tables CASCADE;")


if __name__ == "__main__":
    from basejump.core.database.setup_db.db_connect import (
        ConnectDB,
        conn_params_noasync,
    )

    conn_db = ConnectDB(conn_params=conn_params_noasync)
    basejump_engine = conn_db.connect_db()
    # for suffix in ["", "0", "10", "11"]:
    for suffix in ["", "0"]:
        try:
            # This is specific to a database and so wrapping in try except in case this
            # is ported to another database without the data to migrate
            upgrade_migration(suffix, basejump_engine)
        except Exception:
            print(f"No data to migrate for suffix: {suffix}")
            continue
