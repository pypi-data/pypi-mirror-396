# omni_lid/pipeline.py
import os
import torch
import torchaudio
import requests
from tqdm import tqdm
from .model import OmniLIDModel
from .decoder import ViterbiDecoder
from .config import (
    LANGS, ID2LANG, SILENCE_ID, TARGET_SR, MODEL_DOWNLOAD_URL,
    DEFAULT_TRANSITION_SCALE, DEFAULT_MIN_CONFIDENCE
)

class LIDPipeline:
    def __init__(self, model_path=None, device=None):
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ Device: {self.device}")

        # 1. ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ)
        if model_path is None:
            model_path = "weights/best_lid_model_ce.pth"
            self._download_if_needed(model_path)
        
        # 2. ëª¨ë¸ ì´ˆê¸°í™”
        self.model = OmniLIDModel(len(LANGS))
        self._load_weights(model_path)
        self.model.to(self.device).eval()
        
        if self.device == "cuda":
            self.model.half() # FP16 ì¶”ë¡ 

        # 3. ë””ì½”ë” ì´ˆê¸°í™”
        self.decoder = ViterbiDecoder(
            ID2LANG, SILENCE_ID, 
            transition_scale=DEFAULT_TRANSITION_SCALE, 
            min_confidence=DEFAULT_MIN_CONFIDENCE
        )

    def _download_if_needed(self, path):
        """ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ URLì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        if os.path.exists(path):
            return
        
        print(f"ğŸ“¥ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\nURL: {MODEL_DOWNLOAD_URL}")
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        try:
            response = requests.get(MODEL_DOWNLOAD_URL, stream=True)
            response.raise_for_status()
            total_size = int(response.headers.get('content-length', 0))
            
            with open(path, 'wb') as f, tqdm(total=total_size, unit='B', unit_scale=True, desc=path) as bar:
                for chunk in response.iter_content(chunk_size=1024):
                    if chunk:
                        f.write(chunk)
                        bar.update(len(chunk))
            print("âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"âš ï¸ '{path}' ìœ„ì¹˜ì— ëª¨ë¸ íŒŒì¼ì„ ì§ì ‘ ë„£ì–´ì£¼ì„¸ìš”.")
            raise e

    def _load_weights(self, path):
        try:
            checkpoint = torch.load(path, map_location="cpu")
            state_dict = checkpoint["model_state_dict"] if "model_state_dict" in checkpoint else checkpoint
            new_state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
            self.model.load_state_dict(new_state_dict, strict=False)
            print("âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e

    def predict(self, audio_input):
        """
        audio_input: íŒŒì¼ ê²½ë¡œ(str) ë˜ëŠ” Tensor
        returns: ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        # 1. ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬
        if isinstance(audio_input, str):
            wav, sr = torchaudio.load(audio_input)
            if sr != TARGET_SR:
                wav = torchaudio.transforms.Resample(sr, TARGET_SR)(wav)
        else:
            wav = audio_input

        # ëª¨ë…¸ ë³€í™˜ & ì •ê·œí™”
        if wav.ndim > 1: wav = wav.mean(dim=0)
        wav = (wav - wav.mean()) / torch.sqrt(wav.var() + 1e-7)

        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ & Device ì´ë™
        input_values = wav.unsqueeze(0).to(self.device)
        if self.device == "cuda": input_values = input_values.half()

        # 2. ì¶”ë¡ 
        with torch.no_grad():
            logits = self.model(input_values)

        # 3. ë””ì½”ë”© (Viterbi)
        best_path, probs = self.decoder.decode(logits.squeeze())
        segments = self.decoder.get_segments(best_path)

        return segments