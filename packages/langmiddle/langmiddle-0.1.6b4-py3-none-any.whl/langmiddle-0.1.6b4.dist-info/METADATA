Metadata-Version: 2.4
Name: langmiddle
Version: 0.1.6b4
Summary: Middlewares for LangChain / LangGraph
Author-email: Alpha x1 <alpha.xone@outlook.com>
License: MIT
Project-URL: Homepage, https://github.com/alpha-xone/langmiddle
Project-URL: Repository, https://github.com/alpha-xone/langmiddle
Project-URL: Issues, https://github.com/alpha-xone/langmiddle/issues
Project-URL: Documentation, https://github.com/alpha-xone/langmiddle#readme
Keywords: langchain,langgraph,middleware,ai,llm,agents
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Application Frameworks
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: langchain
Requires-Dist: langgraph
Requires-Dist: langchain-core
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: black>=22.0; extra == "dev"
Requires-Dist: isort>=5.0; extra == "dev"
Requires-Dist: flake8>=4.0; extra == "dev"
Requires-Dist: mypy>=0.950; extra == "dev"
Requires-Dist: build>=0.8; extra == "dev"
Requires-Dist: twine>=4.0; extra == "dev"
Provides-Extra: postgres
Requires-Dist: psycopg2-binary>=2.9.0; extra == "postgres"
Requires-Dist: python-dotenv>=1.0.0; extra == "postgres"
Provides-Extra: supabase
Requires-Dist: supabase>=2.20.0; extra == "supabase"
Requires-Dist: python-jose>=3.3.0; extra == "supabase"
Requires-Dist: python-dotenv>=1.0.0; extra == "supabase"
Requires-Dist: psycopg2-binary>=2.9.0; extra == "supabase"
Requires-Dist: click>=8.0.0; extra == "supabase"
Provides-Extra: firebase
Requires-Dist: firebase-admin>=6.0.0; extra == "firebase"
Provides-Extra: sqlite
Requires-Dist: sqlite-vec>=0.1.0; extra == "sqlite"
Provides-Extra: all
Requires-Dist: supabase>=2.20.0; extra == "all"
Requires-Dist: python-jose>=3.3.0; extra == "all"
Requires-Dist: python-dotenv>=1.0.0; extra == "all"
Requires-Dist: psycopg2-binary>=2.9.0; extra == "all"
Requires-Dist: firebase-admin>=6.0.0; extra == "all"
Requires-Dist: sqlite-vec>=0.1.0; extra == "all"
Dynamic: license-file

# ğŸ§© LangMiddle â€” Production Middleware for LangGraph

> **Supercharge your LangGraph agents with plugâ€‘andâ€‘play memory, context management, and chat persistence.**

[![CI](https://github.com/alpha-xone/langmiddle/actions/workflows/ci.yml/badge.svg)](https://github.com/alpha-xone/langmiddle/actions/workflows/ci.yml)
[![PyPI version](https://img.shields.io/pypi/v/langmiddle.svg)](https://pypi.org/project/langmiddle/)
[![Python versions](https://img.shields.io/pypi/pyversions/langmiddle.svg)](https://pypi.org/project/langmiddle/)
[![License](https://img.shields.io/github/license/alpha-xone/langmiddle.svg)](https://github.com/alpha-xone/langmiddle/blob/main/LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/alpha-xone/langmiddle?style=social)](https://github.com/alpha-xone/langmiddle)

---

## ğŸ¯ Why LangMiddle?

Building production LangGraph agents? You need:
- ğŸ’¾ **Persistent chat history** across sessions
- ğŸ§  **Long-term memory** that remembers user preferences and context
- ğŸ” **Semantic fact retrieval** to inject relevant knowledge
- ğŸ—‘ï¸ **Clean message handling** without tool noise

LangMiddle delivers all of this with **zero boilerplate**â€”just add middleware to your agent.

### âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| **ğŸš€ Zero Config Start** | Works out-of-the-box with in-memory SQLiteâ€”no database setup |
| **ğŸ”„ Multi-Backend Storage** | Switch between SQLite, PostgreSQL, Supabase, Firebase with one parameter |
| **ğŸ§  Semantic Memory** | Automatic fact extraction, deduplication, and context injection |
| **ğŸ“ Smart Summarization** | Auto-compress long conversations while preserving context |
| **ğŸ” Production Ready** | JWT auth, RLS support, type-safe APIs, comprehensive logging |
| **âš¡ LangGraph Native** | Built for LangChain/LangGraph v1 middleware pattern |

---

## ğŸ“‹ Table of Contents

- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Available Middleware](#-available-middleware)
  - [ChatSaver](#chatsaver---persist-conversations)
  - [ToolRemover](#toolremover---clean-tool-messages)
  - [ContextEngineer](#contextengineer---intelligent-memory--context)
- [Storage Backends](#-storage-backends)
- [Examples](#-examples)
- [Contributing](#-contributing)

---

## ğŸ“¦ Installation

**Core Package** (includes SQLite support):
```bash
pip install langmiddle
```

**With Optional Backends:**
```bash
# For SQLite with vector search (sqlite-vec)
pip install langmiddle[sqlite]

# For PostgreSQL
pip install langmiddle[postgres]

# For Supabase (includes PostgreSQL)
pip install langmiddle[supabase]

# For Firebase
pip install langmiddle[firebase]

# All backends + extras
pip install langmiddle[all]
```

---

## ğŸš€ Quick Start

### Basic Chat Persistence (SQLite)

Get started in 30 seconds:

```python
from langchain.agents import create_agent
from langmiddle.history import ChatSaver, StorageContext

agent = create_agent(
    model="openai:gpt-4o",
    tools=[],
    context_schema=StorageContext,
    middleware=[
        ChatSaver()  # Uses in-memory SQLite by default
    ],
)

# Chat history automatically saved!
agent.invoke(
    input={"messages": [{"role": "user", "content": "Hello!"}]},
    context=StorageContext(
        thread_id="conversation-123",
        user_id="user-456"
    )
)
```

### Production Setup (Recommended)

For production apps, use `StorageConfig` to share settings between middleware components (like `ContextEngineer` for memory and `ChatSaver` for history).

```python
from langchain.agents import create_agent
from langmiddle import StorageConfig
from langmiddle.history import ChatSaver, StorageContext
from langmiddle.context import ContextEngineer

# 1. Define shared configuration (e.g., for Supabase)
config = StorageConfig(
    backend="supabase",
    enable_facts=True,
    auto_create_tables=True
)

# 2. Create agent with middleware
agent = create_agent(
    model="openai:gpt-4o",
    tools=[],
    context_schema=StorageContext,
    middleware=[
        # Both components use the same config
        ContextEngineer(
            model="openai:gpt-4o",
            embedder="openai:text-embedding-3-small",
            backend=config
        ),
        ChatSaver(backend=config)
    ]
)

# 3. Invoke with context
agent.invoke(
    input={"messages": [{"role": "user", "content": "I'm vegan."}]},
    context=StorageContext(
        thread_id="thread-1",
        user_id="user-1",
        auth_token="your-jwt-token"
    )
)
```

---

## ğŸ” How It Works

### Message Flow

```
User Input
    â†“
[ToolRemover] â† Cleans tool messages (optional)
    â†“
[ContextEngineer.before_agent] â† Injects facts + summary
    â†“
ğŸ¤– LangGraph Agent
    â†“
[ContextEngineer.after_agent] â† Extracts new facts
    â†“
[ChatSaver] â† Persists conversation
    â†“
Response
```

### Fact Lifecycle

```
Conversation â†’ Extraction â†’ Deduplication â†’ Embedding â†’ Storage
                                â†“                          â†“
                          Query & Retrieve         Relevance Scoring
                                â†“                (recency + access + usage)
                          Context Injection              â†“
                          (adaptive detail)      Combined Score
                                â†“                (70% similarity
                               Agent              + 30% relevance)
```

**Phase 3 Relevance Scoring:**
- **Recency** (40%): Newer facts score higher (exponential decay over 365 days)
- **Access Frequency** (30%): Facts used more often get boosted
- **Usage Feedback** (30%): Facts appearing in agent responses are prioritized
- **Adaptive Formatting**: High-relevance facts (â‰¥0.8) get full detail, medium (0.5-0.8) compact, low (0.3-0.5) minimal

---

## ğŸ› ï¸ Available Middleware

### ChatSaver - Persist Conversations

**Automatically save** chat histories to your database of choice.

**Features:**
- âœ… Multi-backend: SQLite, PostgreSQL, Supabase, Firebase
- âœ… Automatic deduplication (skips already-saved messages)
- âœ… Save interval control (every N turns)
- âœ… Custom state persistence

**Example:**
```python
from langmiddle.history import ChatSaver
from langmiddle import StorageConfig

# Option 1: Simple string setup
ChatSaver(
    backend="sqlite",
    db_path="./chat.db",
    save_interval=1
)

# Option 2: Shared config object (Recommended)
config = StorageConfig(backend="sqlite", db_path="./chat.db")
ChatSaver(backend=config)
```

**Supported Backends:**
| Backend | Use Case | Auth Required |
|---------|----------|---------------|
| SQLite | Development, local apps | âŒ No |
| PostgreSQL | Self-hosted production | âŒ No |
| Supabase | Managed PostgreSQL + RLS | âœ… JWT |
| Firebase | Mobile, real-time apps | âœ… ID token |

---

### ToolRemover - Clean Tool Messages

**Remove tool-related clutter** from conversation history.

**Why?** Tool call messages and responses bloat chat history and aren't relevant for long-term storage.

**Example:**
```python
from langmiddle.history import ToolRemover

middleware=[
    ToolRemover(when="both"),  # Filter before AND after agent
    ChatSaver()  # Clean messages are saved
]
```

**Options:**
- `when="before"` - Filter before agent sees messages
- `when="after"` - Filter before saving to storage
- `when="both"` - Filter in both directions (recommended)

---

### ContextEngineer - Intelligent Memory & Context

**The brain of your agent** â€” automatic fact extraction, semantic search, and context injection.

#### ğŸ§  What It Does

1. **Extracts Facts**: Identifies user preferences, goals, and key information
2. **Stores Semantically**: Embeds facts for similarity search
3. **Retrieves Contextually**: Injects relevant memories based on user queries
4. **Auto-Summarizes**: Compresses old conversations to save tokens

#### ğŸ”¥ Key Features

| Feature | Description |
|---------|-------------|
| **Semantic Fact Storage** | Vector-based storage with deduplication |
| **Smart Extraction** | Filters out ephemeral states (e.g., "user understood") |
| **Namespace Organization** | Hierarchical fact categories (`["user", "preferences", "food"]`) |
| **Automatic Summarization** | Configurable token thresholds |
| **Atomic Query Breaking** | Splits complex queries for better retrieval |
| **Relevance Scoring** | Dynamic scoring based on recency, access patterns, and usage feedback |
| **Adaptive Formatting** | Context detail adjusts based on fact relevance |
| **Caching** | Embeddings and core facts cached for performance |

#### ğŸ“ Example Usage

```python
from langmiddle import StorageConfig
from langmiddle.context import ContextEngineer

# 1. Define configuration
config = StorageConfig(
    backend="supabase",
    auto_create_tables=True,
    enable_facts=True
)

# 2. Initialize middleware
agent = create_agent(
    model="openai:gpt-4o",
    tools=[],
    context_schema=StorageContext,
    middleware=[
        ContextEngineer(
            model="openai:gpt-4o",
            embedder="openai:text-embedding-3-small",
            backend=config,  # Pass config object
            max_tokens_before_summarization=5000,
            extraction_interval=3
        )
    ],
)

# 3. Use it!
response = agent.invoke(
    input={"messages": [{"role": "user", "content": "I love spicy Thai food"}]},
    context=StorageContext(
        thread_id="conversation-123",
        user_id="user-456",
        auth_token="your-jwt-token"
    )
)

# Later in the conversation...
response = agent.invoke(
    input={"messages": [{"role": "user", "content": "Recommend a restaurant"}]},
    context=StorageContext(
        thread_id="conversation-123",
        user_id="user-456",
        auth_token="your-jwt-token"
    )
)
# Agent remembers: "user loves spicy Thai food" and uses it for recommendations!
```

#### âš™ï¸ Configuration Options

```python
ContextEngineer(
    model="openai:gpt-4o",                    # LLM for extraction & summarization
    embedder="openai:text-embedding-3-small", # Embedding model for semantic search
    backend="supabase",                       # Storage backend

    # Extraction settings
    extraction_interval=3,                    # Extract facts every N turns
    max_tokens_before_extraction=None,        # Or trigger by token count

    # Summarization settings
    max_tokens_before_summarization=5000,     # Auto-summarize at 5k tokens

    # Context injection
    core_namespaces=[                         # Always-loaded fact categories
        ["user", "personal_info"],
        ["user", "preferences"]
    ],

    # Relevance scoring (Phase 3)
    relevance_threshold=0.3,                  # Minimum relevance score to inject
    similarity_weight=0.7,                    # Weight for vector similarity
    relevance_weight=0.3,                     # Weight for relevance score
    enable_adaptive_formatting=True,          # Adjust detail based on relevance

    # Backend configuration
    backend_kwargs={'enable_facts': True}
)
```

#### ğŸ“Š What Gets Stored

**Facts Examples:**
```json
[
  {
    "content": "User prefers concise and formal answers",
    "namespace": ["user", "preferences", "communication"],
    "intensity": 1.0,
    "confidence": 0.97,
    "language": "en"
  },
  {
    "content": "User's name is Alex",
    "namespace": ["user", "personal_info"],
    "intensity": 0.9,
    "confidence": 0.98,
    "language": "en"
  }
]
```

**What's NOT stored** (filtered out by design):
- âŒ Transient states: "User understands X", "User is satisfied"
- âŒ Single-use requests: "User wants a code example"
- âŒ Politeness markers: "User says thank you"
- âŒ Momentary emotions: "User feels frustrated right now"

---

## ğŸ’¾ Storage Backends

### Comparison Guide

| Backend | Best For | Setup Complexity | Scalability | Vector Search | Auth | Cost |
|---------|----------|------------------|-------------|---------------|------|------|
| **SQLite** | â€¢ Local development<br>â€¢ Demos<br>â€¢ Single-user apps | â­ Trivial | ğŸ”µ Single machine | âœ… (sqlite-vec) | None | Free |
| **PostgreSQL** | â€¢ Self-hosted production<br>â€¢ Custom infrastructure<br>â€¢ Full control | â­â­ Medium | ğŸ”µğŸ”µğŸ”µ High (with replication) | âœ… (pgvector) | Custom | Infrastructure cost |
| **Supabase** | â€¢ Web apps<br>â€¢ Multi-tenant SaaS<br>â€¢ Real-time features | â­â­ Easy | ğŸ”µğŸ”µğŸ”µ High (managed) | âœ… (pgvector) | JWT + RLS | Free tier + usage |
| **Firebase** | â€¢ Mobile apps<br>â€¢ Google Cloud ecosystem<br>â€¢ Real-time sync | â­â­ Easy | ğŸ”µğŸ”µğŸ”µ Global (managed) | âŒ | Firebase Auth | Free tier + usage |

---

### ğŸ—‚ï¸ Backend Configuration

<details>
<summary><b>SQLite</b> â€” Zero-config local storage with vector search</summary>

```python
from langmiddle.history import ChatSaver
from langmiddle.context import ContextEngineer

# Basic chat storage (file-based)
ChatSaver(backend="sqlite", db_path="./chat.db")

# With semantic memory (requires sqlite-vec)
# Install: pip install langmiddle[sqlite]
store = ChatStorage.create(
    "sqlite",
    db_path="./chat.db",
    auto_create_tables=True,
    enable_facts=True  # Enables vector similarity search
)

# In-memory (testing/dev)
ChatSaver(backend="sqlite", db_path=":memory:")
```

**Features:**
- âœ… Zero configuration required
- âœ… Vector similarity search via [sqlite-vec](https://github.com/asg017/sqlite-vec)
- âœ… Full Phase 3 relevance scoring
- âœ… Perfect for local development and demos

**No environment variables needed!**

</details>

<details>
<summary><b>PostgreSQL</b> â€” Self-hosted database</summary>

**Environment variables** (`.env`):
```bash
POSTGRES_CONNECTION_STRING=postgresql://user:password@localhost:5432/dbname
```

**Python code:**
```python
from langmiddle.storage import ChatStorage

# First-time setup: create tables
store = ChatStorage.create(
    "postgres",
    connection_string="postgresql://user:pass@localhost:5432/db",
    auto_create_tables=True
)

# In middleware
ChatSaver(backend="postgres")
```

ğŸ“– [PostgreSQL Setup Guide](docs/POSTGRES_SETUP.md)

</details>

<details>
<summary><b>Supabase</b> â€” Managed PostgreSQL with RLS</summary>

**Environment variables** (`.env`):
```bash
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your-anon-key

# For table creation (one-time):
SUPABASE_CONNECTION_STRING=postgresql://postgres:[password]@db.[project].supabase.co:5432/postgres
```

**Python code:**
```python
from langmiddle.context import ContextEngineer

# First-time setup: create tables
store = ChatStorage.create(
    "supabase",
    auto_create_tables=True,
    enable_facts=True  # Enable semantic memory tables
)

# In middleware
ContextEngineer(
    model="openai:gpt-4o",
    embedder="openai:text-embedding-3-small",
    backend="supabase",
    backend_kwargs={'enable_facts': True}
)
```

**Context requirements:**
```python
context=StorageContext(
    thread_id="conversation-123",
    user_id="user-456",
    auth_token="jwt-token-from-supabase-auth"  # Required for RLS
)
```

</details>

<details>
<summary><b>Firebase</b> â€” Real-time NoSQL database</summary>

**Service Account Setup:**
1. Download service account JSON from Firebase Console
2. Set environment variable OR pass path directly

**Option 1: Environment variable**
```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/firebase-creds.json"
```

**Option 2: Direct path**
```python
ChatSaver(
    backend="firebase",
    credentials_path="./firebase-creds.json"
)
```

**Context requirements:**
```python
context=StorageContext(
    thread_id="conversation-123",
    user_id="user-456",
    auth_token="firebase-id-token"  # From Firebase Auth
)
```

</details>

---

## ğŸ“š Examples

### ğŸ“ Complete Examples

<details>
<summary><b>1. Simple Chat Bot with Persistence</b></summary>

```python
from langchain.agents import create_agent
from langmiddle.history import ChatSaver, StorageContext

agent = create_agent(
    model="openai:gpt-4o",
    tools=[],
    context_schema=StorageContext,
    middleware=[ChatSaver(backend="sqlite", db_path="./chatbot.db")]
)

# Conversation 1
agent.invoke(
    {"messages": [{"role": "user", "content": "My name is Alice"}]},
    context=StorageContext(thread_id="thread-1", user_id="alice")
)

# Conversation 2 (same thread - history preserved!)
agent.invoke(
    {"messages": [{"role": "user", "content": "What's my name?"}]},
    context=StorageContext(thread_id="thread-1", user_id="alice")
)
# Response: "Your name is Alice"
```

</details>

<details>
<summary><b>2. Agent with Tools (Clean History)</b></summary>

```python
from langchain.agents import create_agent
from langmiddle.history import ChatSaver, ToolRemover, StorageContext

def search_tool(query: str) -> str:
    return f"Search results for: {query}"

agent = create_agent(
    model="openai:gpt-4o",
    tools=[search_tool],
    context_schema=StorageContext,
    middleware=[
        ToolRemover(when="both"),  # Remove tool clutter
        ChatSaver(backend="sqlite", db_path="./agent.db")
    ]
)

agent.invoke(
    {"messages": [{"role": "user", "content": "Search for LangGraph tutorials"}]},
    context=StorageContext(thread_id="thread-1", user_id="user-1")
)
# Only user/assistant messages saved, no tool call noise!
```

</details>

<details>
<summary><b>3. Production Agent with Memory (Supabase)</b></summary>

```python
from langchain.agents import create_agent
from langmiddle import StorageConfig
from langmiddle.context import ContextEngineer
from langmiddle.storage import ChatStorage
import os

# 1. Define shared config
config = StorageConfig(
    backend="supabase",
    enable_facts=True,
    auto_create_tables=True
)

# 2. One-time setup (optional, if not using auto_create_tables in config)
if os.getenv("INIT_TABLES"):
    store = ChatStorage.create(**config.to_kwargs())
    print("âœ… Tables created!")
    exit()

# 3. Create agent
agent = create_agent(
    model="openai:gpt-4o",
    tools=[],
    context_schema=StorageContext,
    middleware=[
        ContextEngineer(
            model="openai:gpt-4o",
            embedder="openai:text-embedding-3-small",
            backend=config,
            max_tokens_before_summarization=5000,
            extraction_interval=3
        )
    ]
)

# 4. Use in your app
def chat(user_id: str, thread_id: str, message: str, jwt_token: str):
    response = agent.invoke(
        {"messages": [{"role": "user", "content": message}]},
        context=StorageContext(
            thread_id=thread_id,
            user_id=user_id,
            auth_token=jwt_token
        )
    )
    return response["messages"][-1]["content"]

# Example usage
chat(
    user_id="user-123",
    thread_id="thread-456",
    message="I prefer vegetarian food and hate spicy dishes",
    jwt_token="eyJ..."
)
# Facts extracted: ["User prefers vegetarian food", "User dislikes spicy dishes"]

chat(
    user_id="user-123",
    thread_id="thread-789",
    message="Recommend a restaurant",
    jwt_token="eyJ..."
)
# Agent uses stored preferences to recommend vegetarian, non-spicy options!
```

</details>

<details>
<summary><b>4. Custom Configuration</b></summary>

```python
from langmiddle.context import (
    ContextEngineer,
    ExtractionConfig,
    SummarizationConfig,
    ContextConfig
)

agent = create_agent(
    model="openai:gpt-4o",
    tools=[],
    context_schema=StorageContext,
    middleware=[
        ContextEngineer(
            model="openai:gpt-4o",
            embedder="openai:text-embedding-3-small",
            backend="supabase",

            # Custom extraction settings
            extraction_config=ExtractionConfig(
                interval=5,              # Extract every 5 turns
                max_tokens=2000,         # Or when 2k tokens accumulated
                prompt="<custom prompt>"  # Override extraction prompt
            ),

            # Custom summarization settings
            summarization_config=SummarizationConfig(
                max_tokens=8000,         # Summarize at 8k tokens
                keep_ratio=0.3,          # Keep last 30% of messages
                prefix="## Summary:\n"  # Custom prefix
            ),

            # Custom context injection
            context_config=ContextConfig(
                core_namespaces=[        # Custom always-loaded categories
                    ["user", "profile"],
                    ["user", "preferences"],
                    ["project", "settings"]
                ]
            ),

            backend_kwargs={'enable_facts': True}
        )
    ]
)
```

</details>

---

## ğŸ¨ Architecture Highlights

- **ğŸ”Œ Modular Design**: Mix and match middleware components
- **ğŸ¯ Single Responsibility**: Each middleware does one thing well
- **âš¡ Performance**: Embedding caching, batch operations, efficient queries
- **ğŸ›¡ï¸ Type Safety**: Full Pydantic validation and type hints
- **ğŸ“Š Observable**: Structured logging with operation IDs and metrics
- **ğŸ§ª Testable**: Clean abstractions, dependency injection

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

- ğŸ› **Report bugs** via [GitHub Issues](https://github.com/alpha-xone/langmiddle/issues)
- ğŸ’¡ **Request features** or improvements
- ğŸ”§ **Submit PRs** for bug fixes or new features
- ğŸ“– **Improve docs** with examples or clarifications
- â­ **Star the repo** if LangMiddle helped you!

### Development Setup

```bash
git clone https://github.com/alpha-xone/langmiddle.git
cd langmiddle
pip install -e ".[dev]"
pytest
```

---

## ğŸ“„ License

Apache License 2.0 â€” see [LICENSE](LICENSE) for details.

---

## ğŸŒŸ Show Your Support

If LangMiddle saves you time or helps your project, please:
- â­ **Star the repo** on GitHub
- ğŸ¦ Share on Twitter/X
- ğŸ’¬ Tell others in the LangChain community

**Built with â¤ï¸ for the LangGraph ecosystem**
