{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning-to-Rank from Scratch: LambdaMART with MovieLens\n",
    "\n",
    "This notebook implements a Learning-to-Rank system using LambdaMART (via LightGBM) for query-document ranking.\n",
    "\n",
    "## Overview\n",
    "- **Dataset**: MovieLens with relevance labels\n",
    "- **Features**: TF-IDF similarity, document popularity, engagement signals\n",
    "- **Model**: LambdaMART using LightGBM with pairwise preference learning\n",
    "- **Baseline**: BM25\n",
    "- **Metrics**: NDCG@10, MAP, Precision@K\n",
    "- **Validation**: Cross-validation with metric comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "from rank_bm25 import BM25Okapi\n",
    "import warnings\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration Constants\n",
    "LIKED_MOVIE_THRESHOLD = 4  # Rating threshold to consider a movie as 'liked'\n",
    "RELEVANCE_MAPPING = {\n",
    "    # Convert 1-5 star ratings to 0-3 relevance labels\n",
    "    # 1-2 stars -> 0 (not relevant)\n",
    "    # 3 stars -> 1 (somewhat relevant) \n",
    "    # 4 stars -> 2 (relevant)\n",
    "    # 5 stars -> 3 (highly relevant)\n",
    "}\n",
    "\n",
    "def rating_to_relevance(rating):\n",
    "    \"\"\"Convert rating (1-5) to relevance label (0-3)\"\"\"\n",
    "    return 0 if rating <= 2 else (rating - 2)\n",
    "\n",
    "print(f\"Configuration: Liked movie threshold = {LIKED_MOVIE_THRESHOLD}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "We'll use the MovieLens 100K dataset and create query-document-relevance triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download and extract MovieLens dataset\n",
    "def download_movielens():\n",
    "    url = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
    "    zip_path = 'ml-100k.zip'\n",
    "    \n",
    "    if not os.path.exists('ml-100k'):\n",
    "        print(\"Downloading MovieLens 100K dataset...\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        \n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        \n",
    "        os.remove(zip_path)\n",
    "        print(\"Dataset ready!\")\n",
    "    else:\n",
    "        print(\"Dataset already exists.\")\n",
    "\n",
    "download_movielens()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load MovieLens data\n",
    "def load_movielens():\n",
    "    # Load ratings\n",
    "    ratings = pd.read_csv('ml-100k/u.data', \n",
    "                         sep='\\t', \n",
    "                         names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
    "    \n",
    "    # Load movies with proper encoding\n",
    "    movies = pd.read_csv('ml-100k/u.item', \n",
    "                        sep='|', \n",
    "                        encoding='latin-1',\n",
    "                        names=['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url',\n",
    "                               'unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy',\n",
    "                               'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n",
    "                               'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])\n",
    "    \n",
    "    # Load user data\n",
    "    users = pd.read_csv('ml-100k/u.user',\n",
    "                       sep='|',\n",
    "                       names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n",
    "    \n",
    "    return ratings, movies, users\n",
    "\n",
    "ratings_df, movies_df, users_df = load_movielens()\n",
    "\n",
    "print(f\"Ratings shape: {ratings_df.shape}\")\n",
    "print(f\"Movies shape: {movies_df.shape}\")\n",
    "print(f\"Users shape: {users_df.shape}\")\n",
    "print(\"\\nSample ratings:\")\n",
    "print(ratings_df.head())\n",
    "print(\"\\nSample movies:\")\n",
    "print(movies_df[['movie_id', 'title']].head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "We'll create three types of features:\n",
    "1. **TF-IDF Similarity**: Text similarity between user profile and movie\n",
    "2. **Document Popularity**: Movie popularity metrics\n",
    "3. **Engagement Signals**: User-movie interaction patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create document text from movie metadata\n",
    "genre_cols = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy',\n",
    "              'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n",
    "              'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "def create_movie_documents(movies_df):\n",
    "    \"\"\"Create text documents from movie metadata for TF-IDF\"\"\"\n",
    "    documents = []\n",
    "    for _, movie in movies_df.iterrows():\n",
    "        # Combine title and genres\n",
    "        title = str(movie['title']).lower()\n",
    "        genres = ' '.join([genre.lower() for genre in genre_cols if movie[genre] == 1])\n",
    "        doc = f\"{title} {genres}\"\n",
    "        documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "movie_documents = create_movie_documents(movies_df)\n",
    "movies_df['document'] = movie_documents\n",
    "\n",
    "print(\"Sample movie documents:\")\n",
    "for i in range(3):\n",
    "    print(f\"Movie {i+1}: {movie_documents[i]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compute popularity features\n",
    "def compute_popularity_features(ratings_df):\n",
    "    \"\"\"Calculate movie popularity metrics\"\"\"\n",
    "    popularity = ratings_df.groupby('movie_id').agg({\n",
    "        'rating': ['count', 'mean', 'std'],\n",
    "        'user_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    popularity.columns = ['movie_id', 'num_ratings', 'avg_rating', 'std_rating', 'num_users']\n",
    "    popularity['std_rating'] = popularity['std_rating'].fillna(0)\n",
    "    \n",
    "    # Popularity score (Wilson score)\n",
    "    popularity['popularity_score'] = popularity['num_ratings'] * popularity['avg_rating']\n",
    "    \n",
    "    return popularity\n",
    "\n",
    "popularity_features = compute_popularity_features(ratings_df)\n",
    "print(\"Popularity features:\")\n",
    "print(popularity_features.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compute engagement features\n",
    "def compute_engagement_features(ratings_df, users_df):\n",
    "    \"\"\"Calculate user engagement metrics\"\"\"\n",
    "    # User activity\n",
    "    user_activity = ratings_df.groupby('user_id').agg({\n",
    "        'rating': ['count', 'mean', 'std'],\n",
    "        'movie_id': 'nunique'\n",
    "    }).reset_index()\n",
    "    user_activity.columns = ['user_id', 'user_num_ratings', 'user_avg_rating', \n",
    "                             'user_std_rating', 'user_num_movies']\n",
    "    user_activity['user_std_rating'] = user_activity['user_std_rating'].fillna(0)\n",
    "    \n",
    "    # Merge with user demographics\n",
    "    user_features = user_activity.merge(users_df, on='user_id', how='left')\n",
    "    \n",
    "    return user_features\n",
    "\n",
    "engagement_features = compute_engagement_features(ratings_df, users_df)\n",
    "print(\"Engagement features:\")\n",
    "print(engagement_features.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create query-document-relevance triplets",
    "def create_ranking_dataset(ratings_df, movies_df, popularity_features, engagement_features):",
    "    \"\"\"Create dataset with query (user), document (movie), and relevance (rating)\"\"\"",
    "    # Merge all features",
    "    dataset = ratings_df.copy()",
    "    dataset = dataset.merge(movies_df[['movie_id', 'document'] + genre_cols], on='movie_id', how='left')",
    "    dataset = dataset.merge(popularity_features, on='movie_id', how='left')",
    "    dataset = dataset.merge(engagement_features, on='user_id', how='left')",
    "    ",
    "    # Create relevance labels (convert ratings to relevance: 1-2 -> 0, 3 -> 1, 4 -> 2, 5 -> 3)",
    "    dataset['relevance'] = dataset['rating'].apply(rating_to_relevance)",
    "    ",
    "    return dataset",
    "",
    "ranking_data = create_ranking_dataset(ratings_df, movies_df, popularity_features, engagement_features)",
    "print(f\"Ranking dataset shape: {ranking_data.shape}\")",
    "print(\"\\nRelevance distribution:\")",
    "print(ranking_data['relevance'].value_counts().sort_index())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compute TF-IDF features for user-movie pairsdef compute_tfidf_features(ranking_data, movies_df):    \"\"\"Compute TF-IDF similarity between user profile and movies\"\"\"    # Create user profiles based on their highly-rated movies    user_profiles = {}    for user_id in ranking_data['user_id'].unique():        user_movies = ranking_data[ranking_data['user_id'] == user_id]        # Get movies rated >= 4 by this user        liked_movies = user_movies[user_movies['rating'] >= LIKED_MOVIE_THRESHOLD]['movie_id'].values        if len(liked_movies) > 0:            liked_docs = movies_df[movies_df['movie_id'].isin(liked_movies)]['document'].values            user_profiles[user_id] = ' '.join(liked_docs)        else:            user_profiles[user_id] = \"\"        # Compute TF-IDF    tfidf = TfidfVectorizer(max_features=100, stop_words='english')    movie_docs = movies_df['document'].values    tfidf_matrix = tfidf.fit_transform(movie_docs)        # Compute similarity for each user-movie pair    tfidf_scores = []    for _, row in ranking_data.iterrows():        user_id = row['user_id']        movie_id = row['movie_id']                user_profile = user_profiles.get(user_id, \"\")        if user_profile:            user_vec = tfidf.transform([user_profile])            movie_idx = movies_df[movies_df['movie_id'] == movie_id].index[0]            movie_vec = tfidf_matrix[movie_idx]            similarity = cosine_similarity(user_vec, movie_vec)[0][0]        else:            similarity = 0.0                tfidf_scores.append(similarity)        return np.array(tfidf_scores)print(\"Computing TF-IDF features (this may take a moment)...\")ranking_data['tfidf_similarity'] = compute_tfidf_features(ranking_data, movies_df)print(\"TF-IDF features computed!\")print(f\"TF-IDF similarity stats: mean={ranking_data['tfidf_similarity'].mean():.4f}, \"      f\"std={ranking_data['tfidf_similarity'].std():.4f}\")",
    "    # Note: This loop processes each pair individually for clarity.",
    "    # For production use with large datasets, consider batch processing or caching."
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Features for LambdaMART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare feature matrix\n",
    "feature_columns = [\n",
    "    'tfidf_similarity',\n",
    "    'num_ratings', 'avg_rating', 'std_rating', 'num_users', 'popularity_score',\n",
    "    'user_num_ratings', 'user_avg_rating', 'user_std_rating', 'user_num_movies',\n",
    "    'age'\n",
    "] + genre_cols\n",
    "\n",
    "# Encode categorical features\n",
    "ranking_data['gender_encoded'] = ranking_data['gender'].map({'M': 1, 'F': 0})\n",
    "feature_columns.append('gender_encoded')\n",
    "\n",
    "# Sort by user_id and timestamp for proper query grouping\n",
    "ranking_data = ranking_data.sort_values(['user_id', 'timestamp'])\n",
    "\n",
    "# Create feature matrix\n",
    "X = ranking_data[feature_columns].fillna(0).values\n",
    "y = ranking_data['relevance'].values\n",
    "groups = ranking_data.groupby('user_id').size().values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Number of query groups: {len(groups)}\")\n",
    "print(f\"\\nFeature columns ({len(feature_columns)}):\")\n",
    "print(feature_columns)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def ndcg_at_k(y_true, y_pred, k=10):\n",
    "    \"\"\"Compute NDCG@K\"\"\"\n",
    "    # Sort by predicted scores\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = y_true[order][:k]\n",
    "    \n",
    "    # DCG\n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(len(y_true_sorted)) + 2)\n",
    "    dcg = np.sum(gains / discounts)\n",
    "    \n",
    "    # IDCG\n",
    "    ideal_order = np.argsort(y_true)[::-1][:k]\n",
    "    ideal_gains = 2 ** y_true[ideal_order] - 1\n",
    "    idcg = np.sum(ideal_gains / discounts[:len(ideal_gains)])\n",
    "    \n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dcg / idcg\n",
    "\n",
    "def average_precision(y_true, y_pred):\n",
    "    \"\"\"Compute Average Precision\"\"\"\n",
    "    # Sort by predicted scores\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = y_true[order]\n",
    "    \n",
    "    # Consider items with relevance > 0 as relevant\n",
    "    relevant = (y_true_sorted > 0).astype(int)\n",
    "    \n",
    "    if relevant.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precisions = []\n",
    "    num_relevant = 0\n",
    "    \n",
    "    for i, rel in enumerate(relevant):\n",
    "        if rel == 1:\n",
    "            num_relevant += 1\n",
    "            precisions.append(num_relevant / (i + 1))\n",
    "    \n",
    "    return np.mean(precisions) if precisions else 0.0\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=10):\n",
    "    \"\"\"Compute Precision@K\"\"\"\n",
    "    # Sort by predicted scores\n",
    "    order = np.argsort(y_pred)[::-1][:k]\n",
    "    y_true_sorted = y_true[order]\n",
    "    \n",
    "    # Consider items with relevance > 0 as relevant\n",
    "    relevant = (y_true_sorted > 0).astype(int)\n",
    "    \n",
    "    return relevant.sum() / k\n",
    "\n",
    "def evaluate_ranking(y_true_groups, y_pred_groups, k=10):\n",
    "    \"\"\"Evaluate ranking metrics for multiple queries\"\"\"\n",
    "    ndcg_scores = []\n",
    "    map_scores = []\n",
    "    precision_scores = []\n",
    "    \n",
    "    for y_true, y_pred in zip(y_true_groups, y_pred_groups):\n",
    "        if len(y_true) > 0:\n",
    "            ndcg_scores.append(ndcg_at_k(y_true, y_pred, k))\n",
    "            map_scores.append(average_precision(y_true, y_pred))\n",
    "            precision_scores.append(precision_at_k(y_true, y_pred, k))\n",
    "    \n",
    "    return {\n",
    "        f'NDCG@{k}': np.mean(ndcg_scores),\n",
    "        'MAP': np.mean(map_scores),\n",
    "        f'Precision@{k}': np.mean(precision_scores)\n",
    "    }\n",
    "\n",
    "print(\"Evaluation metrics implemented!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BM25 Baseline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class BM25Ranker:",
    "    \"\"\"BM25 baseline for movie ranking\"\"\"",
    "    ",
    "    def __init__(self, movies_df):",
    "        self.movies_df = movies_df",
    "        # Tokenize documents",
    "        corpus = [doc.split() for doc in movies_df['document'].values]",
    "        self.bm25 = BM25Okapi(corpus)",
    "        self.movie_id_to_idx = {mid: idx for idx, mid in enumerate(movies_df['movie_id'].values)}",
    "    ",
    "    def predict(self, user_id, movie_ids, user_profiles):",
    "        \"\"\"Predict BM25 scores for user-movie pairs\"\"\"",
    "        query = user_profiles.get(user_id, \"\").split()",
    "        ",
    "        if not query:",
    "            return np.zeros(len(movie_ids))",
    "        ",
    "        scores = []",
    "        for movie_id in movie_ids:",
    "            idx = self.movie_id_to_idx.get(movie_id, 0)",
    "            scores.append(self.bm25.get_scores(query)[idx])",
    "        ",
    "        return np.array(scores)",
    "",
    "def create_user_profiles_bm25(ranking_data, movies_df):",
    "    \"\"\"Create user profiles for BM25\"\"\"",
    "    user_profiles = {}",
    "    for user_id in ranking_data['user_id'].unique():",
    "        user_movies = ranking_data[ranking_data['user_id'] == user_id]",
    "        liked_movies = user_movies[user_movies['rating'] >= LIKED_MOVIE_THRESHOLD]['movie_id'].values",
    "        if len(liked_movies) > 0:",
    "            liked_docs = movies_df[movies_df['movie_id'].isin(liked_movies)]['document'].values",
    "            user_profiles[user_id] = ' '.join(liked_docs)",
    "        else:",
    "            user_profiles[user_id] = \"\"",
    "    return user_profiles",
    "",
    "print(\"BM25 baseline implemented!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LambdaMART Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare for cross-validation\n",
    "n_splits = 5\n",
    "group_kfold = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "# Store results\n",
    "cv_results = {\n",
    "    'lambdamart': {'NDCG@10': [], 'MAP': [], 'Precision@10': []},\n",
    "    'bm25': {'NDCG@10': [], 'MAP': [], 'Precision@10': []}\n",
    "}\n",
    "\n",
    "user_ids = ranking_data['user_id'].values\n",
    "\n",
    "print(f\"Starting {n_splits}-fold cross-validation...\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cross-validation loop\n",
    "fold = 1\n",
    "for train_idx, test_idx in group_kfold.split(X, y, groups=user_ids):\n",
    "    print(f\"Fold {fold}/{n_splits}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Get train and test groups\n",
    "    train_data = ranking_data.iloc[train_idx]\n",
    "    test_data = ranking_data.iloc[test_idx]\n",
    "    \n",
    "    train_groups = train_data.groupby('user_id').size().values\n",
    "    test_groups = test_data.groupby('user_id').size().values\n",
    "    \n",
    "    # Train LambdaMART with LightGBM\n",
    "    print(\"Training LambdaMART...\")\n",
    "    train_dataset = lgb.Dataset(X_train, label=y_train, group=train_groups)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [10],\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': 6,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(params, train_dataset, num_boost_round=100)\n",
    "    \n",
    "    # Predict with LambdaMART\n",
    "    y_pred_lambdamart = model.predict(X_test)\n",
    "    \n",
    "    # Split predictions by query group\n",
    "    y_true_groups = []\n",
    "    y_pred_lambdamart_groups = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    for group_size in test_groups:\n",
    "        end_idx = start_idx + group_size\n",
    "        y_true_groups.append(y_test[start_idx:end_idx])\n",
    "        y_pred_lambdamart_groups.append(y_pred_lambdamart[start_idx:end_idx])\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Evaluate LambdaMART\n",
    "    lambdamart_metrics = evaluate_ranking(y_true_groups, y_pred_lambdamart_groups, k=10)\n",
    "    print(f\"LambdaMART - NDCG@10: {lambdamart_metrics['NDCG@10']:.4f}, \"\n",
    "          f\"MAP: {lambdamart_metrics['MAP']:.4f}, \"\n",
    "          f\"Precision@10: {lambdamart_metrics['Precision@10']:.4f}\")\n",
    "    \n",
    "    # Train and evaluate BM25 baseline\n",
    "    print(\"Evaluating BM25 baseline...\")\n",
    "    bm25_ranker = BM25Ranker(movies_df)\n",
    "    user_profiles = create_user_profiles_bm25(train_data, movies_df)\n",
    "    \n",
    "    y_pred_bm25_groups = []\n",
    "    for user_id in test_data['user_id'].unique():\n",
    "        user_test = test_data[test_data['user_id'] == user_id]\n",
    "        movie_ids = user_test['movie_id'].values\n",
    "        scores = bm25_ranker.predict(user_id, movie_ids, user_profiles)\n",
    "        y_pred_bm25_groups.append(scores)\n",
    "    \n",
    "    # Evaluate BM25\n",
    "    bm25_metrics = evaluate_ranking(y_true_groups, y_pred_bm25_groups, k=10)\n",
    "    print(f\"BM25 - NDCG@10: {bm25_metrics['NDCG@10']:.4f}, \"\n",
    "          f\"MAP: {bm25_metrics['MAP']:.4f}, \"\n",
    "          f\"Precision@10: {bm25_metrics['Precision@10']:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    for metric in ['NDCG@10', 'MAP', 'Precision@10']:\n",
    "        cv_results['lambdamart'][metric].append(lambdamart_metrics[metric])\n",
    "        cv_results['bm25'][metric].append(bm25_metrics[metric])\n",
    "    \n",
    "    print()\n",
    "    fold += 1\n",
    "\n",
    "print(\"Cross-validation completed!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate mean and std for each metric\n",
    "print(\"=\"*60)\n",
    "print(\"CROSS-VALIDATION RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_summary = []\n",
    "for model in ['lambdamart', 'bm25']:\n",
    "    print(f\"\\n{model.upper()}:\")\n",
    "    print(\"-\"*40)\n",
    "    for metric in ['NDCG@10', 'MAP', 'Precision@10']:\n",
    "        values = cv_results[model][metric]\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric}: {mean_val:.4f} \u00b1 {std_val:.4f}\")\n",
    "        results_summary.append({\n",
    "            'Model': model.upper(),\n",
    "            'Metric': metric,\n",
    "            'Mean': mean_val,\n",
    "            'Std': std_val\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(\"\\n\" + \"=\"*60)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comparison charts\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "metrics = ['NDCG@10', 'MAP', 'Precision@10']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    lambdamart_vals = cv_results['lambdamart'][metric]\n",
    "    bm25_vals = cv_results['bm25'][metric]\n",
    "    \n",
    "    x = np.arange(len(lambdamart_vals))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, lambdamart_vals, width, label='LambdaMART', alpha=0.8, color='steelblue')\n",
    "    ax.bar(x + width/2, bm25_vals, width, label='BM25', alpha=0.8, color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Fold', fontsize=12)\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'Fold {i+1}' for i in range(len(lambdamart_vals))])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metric_comparison_by_fold.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as 'metric_comparison_by_fold.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create mean comparison chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "metrics = ['NDCG@10', 'MAP', 'Precision@10']\n",
    "lambdamart_means = [np.mean(cv_results['lambdamart'][m]) for m in metrics]\n",
    "lambdamart_stds = [np.std(cv_results['lambdamart'][m]) for m in metrics]\n",
    "bm25_means = [np.mean(cv_results['bm25'][m]) for m in metrics]\n",
    "bm25_stds = [np.std(cv_results['bm25'][m]) for m in metrics]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, lambdamart_means, width, yerr=lambdamart_stds, \n",
    "       label='LambdaMART', alpha=0.8, color='steelblue', capsize=5)\n",
    "ax.bar(x + width/2, bm25_means, width, yerr=bm25_stds,\n",
    "       label='BM25 Baseline', alpha=0.8, color='coral', capsize=5)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Average Performance Comparison: LambdaMART vs BM25', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics, fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('average_metric_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Chart saved as 'average_metric_comparison.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Train final model on all data to get feature importance\n",
    "final_dataset = lgb.Dataset(X, label=y, group=groups)\n",
    "final_model = lgb.train(params, final_dataset, num_boost_round=100)\n",
    "\n",
    "# Get feature importance\n",
    "importance = final_model.feature_importance(importance_type='gain')\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], alpha=0.8, color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance (Gain)', fontsize=12)\n",
    "plt.title('Top 15 Feature Importance - LambdaMART', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nChart saved as 'feature_importance.png'\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. DATASET:\")\n",
    "print(f\"   - Total interactions: {len(ranking_data):,}\")\n",
    "print(f\"   - Unique users (queries): {ranking_data['user_id'].nunique():,}\")\n",
    "print(f\"   - Unique movies (documents): {ranking_data['movie_id'].nunique():,}\")\n",
    "\n",
    "print(\"\\n2. FEATURES:\")\n",
    "print(f\"   - Total features: {len(feature_columns)}\")\n",
    "print(\"   - TF-IDF similarity: User profile vs movie content\")\n",
    "print(\"   - Popularity: Rating count, average, std, user count\")\n",
    "print(\"   - Engagement: User activity, demographics\")\n",
    "print(\"   - Genre features: 18 binary genre indicators\")\n",
    "\n",
    "print(\"\\n3. MODEL:\")\n",
    "print(\"   - Algorithm: LambdaMART (via LightGBM)\")\n",
    "print(\"   - Learning objective: Pairwise preference learning (lambdarank)\")\n",
    "print(f\"   - Cross-validation: {n_splits}-fold GroupKFold\")\n",
    "\n",
    "print(\"\\n4. PERFORMANCE IMPROVEMENT:\")\n",
    "for metric in ['NDCG@10', 'MAP', 'Precision@10']:\n",
    "    lambdamart_mean = np.mean(cv_results['lambdamart'][metric])\n",
    "    bm25_mean = np.mean(cv_results['bm25'][metric])\n",
    "    improvement = ((lambdamart_mean - bm25_mean) / bm25_mean) * 100\n",
    "    print(f\"   - {metric}: {improvement:+.2f}% improvement over BM25\")\n",
    "\n",
    "print(\"\\n5. KEY FINDINGS:\")\n",
    "top_3_features = feature_importance_df.head(3)['feature'].tolist()\n",
    "print(f\"   - Top 3 features: {', '.join(top_3_features)}\")\n",
    "print(\"   - LambdaMART successfully learns to rank using pairwise preferences\")\n",
    "print(\"   - Machine learning approach outperforms traditional BM25 baseline\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Learning-to-Rank implementation completed successfully!\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}