Metadata-Version: 2.4
Name: dhenara-ai
Version: 1.1.2
Summary: Dhenara Package for Multi Provider AI-Model API calls
Project-URL: Homepage, https://dhenara.com
Project-URL: Documentation, https://docs.dhenara.com/
Project-URL: Bug Reports, https://github.com/dhenara/dhenara-ai/issues
Project-URL: Source Code, https://github.com/dhenara/dhenara-ai
Author-email: Dhenara Inc <support@dhenara.com>
License-Expression: MIT
Keywords: ai,language models,llm,machine learning
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: <3.14,>=3.13
Requires-Dist: aiohttp>=3.11.0
Requires-Dist: anthropic>=0.69.0
Requires-Dist: asgiref>=3.8.0
Requires-Dist: azure-ai-inference>=1.0.0b9
Requires-Dist: boto3>=1.40.51
Requires-Dist: botocore>=1.40.51
Requires-Dist: cryptography>=44.0.0
Requires-Dist: google-genai>=1.43.0
Requires-Dist: httpx>=0.28.0
Requires-Dist: openai>=2.3.0
Requires-Dist: pillow>=11.1.0
Requires-Dist: pydantic>=2.10.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: requests>=2.32.1
Provides-Extra: dev
Requires-Dist: add-trailing-comma; extra == 'dev'
Requires-Dist: black; extra == 'dev'
Requires-Dist: pytest-asyncio; extra == 'dev'
Requires-Dist: pytest-cov; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Requires-Dist: ruff; extra == 'dev'
Description-Content-Type: text/markdown

# Dhenara

Dhenara is a genuinely open source Python package for interacting with various AI models in a unified way. It is a lightweight, straightforward framework for integrating multiple AI models into Python applications. It's similar in spirit to LangChain but with a focus on simplicity and minimal dependencies along with type safety using Pydantic Models.

For full documentation, visit [docs.dhenara.com](https://docs.dhenara.com/).

## Why Dhenara?

- **Genuinely Open Source**: Built from the ground up as a community resource, not an afterthought or internal tool
- **Unified API**: Interact with different AI providers through a consistent interface
- **Type Safety**: Built with Pydantic for robust type checking and validation
- **Easy Regeneration across Providers**: With a unified Pydantic output and built-in prompt formatting, send output from a model to any other model easily
- **Streaming**: First-class support for streaming responses along with accumulated responses similar to non-streaming responses
- **Async Support**: Both synchronous and asynchronous interfaces for maximum flexibility
- **Resource Management**: Automatic handling of connections, retries, and timeouts
- **Foundation Models**: Pre-configured models with sensible defaults
- **Test Mode**: Bring up your app with dummy responses for streaming and non-streaming generation
- **Cost/Usage Data**: Derived cost and usage data along with responses, with optional charge for each model endpoint for commercial deployment
- **Community-Oriented Design**: An architecture separating API credentials, models, and configurations for flexible deployment and scaling

## Example Usage

Here's a simple example of using Dhenara to interact with an AI model. You can find more examples in [docs.dhenara.com](https://docs.dhenara.com/).

```python
from dhenara.ai import AIModelClient
from dhenara.ai.types import AIModelCallConfig, AIModelEndpoint
from dhenara.ai.types.external_api import AIModelAPIProviderEnum
from dhenara.ai.types.genai import AIModelAPI
from dhenara.ai.types.genai.foundation_models.anthropic.chat import Claude37Sonnet

# Create an API
api = AIModelAPI(
    provider=AIModelAPIProviderEnum.ANTHROPIC,
    api_key="your_api_key",
)

# Create an endpoint using a pre-configured model
model_endpoint = AIModelEndpoint(
    api=api,
    ai_model=Claude37Sonnet,
)

# Configure the api call
config = AIModelCallConfig(
    max_output_tokens=16000,
    reasoning=True,  # Thinking/reasoning mode
    max_reasoning_tokens=8000,
    streaming=False,
)

# Create the client
client = AIModelClient(
    model_endpoint=model_endpoint,
    config=config,
    is_async=False,
)

# Create a prompt
prompt = {
    "role": "user",
    "content": "Explain quantum computing in simple terms",
}

# Generate a response
response = client.generate(prompt=prompt)

# If not streaming
if response.chat_response:
    print(response.chat_response.choices[0].contents[0].get_text())

# If streaming
elif response.stream_generator:
    for chunk, _ in response.stream_generator:
        if chunk:
            print(
                chunk.data.choice_deltas[0].content_deltas[0].get_text_delta(),
                end="",
                flush=True,
            )
```

## Documentation

For full documentation, visit [docs.dhenara.com](https://docs.dhenara.com/).