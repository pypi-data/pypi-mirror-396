[build-system]
requires = ["setuptools>=61.0,<70", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "mock-spark"
version = "3.13.0"
description = "Lightning-fast PySpark testing without JVM - 10x faster with 100% API compatibility"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Odos Matthews", email = "odosmatthews@gmail.com"}
]
maintainers = [
    {name = "Odos Matthews", email = "odosmatthews@gmail.com"}
]
keywords = [
    "spark", "pyspark", "mock", "testing", "development", "data-engineering",
    "dataframe", "spark-session", "unit-testing", "type-safe", "mypy",
    "error-simulation", "performance-testing", "data-generation", "enterprise"
]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Intended Audience :: Developers",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Topic :: Software Development :: Testing",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Scientific/Engineering :: Information Analysis",
]
requires-python = ">=3.9"
dependencies = [
    "spark-ddl-parser>=0.1.0",
    "polars>=0.20.0",
    "psutil>=5.8.0",
]

[project.optional-dependencies]
pandas = [
    "pandas>=1.3.0",
    "pandas-stubs>=2.0.0",
]
analytics = [
    "pandas>=1.3.0",
    "pandas-stubs>=2.0.0",
    "numpy>=1.20.0",
    "polars[pyarrow]>=0.20.0",
]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "pytest-xdist>=3.0.0",  # Parallel test execution
    "mypy>=1.19.0",
    "ruff>=0.4.0",
    "pandas>=1.3.0",
    "pandas-stubs>=2.0.0",
    "types-psutil>=6.0.0",
]
test = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "pytest-xdist>=3.0.0",  # Parallel test execution
    "hypothesis>=6.0.0",
]
generate-outputs = [
    "pyspark>=3.2.0,<3.6.0",  # PySpark for generating expected outputs
    "delta-spark>=2.0.0,<2.2.0",  # Delta Lake for generating expected outputs
]

[project.urls]
Homepage = "https://github.com/eddiethedean/mock-spark"
Repository = "https://github.com/eddiethedean/mock-spark"
Issues = "https://github.com/eddiethedean/mock-spark/issues"

[tool.setuptools.packages.find]
where = ["."]
include = ["mock_spark*"]

[tool.setuptools.package-data]
mock_spark = ["py.typed"]

[tool.black]
line-length = 100
target-version = ['py39']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
line_length = 100
multi_line_output = 3
include_trailing_comma = true
force_grid_wrap = 0
use_parentheses = true
ensure_newline_before_comments = true

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
show_error_codes = true
exclude = '(^venv/|^\.venv/|^venv_exploration_py39/)'
files = ["mock_spark", "tests"]

[[tool.mypy.overrides]]
module = [
    "pandas.*",
    "psutil.*",

]
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = [
    "mock_spark.testing.*",
]
ignore_errors = true

[[tool.mypy.overrides]]
module = [
    "mock_spark.dataframe.grouped_data",
]
warn_unreachable = false

[[tool.mypy.overrides]]
module = [
    "mock_spark.dataframe.transformations.operations",
    "mock_spark.dataframe.aggregations.operations",
]
ignore_errors = true

[[tool.mypy.overrides]]
module = [
    "mock_spark.functions.conditional",
    "mock_spark.functions.datetime",
    "mock_spark.dataframe.grouped.base",
    "mock_spark.backend.polars.export",
    "mock_spark.functions.functions",
    "mock_spark.dataframe.operations.set_operations",
    "mock_spark.core.schema_inference",
    "mock_spark.core.data_validation",
    "mock_spark.functions.window_execution",
    "mock_spark.dataframe.window_handler",
    "mock_spark.spark_types",
    "mock_spark.optimizer.query_optimizer",
    "mock_spark.dataframe.writer",
]
warn_unreachable = false

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-v",
    "--tb=short",
    "--strict-markers",
    "--disable-warnings",
    "--cov=mock_spark",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-report=xml",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "compatibility: marks tests as compatibility tests with real PySpark",
    "fast: marks tests as fast unit tests without PySpark",
    "delta: marks tests that require Delta Lake JARs (run serially for isolation)",
    "performance: marks tests that measure performance (run serially for stable timing)",
]

[tool.ruff]
target-version = "py39"
exclude = [
    "venv",
    ".venv",
    ".venv-test",
    "venv-test",
    "venv_exploration_py39",
]

[tool.ruff.lint]
extend-select = ["C4", "SIM", "TCH", "UP"]