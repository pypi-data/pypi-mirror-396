# ğŸš€ UltraRAG - Complete Unified Package

**The ONLY RAG package you need!**

âœ… Revolutionary RAG engine  
âœ… Built-in Ollama integration  
âœ… Built-in FastAPI + Swagger UI  
âœ… **ONE command to start!**  
âœ… Zero configuration needed!

---

## ğŸ¯ Quick Start (2 Commands!)

### Step 1: Install

```bash
pip install ultrarag[server]
```

### Step 2: Start Server

```bash
ultrarag serve --ollama-host localhost --ollama-model llama3.2
```

**Open Swagger UI:**
```
http://localhost:8000/docs
```

ğŸ‰ **DONE! RAG chatbot ready!**

---

## ğŸ“š Three Ways to Use

### Method 1: Web Server (with Swagger UI)

```bash
# Start server
ultrarag serve --ollama-host localhost --ollama-model llama3.2

# Open browser â†’ http://localhost:8000/docs
# Upload documents, ask questions via Swagger!
```

### Method 2: Python Code (Simple)

```python
from ultrarag import RAG

# Create RAG
rag = RAG()

# Add document
rag.add("Python is a programming language created by Guido van Rossum.")

# Ask question
answer = rag.ask("Who created Python?")
print(answer)
# Output: "Python is a programming language created by Guido van Rossum."
```

### Method 3: Python Code (with Ollama)

```python
from ultrarag import RAG, OllamaLLM

# Initialize Ollama
llm = OllamaLLM(host="localhost", port=11434, model="llama3.2")

# Create RAG
rag = RAG()

# Add document
rag.add("Python is used for AI, web development, and data science.")

# Get context
query = "What is Python used for?"
query_analysis = rag.query_processor.analyze(query)
chunks = rag.retriever.retrieve(rag.chunks, query_analysis, top_k=3)
context = " ".join([c.text for c in chunks])

# Generate with LLM
prompt = f"Based on: {context}\n\nQuestion: {query}\n\nAnswer:"
answer = llm.generate(prompt)
print(answer)
```

---

## ğŸ¯ Complete Example (CLI Server)

### Prerequisites

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama
ollama serve

# Pull model (in another terminal)
ollama pull llama3.2
```

### Install UltraRAG

```bash
pip install ultrarag[server]
```

### Start Server

```bash
ultrarag serve --ollama-host localhost --ollama-model llama3.2
```

**Output:**
```
ğŸš€ Starting UltraRAG Server...
ğŸ“¡ Ollama: localhost:11434
ğŸ¤– Model: llama3.2
ğŸ“š Swagger UI: http://localhost:8000/docs
```

### Use Swagger UI

1. **Open:** `http://localhost:8000/docs`

2. **Upload document:**
   - Click `POST /upload`
   - Choose file
   - Execute

3. **Ask question:**
   - Click `POST /query`
   - Enter:
     ```json
     {
       "question": "Your question?",
       "use_llm": true
     }
     ```
   - Execute

4. **Get answer!** âœ…

---

## ğŸ”§ Configuration Options

### Server Command

```bash
ultrarag serve \
  --ollama-host localhost \      # Ollama IP
  --ollama-port 11434 \          # Ollama port
  --ollama-model llama3.2 \      # Model name
  --port 8000                     # Server port
```

### Different Ollama Machine

```bash
ultrarag serve --ollama-host 192.168.1.100 --ollama-model llama3.2
```

---

## ğŸ“Š API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/docs` | GET | Swagger UI |
| `/upload` | POST | Upload file |
| `/upload-text` | POST | Upload text |
| `/query` | POST | Ask question |
| `/stats` | GET | Statistics |
| `/clear` | DELETE | Clear documents |

---

## ğŸ’» Python API

### Basic Usage

```python
from ultrarag import RAG

rag = RAG()
rag.add("document text...")
answer = rag.ask("question?")
```

### With Ollama

```python
from ultrarag import RAG, OllamaLLM

llm = OllamaLLM(host="localhost", model="llama3.2")
rag = RAG()

# Test Ollama
if llm.test():
    print("âœ… Ollama connected")
else:
    print("âŒ Ollama not available")

# Add documents
rag.add("Your documents...")

# Generate answer
chunks = rag.retriever.retrieve(rag.chunks, query_analysis, top_k=3)
context = " ".join([c.text for c in chunks])
answer = llm.generate(f"Context: {context}\n\nQuestion: {question}")
```

### Advanced Usage

```python
# Custom configuration
rag = RAG(
    min_chunk_completeness=0.85,
    min_grounding_score=0.85
)

# Add with metadata
rag.add("text...", metadata={"source": "doc1.pdf"})

# Detailed response
response = rag.ask("question?", explain=True)
print(f"Answer: {response.answer}")
print(f"Confidence: {response.confidence}")
print(f"Grounding: {response.grounding_score}")
print(f"Verdict: {response.metadata['verdict']}")

# Statistics
stats = rag.get_stats()
print(f"Total chunks: {stats['total_chunks']}")
```

---

## ğŸ¯ Installation Options

### Minimal (RAG only)

```bash
pip install ultrarag
```

**Use in Python code only (no web server)**

### Full (RAG + Web Server)

```bash
pip install ultrarag[server]
```

**Includes FastAPI + Swagger UI**

### From Source

```bash
git clone https://github.com/kumar123ips/ultrarag
cd ultrarag
pip install -e .[server]
```

---

## ğŸ”¥ Features

### Revolutionary RAG Components

âœ… **AtomicChunk** - Guaranteed completeness (ICS â‰¥ 0.75)  
âœ… **QueryDNA** - Multi-dimensional query analysis  
âœ… **AdaptiveRetriever** - Intent-based retrieval  
âœ… **ProvenAnswer** - Mathematical validation  
âœ… **Zero Dependencies** - Core package is pure Python!

### Built-in Integrations

âœ… **Ollama** - Local LLM support  
âœ… **FastAPI** - Production web server  
âœ… **Swagger UI** - Interactive API docs  
âœ… **CLI** - One command to start!

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE)

---

## ğŸ‘¤ Author

**Abhishek Kumar**  
Email: ipsabhi420@gmail.com  
GitHub: [@kumar123ips](https://github.com/kumar123ips)

---

## ğŸ‰ Success!

**The ONLY RAG package you need!**

```bash
pip install ultrarag[server]
ultrarag serve --ollama-model llama3.2
```

**That's it!** ğŸš€

---

**Made with â¤ï¸ by Abhishek Kumar**
