model_path: meta-llama/Llama-3.1-70B-Instruct
tp_size: 4
dp_size: 1
ep_size: 1
nodes_per_worker: 1
workers: 1
num_gpus_per_worker: 4
cuda_graph_max_bs: 256
grammar_backend: llguidance
reasoning_parser: ""
use_router: false
router_policy: cache_aware
description: "Llama 3.1 70B - fits on single node with TP=4"
