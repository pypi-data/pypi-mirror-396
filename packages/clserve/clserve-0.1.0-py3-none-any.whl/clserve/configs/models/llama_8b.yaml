model_path: meta-llama/Llama-3.1-8B-Instruct
tp_size: 1
dp_size: 4
ep_size: 1
nodes_per_worker: 1
workers: 1
num_gpus_per_worker: 1
cuda_graph_max_bs: 256
grammar_backend: llguidance
reasoning_parser: ""
use_router: true
router_policy: cache_aware
description: "Llama 3.1 8B - small model, 4 instances per node with router"
