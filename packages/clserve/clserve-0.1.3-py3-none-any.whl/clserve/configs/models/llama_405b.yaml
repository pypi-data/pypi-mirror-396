model_path: meta-llama/Llama-3.1-405B-Instruct
tp_size: 16
dp_size: 1
ep_size: 1
nodes_per_worker: 4
workers: 1
num_gpus_per_worker: 4
cuda_graph_max_bs: 256
grammar_backend: llguidance
reasoning_parser: ""
use_router: false
router_policy: cache_aware
description: "Llama 3.1 405B - requires 4 nodes per worker with TP=16"
