"""
Generate robots.txt with configurable AI bot policies.

This module provides functionality to generate robots.txt files with
fine-grained control over different types of bots (training, search, traditional).
"""

from typing import List, Optional


class RobotsConfig:
    """Configuration for robots.txt generation."""

    def __init__(
        self,
        block_ai_training: bool = True,
        allow_ai_search: bool = True,
        allow_traditional: bool = True,
        crawl_delay: Optional[int] = None,
        custom_rules: Optional[List[str]] = None,
        disallowed_paths: Optional[List[str]] = None,
    ):
        """
        Initialize robots.txt configuration.

        Args:
            block_ai_training: Block AI training data collection bots
            allow_ai_search: Allow AI search and citation bots
            allow_traditional: Allow traditional search engine bots
            crawl_delay: Crawl delay in seconds (rate limiting)
            custom_rules: Additional custom rules to include
            disallowed_paths: Paths to disallow for all bots
        """
        self.block_ai_training = block_ai_training
        self.allow_ai_search = allow_ai_search
        self.allow_traditional = allow_traditional
        self.crawl_delay = crawl_delay
        self.custom_rules = custom_rules or []
        self.disallowed_paths = disallowed_paths or []


def generate_robots_txt(
    config: RobotsConfig, sitemap_url: str, base_url: str
) -> str:
    """
    Generate robots.txt content based on configuration.

    Args:
        config: Robots.txt configuration
        sitemap_url: Full URL to sitemap.xml
        base_url: Base URL of the application

    Returns:
        Complete robots.txt content
    """

    lines = [
        "# Robots.txt for Dash Application",
        "# Generated by dash-improve-my-llms",
        "# https://pip-install-python.com",
        "",
        "# Default policy - allow all standard crawlers",
        "User-agent: *",
        "Allow: /",
        "",
    ]

    # Add disallowed paths for all bots
    if config.disallowed_paths:
        for path in config.disallowed_paths:
            lines.append(f"Disallow: {path}")
        lines.append("")

    # Add crawl delay if specified
    if config.crawl_delay:
        lines.extend([f"Crawl-delay: {config.crawl_delay}", ""])

    # Block AI training bots if configured
    if config.block_ai_training:
        lines.extend(
            [
                "# ==========================================",
                "# Block AI Training Data Collection",
                "# ==========================================",
                "# These bots collect data to train AI models.",
                "# Blocking them prevents your content from being",
                "# used in training datasets without permission.",
                "",
                "User-agent: GPTBot",
                "Disallow: /",
                "",
                "User-agent: anthropic-ai",
                "Disallow: /",
                "",
                "User-agent: Claude-Web",
                "Disallow: /",
                "",
                "User-agent: CCBot",
                "Disallow: /",
                "",
                "User-agent: Google-Extended",
                "Disallow: /",
                "",
                "User-agent: FacebookBot",
                "Disallow: /",
                "",
                "User-agent: Omgilibot",
                "Disallow: /",
                "",
                "User-agent: Omgili",
                "Disallow: /",
                "",
                "User-agent: ByteSpider",
                "Disallow: /",
                "",
            ]
        )

    # Allow AI search/citation bots if configured
    if config.allow_ai_search:
        lines.extend(
            [
                "# ==========================================",
                "# Allow AI Search and Citation Bots",
                "# ==========================================",
                "# These bots help users find your content through",
                "# AI-powered search engines and assistants.",
                "",
                "User-agent: ChatGPT-User",
                "Allow: /",
                "",
                "User-agent: ClaudeBot",
                "Allow: /",
                "",
                "User-agent: PerplexityBot",
                "Allow: /",
                "",
                "User-agent: OAI-SearchBot",
                "Disallow: /",
                "",
            ]
        )

    # Allow traditional search bots (usually always allowed)
    if config.allow_traditional:
        lines.extend(
            [
                "# ==========================================",
                "# Traditional Search Engines",
                "# ==========================================",
                "# Standard search engine bots are allowed",
                "# by default with the User-agent: * rule above.",
                "",
                "# Googlebot, Bingbot, etc. - covered by *",
                "",
            ]
        )

    # Add custom rules
    if config.custom_rules:
        lines.extend(
            [
                "# ==========================================",
                "# Custom Rules",
                "# ==========================================",
                "",
                *config.custom_rules,
                "",
            ]
        )

    # Always include sitemap reference
    lines.extend(
        [
            "# ==========================================",
            "# Sitemaps and Discovery",
            "# ==========================================",
            f"Sitemap: {sitemap_url}",
            "",
        ]
    )

    # Add helpful links for AI agents
    lines.extend(
        [
            "# ==========================================",
            "# AI-Friendly Documentation",
            "# ==========================================",
            "# For AI agents and LLMs, we provide structured",
            "# documentation in multiple formats:",
            "#",
            f"# {base_url}/llms.txt - LLM-friendly content structure",
            f"# {base_url}/architecture.txt - Application architecture",
            f"# {base_url}/page.json - Page metadata (JSON)",
            f"# {base_url}/sitemap.xml - Complete sitemap",
            "",
        ]
    )

    return "\n".join(lines)