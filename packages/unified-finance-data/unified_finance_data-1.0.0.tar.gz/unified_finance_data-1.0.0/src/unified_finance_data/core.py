#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Finance Data æ ¸å¿ƒæ¨¡å—

æä¾›ç»Ÿä¸€çš„æ•°æ®è·å–æ¥å£ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ•°æ®æºã€‚

éµå¾ªGolangè®¾è®¡å“²å­¦ï¼š
- ç®€å•æ€§ä¼˜äºå¤æ‚æ€§
- æ¥å£åº”è¯¥å°è€Œä¸“æ³¨
- ç»„åˆä¼˜äºç»§æ‰¿
- æ˜¾å¼ä¼˜äºéšå¼
- å‡å°‘æŠ½è±¡å±‚æ¬¡
"""

import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
import warnings
from typing import Optional, List, Dict, Any

from .exceptions import (
    UnifiedFinanceDataError,
    DataSourceUnavailableError,
    InvalidParameterError,
    DataFetchError,
    DataQualityError,
    NetworkError,
    ParsingError,
)

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)


class FuquanType:
    """å¤æƒç±»å‹å¸¸é‡"""
    NONE = 0      # ä¸å¤æƒ
    FRONT = 1     # å‰å¤æƒ
    BACK = 2      # åå¤æƒ

    @classmethod
    def get_name(cls, fqt: int) -> str:
        """è·å–å¤æƒç±»å‹çš„ä¸­æ–‡åç§°"""
        mapping = {
            cls.NONE: "ä¸å¤æƒ",
            cls.FRONT: "å‰å¤æƒ",
            cls.BACK: "åå¤æƒ"
        }
        return mapping.get(fqt, "æœªçŸ¥")

    @classmethod
    def validate(cls, fqt: int) -> bool:
        """éªŒè¯å¤æƒç±»å‹æ˜¯å¦æœ‰æ•ˆ"""
        return fqt in [cls.NONE, cls.FRONT, cls.BACK]


class DataSourceManager:
    """æ•°æ®æºç®¡ç†å™¨ - ç®€å•ã€ä¸“æ³¨çš„è®¾è®¡"""

    def __init__(self, debug: bool = False):
        self.sources = []
        self.debug = debug
        self._init_sources()

    def _init_sources(self):
        """æŒ‰ç…§æ•°æ®è´¨é‡ä¼˜å…ˆçº§åˆå§‹åŒ–æ•°æ®æº"""
        # å¯¼å…¥æ•°æ®æºæ¨¡å—
        self._ths_crawler = None
        self._baidu_playwright_get_data = None
        self._baidu_api_get_data = None
        self._sina_get_data = None

        # 1. åŒèŠ±é¡º (æ•°æ®è´¨é‡æœ€é«˜)
        try:
            from .crawlers.ths_kline import ThsKLineCrawler
            self._ths_crawler = ThsKLineCrawler()
            self.sources.append({
                'name': 'åŒèŠ±é¡º',
                'priority': 1,
                'function': self._get_ths_data,
                'available': True
            })
            if self.debug:
                logger.info("âœ“ åŒèŠ±é¡ºæ•°æ®æºåˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            if self.debug:
                logger.warning(f"âœ— åŒèŠ±é¡ºæ•°æ®æºä¸å¯ç”¨: {e}")

        # 2. ç™¾åº¦è‚¡å¸‚é€š-Playwrightç‰ˆæœ¬ (æ•°æ®æ›´æ–°åŠæ—¶ï¼ŒåŠŸèƒ½å®Œæ•´)
        try:
            from .crawlers.baidu_playwright import get_fund_k_history as baidu_playwright_get_data
            self._baidu_playwright_get_data = baidu_playwright_get_data
            self.sources.append({
                'name': 'ç™¾åº¦Playwright',
                'priority': 2,
                'function': self._get_baidu_playwright_data,
                'available': True
            })
            if self.debug:
                logger.info("âœ“ ç™¾åº¦Playwrightæ•°æ®æºåˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            if self.debug:
                logger.warning(f"âœ— ç™¾åº¦Playwrightæ•°æ®æºä¸å¯ç”¨: {e}")

        # 3. ç™¾åº¦è‚¡å¸‚é€š-APIç‰ˆæœ¬ (é€Ÿåº¦å¿«ï¼Œä½†åŠŸèƒ½æœ‰é™)
        try:
            from .crawlers.baidu_api import get_fund_k_history as baidu_api_get_data
            self._baidu_api_get_data = baidu_api_get_data
            self.sources.append({
                'name': 'ç™¾åº¦API',
                'priority': 3,
                'function': self._get_baidu_api_data,
                'available': True
            })
            if self.debug:
                logger.info("âœ“ ç™¾åº¦APIæ•°æ®æºåˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            if self.debug:
                logger.warning(f"âœ— ç™¾åº¦APIæ•°æ®æºä¸å¯ç”¨: {e}")

        # 4. æ–°æµªè´¢ç» (ç¨³å®šæ€§å¥½)
        try:
            from .crawlers.sina_kline import get_fund_k_history as sina_get_data
            self._sina_get_data = sina_get_data
            self.sources.append({
                'name': 'æ–°æµª',
                'priority': 4,
                'function': self._get_sina_data,
                'available': True
            })
            if self.debug:
                logger.info("âœ“ æ–°æµªæ•°æ®æºåˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            if self.debug:
                logger.warning(f"âœ— æ–°æµªæ•°æ®æºä¸å¯ç”¨: {e}")

        if not self.sources:
            raise DataSourceUnavailableError("æ‰€æœ‰æ•°æ®æºéƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–å®‰è£…")

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.sources.sort(key=lambda x: x['priority'])

    def _get_ths_data(self, fund_code: str, beg: str, end: str, fqt: int, proxy: Optional[str]) -> Optional[pd.DataFrame]:
        """è·å–åŒèŠ±é¡ºæ•°æ®"""
        try:
            if self.debug:
                logger.info(f"ğŸ”„ å°è¯•ä»åŒèŠ±é¡ºè·å– {fund_code} æ•°æ®...")

            # åŒèŠ±é¡ºå‚æ•°æ ¼å¼
            data = self._ths_crawler.get_fund_k_history(
                code=fund_code,
                beg=beg,
                end=end
            )

            if data is not None and not data.empty:
                if self.debug:
                    logger.info(f"âœ… åŒèŠ±é¡ºæ•°æ®è·å–æˆåŠŸ: {len(data)} æ¡è®°å½•")
                return self._standardize_data(data, 'åŒèŠ±é¡º')
            else:
                if self.debug:
                    logger.warning(f"âŒ åŒèŠ±é¡ºè¿”å›ç©ºæ•°æ®")
                return None
        except Exception as e:
            if self.debug:
                logger.error(f"âŒ åŒèŠ±é¡ºæ•°æ®è·å–å¤±è´¥: {e}")
            return None

    def _get_baidu_playwright_data(self, fund_code: str, beg: str, end: str, fqt: int, proxy: Optional[str]) -> Optional[pd.DataFrame]:
        """è·å–ç™¾åº¦Playwrightæ•°æ®"""
        try:
            if self.debug:
                logger.info(f"ğŸ”„ å°è¯•ä»ç™¾åº¦Playwrightè·å– {fund_code} æ•°æ®...")

            # ç™¾åº¦Playwrightå‚æ•°æ ¼å¼
            data = self._baidu_playwright_get_data(
                fund_code=fund_code,
                beg=beg,
                end=end,
                headless=True,
                debug=self.debug
            )

            if data is not None and not data.empty:
                if self.debug:
                    logger.info(f"âœ… ç™¾åº¦Playwrightæ•°æ®è·å–æˆåŠŸ: {len(data)} æ¡è®°å½•")
                return self._standardize_data(data, 'ç™¾åº¦Playwright')
            else:
                if self.debug:
                    logger.warning(f"âŒ ç™¾åº¦Playwrightè¿”å›ç©ºæ•°æ®")
                return None
        except Exception as e:
            if self.debug:
                logger.error(f"âŒ ç™¾åº¦Playwrightæ•°æ®è·å–å¤±è´¥: {e}")
            return None

    def _get_baidu_api_data(self, fund_code: str, beg: str, end: str, fqt: int, proxy: Optional[str]) -> Optional[pd.DataFrame]:
        """è·å–ç™¾åº¦APIæ•°æ®"""
        try:
            if self.debug:
                logger.info(f"ğŸ”„ å°è¯•ä»ç™¾åº¦APIè·å– {fund_code} æ•°æ®...")

            # ç™¾åº¦APIå‚æ•°æ ¼å¼ - æ³¨æ„å‚æ•°ä¸åŒ
            data = self._baidu_api_get_data(
                fund_code=fund_code,
                beg=beg,
                end=end,
                debug=self.debug
            )

            if data is not None and not data.empty:
                if self.debug:
                    logger.info(f"âœ… ç™¾åº¦APIæ•°æ®è·å–æˆåŠŸ: {len(data)} æ¡è®°å½•")
                return self._standardize_data(data, 'ç™¾åº¦API')
            else:
                if self.debug:
                    logger.warning(f"âŒ ç™¾åº¦APIè¿”å›ç©ºæ•°æ®")
                return None
        except Exception as e:
            if self.debug:
                logger.error(f"âŒ ç™¾åº¦APIæ•°æ®è·å–å¤±è´¥: {e}")
            return None

    def _get_sina_data(self, fund_code: str, beg: str, end: str, fqt: int, proxy: Optional[str]) -> Optional[pd.DataFrame]:
        """è·å–æ–°æµªæ•°æ®"""
        try:
            if self.debug:
                logger.info(f"ğŸ”„ å°è¯•ä»æ–°æµªè·å– {fund_code} æ•°æ®...")

            # æ–°æµªå‚æ•°æ ¼å¼
            data = self._sina_get_data(
                fund_code=fund_code,
                beg=beg,
                end=end,
                debug=self.debug
            )

            if data is not None and not data.empty:
                if self.debug:
                    logger.info(f"âœ… æ–°æµªæ•°æ®è·å–æˆåŠŸ: {len(data)} æ¡è®°å½•")
                return self._standardize_data(data, 'æ–°æµª')
            else:
                if self.debug:
                    logger.warning(f"âŒ æ–°æµªè¿”å›ç©ºæ•°æ®")
                return None
        except Exception as e:
            if self.debug:
                logger.error(f"âŒ æ–°æµªæ•°æ®è·å–å¤±è´¥: {e}")
            return None

    def _standardize_data(self, df: pd.DataFrame, source_name: str) -> pd.DataFrame:
        """æ ‡å‡†åŒ–æ•°æ®æ ¼å¼"""
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        required_columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢']

        # åˆ›å»ºæ–°çš„DataFrame
        result_df = pd.DataFrame()

        # å¤åˆ¶åŸå§‹æ•°æ®
        for col in required_columns:
            if col in df.columns:
                result_df[col] = df[col]
            else:
                # ç™¾åº¦APIå’Œæ–°æµªçš„æˆäº¤é¢ä¸ºç©ºæ˜¯æ­£å¸¸çš„
                if source_name not in ['æ–°æµª', 'ç™¾åº¦API'] or col not in ['æˆäº¤é¢']:
                    logger.warning(f"âš ï¸  {source_name}æ•°æ®ç¼ºå°‘åˆ— '{col}'ï¼Œç”¨0å¡«å……")
                result_df[col] = 0

        # æ·»åŠ æ´¾ç”Ÿåˆ—
        if 'æ¶¨è·Œå¹…' not in df.columns:
            result_df['æ¶¨è·Œå¹…'] = ((result_df['æ”¶ç›˜'] - result_df['å¼€ç›˜']) / result_df['å¼€ç›˜'] * 100).round(2)
        else:
            result_df['æ¶¨è·Œå¹…'] = df['æ¶¨è·Œå¹…']

        if 'æ¶¨è·Œé¢' not in df.columns:
            result_df['æ¶¨è·Œé¢'] = (result_df['æ”¶ç›˜'] - result_df['å¼€ç›˜']).round(3)
        else:
            result_df['æ¶¨è·Œé¢'] = df['æ¶¨è·Œé¢']

        if 'æŒ¯å¹…' not in df.columns:
            if len(result_df) > 0 and result_df['å¼€ç›˜'].iloc[0] > 0:
                result_df['æŒ¯å¹…'] = ((result_df['æœ€é«˜'] - result_df['æœ€ä½']) / result_df['å¼€ç›˜'].iloc[0] * 100).round(2)
            else:
                result_df['æŒ¯å¹…'] = 0.0
        else:
            result_df['æŒ¯å¹…'] = df['æŒ¯å¹…']

        if 'æ¢æ‰‹ç‡' not in df.columns:
            result_df['æ¢æ‰‹ç‡'] = 0.0
        else:
            result_df['æ¢æ‰‹ç‡'] = df['æ¢æ‰‹ç‡']

        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
        result_df['æ—¥æœŸ'] = pd.to_datetime(result_df['æ—¥æœŸ']).dt.strftime('%Y-%m-%d')

        # æŒ‰æ—¥æœŸæ’åº
        result_df = result_df.sort_values('æ—¥æœŸ').reset_index(drop=True)

        # æ·»åŠ æ•°æ®æºæ ‡è¯†åˆ—
        result_df['æ•°æ®æº'] = source_name

        # æ•°æ®è´¨é‡æ£€æŸ¥
        if self.debug:
            self._validate_data(result_df, source_name)

        return result_df

    def _validate_data(self, df: pd.DataFrame, source_name: str):
        """éªŒè¯æ•°æ®è´¨é‡"""
        if df.empty:
            return

        issues = []

        # æ£€æŸ¥ä»·æ ¼æ•°æ®
        for col in ['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½']:
            if col in df.columns:
                invalid_count = (df[col] <= 0).sum()
                if invalid_count > 0:
                    issues.append(f"{col}<=0: {invalid_count}å¤„")

        # æ£€æŸ¥æˆäº¤é‡
        if 'æˆäº¤é‡' in df.columns:
            invalid_volume = (df['æˆäº¤é‡'] < 0).sum()
            if invalid_volume > 0:
                issues.append(f"æˆäº¤é‡<0: {invalid_volume}å¤„")

        # æ£€æŸ¥ä»·æ ¼è·³è·ƒ
        if 'æ”¶ç›˜' in df.columns and len(df) > 1:
            price_change = df['æ”¶ç›˜'].pct_change().abs()
            jumps = (price_change > 0.2).sum()
            if jumps > 0:
                issues.append(f"ä»·æ ¼è·³è·ƒ>20%: {jumps}å¤„")

        if issues:
            logger.warning(f"âš ï¸  {source_name}æ•°æ®è´¨é‡è­¦å‘Š: {', '.join(issues)}")

    def get_data(self, fund_code: str, beg: str, end: str, fqt: int, proxy: Optional[str]) -> pd.DataFrame:
        """æŒ‰ä¼˜å…ˆçº§è·å–æ•°æ®"""
        # éªŒè¯å‚æ•°
        if not FuquanType.validate(fqt):
            raise InvalidParameterError("fqt", str(fqt), "å¿…é¡»æ˜¯ FuquanType.NONE, FuquanType.FRONT æˆ– FuquanType.BACK")

        if self.debug:
            logger.info(f"ğŸ¯ å¼€å§‹è·å–åŸºé‡‘ {fund_code} æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {beg} ~ {end}")
            logger.info(f"ğŸ“Š å¯ç”¨æ•°æ®æº: {[s['name'] for s in self.sources]} (æŒ‰è´¨é‡æ’åº)")

        last_error = None

        # æŒ‰ä¼˜å…ˆçº§å°è¯•å„ä¸ªæ•°æ®æº
        for source in self.sources:
            try:
                data = source['function'](fund_code, beg, end, fqt, proxy)

                if data is not None and not data.empty:
                    # éªŒè¯æ•°æ®è´¨é‡
                    if self._is_data_acceptable(data):
                        if self.debug:
                            logger.info(f"ğŸ‰ æˆåŠŸä» {source['name']} è·å–æ•°æ®")
                        return data
                    else:
                        if self.debug:
                            logger.warning(f"âš ï¸  {source['name']} æ•°æ®è´¨é‡ä¸ä½³ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ•°æ®æº")
                        continue
                else:
                    if self.debug:
                        logger.warning(f"âš ï¸  {source['name']} è¿”å›ç©ºæ•°æ®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ•°æ®æº")

            except Exception as e:
                last_error = e
                if self.debug:
                    logger.error(f"âŒ {source['name']} è·å–å¤±è´¥: {e}")
                continue

        # æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥
        error_msg = f"æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥äº†"
        if last_error:
            error_msg += f"ã€‚æœ€åé”™è¯¯: {last_error}"

        raise DataFetchError(fund_code, last_error)

    def _is_data_acceptable(self, df: pd.DataFrame) -> bool:
        """æ£€æŸ¥æ•°æ®è´¨é‡æ˜¯å¦å¯æ¥å—"""
        if df.empty:
            return False

        # åŸºæœ¬è´¨é‡æ£€æŸ¥
        if len(df) < 5:  # æ•°æ®å¤ªå°‘
            return False

        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_cols = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡']
        for col in required_cols:
            if col not in df.columns:
                return False

        # æ£€æŸ¥ä»·æ ¼æ•°æ®åˆç†æ€§
        price_cols = ['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½']
        for col in price_cols:
            if col in df.columns:
                invalid_count = (df[col] <= 0).sum()
                if invalid_count > len(df) * 0.1:  # è¶…è¿‡10%çš„æ•°æ®æ— æ•ˆ
                    return False

        return True

    def get_available_sources(self) -> List[str]:
        """è·å–å¯ç”¨æ•°æ®æºåˆ—è¡¨"""
        return [source['name'] for source in self.sources]


# å…¨å±€æ•°æ®æºç®¡ç†å™¨å®ä¾‹
_data_manager: Optional[DataSourceManager] = None


def get_fund_k_history(fund_code: str, beg: str = '20200101', end: Optional[str] = None,
                       fqt: int = FuquanType.FRONT, proxy: Optional[str] = None,
                       debug: Optional[bool] = None) -> pd.DataFrame:
    """
    ç»Ÿä¸€çš„åŸºé‡‘Kçº¿å†å²æ•°æ®è·å–å‡½æ•°

    æŒ‰ç…§æ•°æ®è´¨é‡ä¼˜å…ˆçº§è‡ªåŠ¨å°è¯•ä¸åŒæ•°æ®æºï¼š
    1. åŒèŠ±é¡º (æ•°æ®è´¨é‡æœ€é«˜)
    2. ç™¾åº¦è‚¡å¸‚é€š (æ•°æ®æ›´æ–°åŠæ—¶)
    3. æ–°æµªè´¢ç» (ç¨³å®šæ€§å¥½)

    Args:
        fund_code: åŸºé‡‘ä»£ç ï¼Œå¦‚ '159915'
        beg: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤'20200101'
        end: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤ä»Šå¤©
        fqt: å¤æƒç±»å‹ï¼ŒFuquanType.FRONT(1)å‰å¤æƒï¼ŒFuquanType.BACK(2)åå¤æƒï¼ŒFuquanType.NONE(0)ä¸å¤æƒ
        proxy: ä»£ç†åœ°å€ï¼ˆå¯é€‰ï¼‰
        debug: æ˜¯å¦æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯

    Returns:
        pd.DataFrame: æ ‡å‡†åŒ–çš„Kçº¿æ•°æ®

    Raises:
        DataFetchError: å½“æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥æ—¶
        InvalidParameterError: å½“å‚æ•°æ— æ•ˆæ—¶

    Examples:
        # åŸºæœ¬ç”¨æ³•
        df = get_fund_k_history('159915', '20240101', '20241201')

        # ä½¿ç”¨åå¤æƒ
        df = get_fund_k_history('159915', '20240101', '20241201', fqt=FuquanType.BACK)

        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
        df = get_fund_k_history('159915', debug=True)
    """
    global _data_manager

    # åˆå§‹åŒ–æ•°æ®æºç®¡ç†å™¨
    if _data_manager is None:
        try:
            _data_manager = DataSourceManager(debug=debug if debug is not None else False)
        except DataSourceUnavailableError as e:
            raise DataFetchError(fund_code, e)

    # å¤„ç†é»˜è®¤å€¼
    if end is None:
        end = datetime.now().strftime('%Y%m%d')

    if debug is None:
        debug = False

    # æ ¼å¼åŒ–æ—¥æœŸ
    if len(beg) == 8:  # YYYYMMDDæ ¼å¼
        beg_formatted = f"{beg[:4]}-{beg[4:6]}-{beg[6:8]}"
    else:  # YYYY-MM-DDæ ¼å¼
        beg_formatted = beg
        beg = beg.replace('-', '')

    if len(end) == 8:  # YYYYMMDDæ ¼å¼
        end_formatted = f"{end[:4]}-{end[4:6]}-{end[6:8]}"
    else:  # YYYY-MM-DDæ ¼å¼
        end_formatted = end
        end = end.replace('-', '')

    # è·å–æ•°æ®
    try:
        data = _data_manager.get_data(fund_code, beg, end, fqt, proxy)
        return data
    except Exception as e:
        if isinstance(e, UnifiedFinanceDataError):
            raise
        else:
            raise DataFetchError(fund_code, e)


def get_available_sources() -> List[str]:
    """è·å–å¯ç”¨çš„æ•°æ®æºåˆ—è¡¨"""
    global _data_manager

    if _data_manager is None:
        try:
            _data_manager = DataSourceManager()
        except DataSourceUnavailableError:
            return []

    return _data_manager.get_available_sources()


def test_data_sources(fund_code: str = '159915', beg: str = None, end: str = None, debug: bool = True) -> bool:
    """æµ‹è¯•æ‰€æœ‰æ•°æ®æºçš„å¯ç”¨æ€§"""
    logger.info("ğŸ§ª æµ‹è¯•æ•°æ®æºå¯ç”¨æ€§...")
    logger.info("=" * 60)

    # è®¾ç½®é»˜è®¤æ—¥æœŸ
    if end is None:
        end = datetime.now().strftime('%Y%m%d')
    if beg is None:
        beg = (datetime.now() - timedelta(days=10)).strftime('%Y%m%d')

    available_sources = get_available_sources()
    logger.info(f"ğŸ“Š å¯ç”¨æ•°æ®æº: {available_sources}")

    # æµ‹è¯•ç»Ÿä¸€æ¥å£
    try:
        logger.info(f"\nğŸ”„ æµ‹è¯•ç»Ÿä¸€æ¥å£è·å– {fund_code} æ•°æ®...")
        df = get_fund_k_history(fund_code, beg, end, debug=debug)
        logger.info(f"âœ… ç»Ÿä¸€æ¥å£æµ‹è¯•æˆåŠŸ: è·å–åˆ° {len(df)} æ¡æ•°æ®")
        logger.info(f"ğŸ“… æ•°æ®èŒƒå›´: {df['æ—¥æœŸ'].min()} ~ {df['æ—¥æœŸ'].max()}")
        return True
    except Exception as e:
        logger.error(f"âŒ ç»Ÿä¸€æ¥å£æµ‹è¯•å¤±è´¥: {e}")
        return False